{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Time Separated Model\n","This origin comming from https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model/notebook"]},{"cell_type":"code","execution_count":125,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:28:43.280641Z","iopub.status.busy":"2023-11-23T00:28:43.280357Z","iopub.status.idle":"2023-11-23T00:28:59.446106Z","shell.execute_reply":"2023-11-23T00:28:59.445172Z","shell.execute_reply.started":"2023-11-23T00:28:43.280616Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BASE_OUTPUT_PATH: ../output\n","BASE_INPUT_PATH: ../kaggle/input/optiver-trading-at-the-close\n","TRAIN_FILE: ../kaggle/input/optiver-trading-at-the-close/train.csv\n","TEST_FILE: ../kaggle/input/optiver-trading-at-the-close/test.csv\n","IS_LOCAL: True\n","IS_TRAIN: True\n","IS_INFER: True\n","IS_USE_SAVED_MODEL: False\n"]}],"source":["from pathlib import Path\n","import os\n","import warnings\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import gc  # Garbage collection for memory management\n","import os  # Operating system-related functions\n","import time  # Time-related functions\n","import warnings  # Handling warnings\n","from itertools import combinations  # For creating combinations of elements\n","from warnings import simplefilter  # Simplifying warning handling\n","\n","# 📦 Importing machine learning libraries\n","import joblib  # For saving and loading models\n","import numpy as np  # Numerical operations\n","import pandas as pd  # Data manipulation and analysis\n","from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n","from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n","\n","# 🤐 Disable warnings to keep the code clean\n","warnings.filterwarnings(\"ignore\")\n","simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n","\n","max_lookback = np.nan  # Maximum lookback (not specified)\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","seed = 2023\n","num_folds = 5 # クロスバリデーションの分割数\n","\n","# For kaggle environment\n","if os.environ.get(\"KAGGLE_DATA_PROXY_TOKEN\") != None:\n","    BASE_OUTPUT_PATH = Path(f'/kaggle/working')\n","    BASE_INPUT_PATH = Path(f'/kaggle/input/optiver-trading-at-the-close')\n","    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n","    TEST_FILE = Path(f'{BASE_INPUT_PATH}/test.csv')\n","    \n","    IS_LOCAL = False # If kaggle environment, set False\n","    IS_TRAIN = True # If kaggle environment, set True\n","    IS_INFER = True # If kaggle environment, set True\n","    IS_USE_SAVED_MODEL = False # Use saved model or not\n","    USE_OPTUNA = True # Use optuna or not\n","\n","    if IS_LOCAL:\n","        SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/sample_submission.csv')\n","        REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/revealed_targets.csv')\n","\n","    stopping_rounds = 3 # early_stopping用コールバック関数\n","    num_boost_round = 30 # 計算回数\n","    DEVICE = 'gpu' # cpu or gpu\n","\n","    lgb_params = {\n","        'task': 'train',                   # 学習\n","        'boosting_type': 'gbdt',           # GBDT\n","        'objective': 'regression',         # 回帰\n","        'metric': 'rmse',                  # 損失（誤差）\n","        'learning_rate': 0.01,             # 学習率\n","        'lambda_l1': 0.5,                  # L1正則化項の係数\n","        'lambda_l2': 0.5,                  # L2正則化項の係数\n","        'num_leaves': 10,                  # 最大葉枚数\n","        'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n","        'bagging_fraction': 0.5,           # ランダムに抽出される標本の割合\n","        'bagging_freq': 5,                 # バギング実施頻度\n","        'min_child_samples': 10,           # 葉に含まれる最小データ数\n","        'seed': seed,                       # シード値\n","        \"device\": DEVICE,\n","        'verbosity': -1\n","    }\n","\n","\n","    \"\"\"\n","    lgb_params = {\n","        'task': 'train',                   # 学習\n","        'objective': 'regression',                # 目的関数の種類。ここでは回帰タスクを指定\n","        'metric': 'rmse',                          # 評価指標\n","        'boosting_type': 'gbdt',                  # ブースティングタイプ。勾配ブースティング決定木\n","        \"n_estimators\": 32,                        # ブースティングに使用する木の数。多いほど性能が向上するが計算コストが増加\n","        \"num_leaves\": 64,                         # 木に存在する最大の葉の数。大きい値は精度を向上させるが過学習のリスクが増加\n","        \"subsample\": 0.8,                         # 各木のトレーニングに使用されるデータの割合。過学習を防ぐために一部のデータをサンプリング\n","        \"colsample_bytree\": 0.8,                  # 木を構築する際に使用される特徴の割合。特徴のサブセットを使用し過学習を防ぐ\n","        \"learning_rate\": 0.01,                 # 学習率。小さい値は堅牢なモデルを生成するが収束に時間がかかる\n","        'max_depth': 32,                           # 木の最大の深さ。深い木は複雑なモデルを作成するが過学習のリスクがある\n","        \"device\": DEVICE,                         # トレーニングに使用するデバイス（CPUまたはGPU）\n","        \"verbosity\": -1,                          # LightGBMのログ出力のレベル。-1はログを出力しないことを意味する\n","        \"importance_type\": \"gain\",                # 特徴重要度を計算する際の指標。\"gain\"は分割による平均情報利得\n","        'lambda_l1': 0.5,                         # L1正則化項の係数。過学習を防ぐためにモデルの複雑さにペナルティを課す\n","        'lambda_l2': 0.5,                         # L2正則化項の係数。同じく過学習を防ぐ\n","        'bagging_freq': 5,                 # バギング実施頻度\n","        'min_child_samples': 10,           # 葉に含まれる最小データ数\n","        'seed': seed,                       # シード値\n","    }\n","    \"\"\"\n","\n","# For local environment\n","else:\n","    BASE_OUTPUT_PATH = Path(f'../output')\n","    BASE_INPUT_PATH = Path(f'../kaggle/input/optiver-trading-at-the-close')\n","    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n","    TEST_FILE = Path(f'{BASE_INPUT_PATH}/test.csv')\n","\n","    SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/sample_submission.csv')\n","    REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/revealed_targets.csv')\n","\n","    IS_LOCAL = True\n","    IS_TRAIN = True\n","    IS_INFER = True\n","    IS_USE_SAVED_MODEL = False # Use saved model or not\n","    USE_OPTUNA = True\n","    TARGET_STOCK_IDS = [0,1,2,3,4,5,6,7,8,9]\n","\n","    # For training\n","    stopping_rounds = 1 # early_stopping用コールバック関数\n","    num_boost_round = 1 # 計算回数\n","    DEVICE = 'cpu' # cpu or gpu\n","\n","    lgb_params = {\n","        'task': 'train',                   # 学習\n","        'boosting_type': 'gbdt',           # GBDT\n","        'objective': 'regression',         # 回帰\n","        'metric': 'rmse',                  # 損失（誤差）\n","        'learning_rate': 0.01,             # 学習率\n","        'lambda_l1': 0.5,                  # L1正則化項の係数\n","        'lambda_l2': 0.5,                  # L2正則化項の係数\n","        'num_leaves': 10,                  # 最大葉枚数\n","        'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n","        'bagging_fraction': 0.5,           # ランダムに抽出される標本の割合\n","        'bagging_freq': 5,                 # バギング実施頻度\n","        'min_child_samples': 10,           # 葉に含まれる最小データ数\n","        'seed': seed,                       # シード値\n","        \"device\": DEVICE,\n","        'verbosity': -1\n","    }\n","\n","\n","print(f\"BASE_OUTPUT_PATH: {BASE_OUTPUT_PATH}\")\n","print(f\"BASE_INPUT_PATH: {BASE_INPUT_PATH}\")\n","print(f\"TRAIN_FILE: {TRAIN_FILE}\")\n","print(f\"TEST_FILE: {TEST_FILE}\")\n","print(f\"IS_LOCAL: {IS_LOCAL}\")\n","print(f\"IS_TRAIN: {IS_TRAIN}\")\n","print(f\"IS_INFER: {IS_INFER}\")\n","print(f\"IS_USE_SAVED_MODEL: {IS_USE_SAVED_MODEL}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Model Train Results"]},{"cell_type":"code","execution_count":126,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:28:59.447909Z","iopub.status.busy":"2023-11-23T00:28:59.447618Z","iopub.status.idle":"2023-11-23T00:28:59.691188Z","shell.execute_reply":"2023-11-23T00:28:59.690256Z","shell.execute_reply.started":"2023-11-23T00:28:59.447883Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["RAM memory GB usage = 1.239\n","CPU times: user 179 ms, sys: 17.7 ms, total: 197 ms\n","Wall time: 196 ms\n"]}],"source":["%%time \n","\n","from gc import collect;\n","from psutil import Process;\n","from os import system, getpid, walk;\n","\n","# Defining global configurations and functions:-\n","\n","    \n","def GetMemUsage():\n","    \"\"\"\n","    This function defines the memory usage across the kernel. \n","    Source-\n","    https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\n","    \"\"\";\n","    \n","    pid = getpid();\n","    py = Process(pid);\n","    memory_use = py.memory_info()[0] / 2. ** 30;\n","    return f\"RAM memory GB usage = {memory_use :.4}\";\n","\n","\n","collect();\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Functions"]},{"cell_type":"code","execution_count":127,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:28:59.692816Z","iopub.status.busy":"2023-11-23T00:28:59.692460Z","iopub.status.idle":"2023-11-23T00:28:59.713818Z","shell.execute_reply":"2023-11-23T00:28:59.712908Z","shell.execute_reply.started":"2023-11-23T00:28:59.692784Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 30 µs, sys: 0 ns, total: 30 µs\n","Wall time: 31.9 µs\n"]}],"source":["%%time \n","\n","from typing import Sequence, Tuple\n","import pandas as pd\n","\n","# for local execution\n","class MockApi:\n","    def __init__(self):\n","        '''\n","        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n","        They've been intentionally left in an invalid state.\n","\n","        Variables to set:\n","            input_paths: a list of two or more paths to the csv files to be served\n","            group_id_column: the column that identifies which groups of rows the API should serve.\n","                A call to iter_test serves all rows of all dataframes with the current group ID value.\n","            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n","        '''\n","        self.input_paths: Sequence[str] = [TEST_FILE, REVEALED_TARGETS_FILE, SAMPLE_SUBMISSION_FILE]\n","        self.group_id_column: str = 'time_id'\n","        self.export_group_id_column: bool = True\n","        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n","        assert len(self.input_paths) >= 2\n","\n","        self._status = 'initialized'\n","        self.predictions = []\n","\n","    def iter_test(self) -> Tuple[pd.DataFrame]:\n","        '''\n","        Loads all of the dataframes specified in self.input_paths,\n","        then yields all rows in those dataframes that equal the current self.group_id_column value.\n","        '''\n","        if self._status != 'initialized':\n","\n","            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n","\n","        dataframes = []\n","        for pth in self.input_paths:\n","            dataframes.append(pd.read_csv(pth, low_memory=False))\n","        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n","        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n","\n","        for group_id in group_order:\n","            self._status = 'prediction_needed'\n","            current_data = []\n","            for df in dataframes:\n","                cur_df = df.loc[group_id].copy()\n","                # returning single line dataframes from df.loc requires special handling\n","                if not isinstance(cur_df, pd.DataFrame):\n","                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n","                    cur_df.index.name = self.group_id_column\n","                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n","                current_data.append(cur_df)\n","            yield tuple(current_data)\n","\n","            while self._status != 'prediction_received':\n","                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n","                yield None\n","\n","        with open('submission.csv', 'w') as f_open:\n","            pd.concat(self.predictions).to_csv(f_open, index=False)\n","        self._status = 'finished'\n","\n","    def predict(self, user_predictions: pd.DataFrame):\n","        '''\n","        Accepts and stores the user's predictions and unlocks iter_test once that is done\n","        '''\n","        if self._status == 'finished':\n","            raise Exception('You have already made predictions for the full test set.')\n","        if self._status != 'prediction_needed':\n","            raise Exception('You must get the next test sample from `iter_test()` first.')\n","        if not isinstance(user_predictions, pd.DataFrame):\n","            raise Exception('You must provide a DataFrame.')\n","\n","        self.predictions.append(user_predictions)\n","        self._status = 'prediction_received'\n","\n","\n","def make_env():\n","    return MockApi()"]},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:28:59.716182Z","iopub.status.busy":"2023-11-23T00:28:59.715904Z","iopub.status.idle":"2023-11-23T00:28:59.728659Z","shell.execute_reply":"2023-11-23T00:28:59.727815Z","shell.execute_reply.started":"2023-11-23T00:28:59.716159Z"},"trusted":true},"outputs":[],"source":["def pd_display_max():\n","    pd.set_option('display.max_rows', None)  # 行の最大表示数を無制限に設定\n","    pd.set_option('display.max_columns', None)  # 列の最大表示数を無制限に設定\n","    pd.set_option('display.width', None)  # 表示幅を拡張\n","    pd.set_option('display.max_colwidth', None)  # 列の幅を最大に設定\n","\n","def pd_clear_display_max():\n","    pd.set_option('display.max_rows', 10)\n","    pd.set_option('display.max_columns', 10)\n","    pd.set_option('display.width', None)  # 表示幅を拡張\n","    pd.set_option('display.max_colwidth', None)  # 列の幅を最大に設定"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:28:59.730354Z","iopub.status.busy":"2023-11-23T00:28:59.730025Z","iopub.status.idle":"2023-11-23T00:28:59.745078Z","shell.execute_reply":"2023-11-23T00:28:59.744302Z","shell.execute_reply.started":"2023-11-23T00:28:59.730324Z"},"trusted":true},"outputs":[],"source":["# 🧹 Function to reduce memory usage of a Pandas DataFrame\n","def reduce_mem_usage(df, name: str):\n","    \"\"\"\n","    Iterate through all numeric columns of a dataframe and modify the data type\n","    to reduce memory usage.\n","    \"\"\"\n","    \n","    # 📏 Calculate the initial memory usage of the DataFrame\n","    start_mem = df.memory_usage().sum() / 1024**2\n","\n","    # 🔄 Iterate through each column in the DataFrame\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","\n","        # Check if the column's data type is not 'object' (i.e., numeric)\n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            \n","            # Check if the column's data type is an integer\n","            if str(col_type)[:3] == \"int\":\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                # Check if the column's data type is a float\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float32)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float32)\n","\n","\n","    print(f\"Memory usage of {name} is {start_mem:.2f} MB\")\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n","    decrease = 100 * (start_mem - end_mem) / start_mem\n","    print(f\"Decreased by {decrease:.2f}%\")\n","\n","    # 🔄 Return the DataFrame with optimized memory usage\n","\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    return df"]},{"cell_type":"code","execution_count":130,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:28:59.746762Z","iopub.status.busy":"2023-11-23T00:28:59.746419Z","iopub.status.idle":"2023-11-23T00:29:00.392193Z","shell.execute_reply":"2023-11-23T00:29:00.391454Z","shell.execute_reply.started":"2023-11-23T00:28:59.746732Z"},"trusted":true},"outputs":[],"source":["# 🏎️ Import Numba for just-in-time (JIT) compilation and parallel processing\n","from numba import njit, prange\n","\n","# 📊 Function to compute triplet imbalance in parallel using Numba\n","@njit(parallel=True)\n","def compute_triplet_imbalance(df_values, comb_indices):\n","    num_rows = df_values.shape[0]\n","    num_combinations = len(comb_indices)\n","    imbalance_features = np.empty((num_rows, num_combinations))\n","\n","    # 🔁 Loop through all combinations of triplets\n","    for i in prange(num_combinations):\n","        a, b, c = comb_indices[i]\n","        \n","        # 🔁 Loop through rows of the DataFrame\n","        for j in range(num_rows):\n","            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n","            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n","            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n","            \n","            # 🚫 Prevent division by zero\n","            if mid_val == min_val:\n","                imbalance_features[j, i] = np.nan\n","            else:\n","                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n","\n","    return imbalance_features\n","\n","# 📈 Function to calculate triplet imbalance for given price data and a DataFrame\n","def calculate_triplet_imbalance_numba(price, df):\n","    # Convert DataFrame to numpy array for Numba compatibility\n","    df_values = df[price].values\n","    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n","\n","    # Calculate the triplet imbalance using the Numba-optimized function\n","    features_array = compute_triplet_imbalance(df_values, comb_indices)\n","\n","    # Create a DataFrame from the results\n","    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n","    features = pd.DataFrame(features_array, columns=columns)\n","\n","    return features"]},{"cell_type":"code","execution_count":131,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:00.393697Z","iopub.status.busy":"2023-11-23T00:29:00.393368Z","iopub.status.idle":"2023-11-23T00:29:00.417347Z","shell.execute_reply":"2023-11-23T00:29:00.416464Z","shell.execute_reply.started":"2023-11-23T00:29:00.393671Z"},"trusted":true},"outputs":[],"source":["# 📊 Function to generate imbalance features\n","def imbalance_features(df):\n","    if DEVICE == 'gpu':\n","        import cudf\n","        df = cudf.from_pandas(df)\n","    \n","    # Define lists of price and size-related column names\n","    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","\n","    # V1 features\n","    # Calculate various features using Pandas eval function\n","    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n","    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n","    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n","    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n","    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n","    \n","    # Create features for pairwise price imbalances\n","    for c in combinations(prices, 2):\n","        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n","        \n","    # V2 features\n","    # Calculate additional features\n","    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n","    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n","    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n","    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n","    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n","    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n","    df['match_balance'] = ( df['matched_size']  + (df['imbalance_buy_sell_flag'] * df['imbalance_size'])) / df['matched_size']\n","    \n","    # Calculate various statistical aggregation features\n","    \n","        \n","    # V3 features\n","    # Calculate shifted and return features for specific columns\n","    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n","        for window in [1, 2, 3, 10]:\n","            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n","            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n","    \n","    # Calculate diff features for specific columns\n","    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n","        for window in [1, 2, 3, 10]:\n","            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n","    if DEVICE == 'gpu':\n","        df = df.to_pandas()\n","    # Replace infinite values with 0\n","    return df.replace([np.inf, -np.inf], 0)\n","\n","def numba_imb_features(df):\n","    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","    \n","    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n","        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n","        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n","        \n","    # Calculate triplet imbalance features using the Numba-optimized function\n","    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n","        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n","        df[triplet_feature.columns] = triplet_feature.values\n","    return df\n","\n","# 📅 Function to generate time and stock-related features\n","def other_features(df):\n","    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n","    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n","    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n","\n","    # Map global features to the DataFrame\n","    for key, value in global_stock_id_feats.items():\n","        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n","\n","    return df\n","\n","\n","def cal_vix(df, group_key, target_col, period):\n","    return df.groupby(group_key)[target_col].transform(lambda x: np.log(x).diff().rolling(period).std())\n","\n","\n","def generate_historical_features(df, is_train):\n","    for col in ['wap', 'match_balance']:\n","        for window in [1, 2, 3, 10]:\n","            col_name = f\"{col}_diff_{window}\"\n","            df[col_name] = df.groupby(\"stock_id\")[col].diff(window)\n","            #df[col_name] = df[col_name].fillna(0)  # NaNを0で置き換える\n","        for period in [5]:\n","            col_name = f\"{col}_vix_{period}\"\n","            df[col_name] = cal_vix(df, ['stock_id', 'date_id'], col, period)\n","\n","    df = df.replace([np.inf, -np.inf], 0)\n","    return df\n","\n","# 🚀 Function to generate all features by combining imbalance and other features\n","def generate_all_features(df, is_train):\n","    prev_cols = list(df.columns)\n","\n","    # Generate imbalance features\n","    df = imbalance_features(df)\n","    df = numba_imb_features(df)\n","    df = generate_historical_features(df, is_train)\n","    df = other_features(df)\n","    \n","    generated_feature_name = list(set(df.columns) - set(prev_cols))\n","    gc.collect()  # Perform garbage collection to free up memory\n","\n","    df = reduce_mem_usage(df, 'features')\n","    return df, generated_feature_name\n","\n","def normarized_features(df):\n","    df['normalized_wap'] = (df['wap'] - global_population_wap['mean']) / global_population_wap['std']\n","    df['normalized_match_balance'] = (df['match_balance'] - global_population_mathch_balance['mean']) / global_population_mathch_balance['std']\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["# Generationg train dataset"]},{"cell_type":"code","execution_count":132,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:00.419125Z","iopub.status.busy":"2023-11-23T00:29:00.418758Z","iopub.status.idle":"2023-11-23T00:29:00.434666Z","shell.execute_reply":"2023-11-23T00:29:00.433818Z","shell.execute_reply.started":"2023-11-23T00:29:00.419092Z"},"trusted":true},"outputs":[],"source":["def load_train_dataset():\n","    df = pd.read_csv(TRAIN_FILE)\n","    # 🧹 Remove rows with missing values in the \"target\" column\n","    df = df.dropna(subset=[\"target\"])\n","    # 🔁 Reset the index of the DataFrame and apply the changes in place\n","    df.reset_index(drop=True, inplace=True)\n","    return df\n"]},{"cell_type":"code","execution_count":133,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:00.436164Z","iopub.status.busy":"2023-11-23T00:29:00.435884Z","iopub.status.idle":"2023-11-23T00:29:22.634330Z","shell.execute_reply":"2023-11-23T00:29:22.633415Z","shell.execute_reply.started":"2023-11-23T00:29:00.436140Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Memory usage of features is 258.35 MB\n","Memory usage after optimization is: 125.89 MB\n","Decreased by 51.27%\n","Build Online Train Feats Finished.\n","RAM memory GB usage = 1.824\n","CPU times: user 10.1 s, sys: 2.15 s, total: 12.2 s\n","Wall time: 13.2 s\n"]}],"source":["%%time\n","\n","# Check if the code is running in offline or online mode\n","df_train = load_train_dataset()\n","\n","if IS_LOCAL:\n","    # In local mode, stock id TARGET_STOCK_ID is used for training\n","    df_train = df_train[df_train[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","\n","if IS_TRAIN:\n","    global_stock_id_feats = {\n","        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n","        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n","        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n","        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n","        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n","        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n","    }\n","\n","    df_train, generated_feature_name = generate_all_features(df_train, True)\n","\n","    #global_population_wap = df_train['wap'].describe()\n","    #global_population_mathch_balance = df_train['match_balance'].describe()\n","\n","    # normarize\n","    # df_train = normarized_features(df_train)\n","\n","    print(\"Build Online Train Feats Finished.\")\n","\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"code","execution_count":144,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:22.637651Z","iopub.status.busy":"2023-11-23T00:29:22.637344Z","iopub.status.idle":"2023-11-23T00:29:22.646354Z","shell.execute_reply":"2023-11-23T00:29:22.645449Z","shell.execute_reply.started":"2023-11-23T00:29:22.637627Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['all_sizes_std',\n"," 'wap_diff_2',\n"," 'wap_diff_3',\n"," 'global_std_size',\n"," 'all_sizes_kurt',\n"," 'all_prices_std',\n"," 'minute',\n"," 'match_balance_diff_10',\n"," 'global_median_price',\n"," 'global_ptp_price',\n"," 'bid_size_ask_size_imbalance_size_imb2',\n"," 'all_prices_kurt',\n"," 'all_prices_skew',\n"," 'matched_size_bid_size_imbalance_size_imb2',\n"," 'all_sizes_skew',\n"," 'global_std_price',\n"," 'all_sizes_mean',\n"," 'ask_price_bid_price_reference_price_imb2',\n"," 'bid_price_wap_reference_price_imb2',\n"," 'all_prices_mean',\n"," 'dow',\n"," 'wap_diff_10',\n"," 'match_balance_diff_1',\n"," 'ask_price_wap_reference_price_imb2',\n"," 'wap_diff_1',\n"," 'match_balance_diff_2',\n"," 'seconds',\n"," 'match_balance_diff_3',\n"," 'match_balance_vix_5',\n"," 'ask_price_bid_price_wap_imb2',\n"," 'matched_size_bid_size_ask_size_imb2',\n"," 'global_ptp_size',\n"," 'wap_vix_5',\n"," 'matched_size_ask_size_imbalance_size_imb2',\n"," 'global_median_size']"]},"execution_count":144,"metadata":{},"output_type":"execute_result"}],"source":["# feature selection\n","\"\"\"\n","feature_name = [\n","    \"wap_vix_5\", \"reference_price_shift_10\", \"matched_size_ret_10\",\n","    \"matched_size_shift_10\", \"match_balance_vix_5\", \"ask_price_bid_price_reference_price_imb2\",\n","    \"seconds_in_bucket\", \"match_balance_diff_10\", \"imbalance_size_ret_10\",\n","    \"ask_size_diff_10\", \"imbalance_size_shift_10\", \"reference_price\",\n","    \"ask_price_bid_price_imb\", \"reference_price_ret_10\", \"all_sizes_mean\",\n","    \"matched_size\", \"bid_size_diff_10\", \"volume\", \"reference_price_shift_3\",\n","    \"bid_size\", \"bid_price_wap_reference_price_imb2\", \"ask_size\",\n","    \"ask_size_diff_3\", \"reference_price_bid_price_imb\", \"reference_price_wap_imb\",\n","    \"all_prices_kurt\", \"matched_size_bid_size_ask_size_imb2\", \"bid_price\",\n","    \"wap_diff_10\", \"bid_size_diff_3\", \"all_prices_std\", \"bid_size_ask_size_imbalance_size_imb2\",\n","    \"all_prices_skew\", \"all_sizes_skew\", \"bid_price_wap_imb\",\n","    \"ask_price_diff_10\", \"ask_price_wap_imb\", \"imbalance_size_ret_3\",\n","    \"matched_size_shift_3\", \"reference_price_shift_1\", \"all_prices_mean\",\n","    \"imbalance_size\", \"matched_size_ret_3\", \"reference_price_shift_2\",\n","    \"ask_price\", \"ask_size_diff_2\", \"bid_size_diff_2\", \"price_pressure\",\n","    \"reference_price_ask_price_imb\", \"matched_size_shift_1\", \"bid_size_diff_1\",\n","    \"market_urgency\", \"wap_diff_3\", \"price_spread\", \"all_sizes_std\",\n","    \"matched_size_shift_2\", \"imbalance_size_shift_3\", \"bid_price_diff_10\",\n","    \"ask_size_diff_1\", \"far_price\", \"reference_price_ret_3\", \"match_balance\",\n","    \"wap_diff_2\", \"wap_diff_1\", \"matched_imbalance\", \"wap\"\n","]\"\"\"\n","feature_name = generated_feature_name\n","feature_name"]},{"cell_type":"markdown","metadata":{},"source":["## global_stock_id_feats"]},{"cell_type":"code","execution_count":135,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:22.648042Z","iopub.status.busy":"2023-11-23T00:29:22.647655Z","iopub.status.idle":"2023-11-23T00:29:22.706378Z","shell.execute_reply":"2023-11-23T00:29:22.705536Z","shell.execute_reply.started":"2023-11-23T00:29:22.648009Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>median_size</th>\n","      <th>std_size</th>\n","      <th>ptp_size</th>\n","      <th>median_price</th>\n","      <th>std_price</th>\n","      <th>ptp_price</th>\n","    </tr>\n","    <tr>\n","      <th>stock_id</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>42739.16</td>\n","      <td>132986.920030</td>\n","      <td>5898989.29</td>\n","      <td>1.999695</td>\n","      <td>0.003353</td>\n","      <td>0.017414</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>25548.50</td>\n","      <td>66444.908534</td>\n","      <td>693898.57</td>\n","      <td>1.999827</td>\n","      <td>0.005588</td>\n","      <td>0.029370</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>26228.10</td>\n","      <td>75674.654248</td>\n","      <td>1069837.58</td>\n","      <td>2.000200</td>\n","      <td>0.005333</td>\n","      <td>0.051622</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>41667.00</td>\n","      <td>93875.770520</td>\n","      <td>1928848.21</td>\n","      <td>1.999980</td>\n","      <td>0.002903</td>\n","      <td>0.018551</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>34014.58</td>\n","      <td>80670.274550</td>\n","      <td>1604065.54</td>\n","      <td>1.999816</td>\n","      <td>0.003717</td>\n","      <td>0.017379</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>28287.94</td>\n","      <td>108045.498152</td>\n","      <td>2265456.70</td>\n","      <td>2.000082</td>\n","      <td>0.006301</td>\n","      <td>0.072807</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>24720.61</td>\n","      <td>109732.904460</td>\n","      <td>949838.58</td>\n","      <td>1.999907</td>\n","      <td>0.004339</td>\n","      <td>0.018155</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>29798.00</td>\n","      <td>139523.985990</td>\n","      <td>1383196.54</td>\n","      <td>1.999987</td>\n","      <td>0.005259</td>\n","      <td>0.022833</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>57259.95</td>\n","      <td>387634.914143</td>\n","      <td>3047677.20</td>\n","      <td>1.999989</td>\n","      <td>0.004006</td>\n","      <td>0.017607</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>33052.00</td>\n","      <td>69988.944079</td>\n","      <td>831560.60</td>\n","      <td>1.999803</td>\n","      <td>0.003837</td>\n","      <td>0.021646</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          median_size       std_size    ptp_size  median_price  std_price  \\\n","stock_id                                                                    \n","0            42739.16  132986.920030  5898989.29      1.999695   0.003353   \n","1            25548.50   66444.908534   693898.57      1.999827   0.005588   \n","2            26228.10   75674.654248  1069837.58      2.000200   0.005333   \n","3            41667.00   93875.770520  1928848.21      1.999980   0.002903   \n","4            34014.58   80670.274550  1604065.54      1.999816   0.003717   \n","5            28287.94  108045.498152  2265456.70      2.000082   0.006301   \n","6            24720.61  109732.904460   949838.58      1.999907   0.004339   \n","7            29798.00  139523.985990  1383196.54      1.999987   0.005259   \n","8            57259.95  387634.914143  3047677.20      1.999989   0.004006   \n","9            33052.00   69988.944079   831560.60      1.999803   0.003837   \n","\n","          ptp_price  \n","stock_id             \n","0          0.017414  \n","1          0.029370  \n","2          0.051622  \n","3          0.018551  \n","4          0.017379  \n","5          0.072807  \n","6          0.018155  \n","7          0.022833  \n","8          0.017607  \n","9          0.021646  "]},"execution_count":135,"metadata":{},"output_type":"execute_result"}],"source":["df_global_stock_id_feats = pd.DataFrame(global_stock_id_feats)\n","df_global_stock_id_feats"]},{"cell_type":"markdown","metadata":{},"source":["## df_train_feats"]},{"cell_type":"code","execution_count":136,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:22.708217Z","iopub.status.busy":"2023-11-23T00:29:22.707640Z","iopub.status.idle":"2023-11-23T00:29:24.165586Z","shell.execute_reply":"2023-11-23T00:29:24.164721Z","shell.execute_reply.started":"2023-11-23T00:29:22.708183Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>stock_id</th>\n","      <th>date_id</th>\n","      <th>seconds_in_bucket</th>\n","      <th>imbalance_size</th>\n","      <th>imbalance_buy_sell_flag</th>\n","      <th>reference_price</th>\n","      <th>matched_size</th>\n","      <th>far_price</th>\n","      <th>near_price</th>\n","      <th>bid_price</th>\n","      <th>...</th>\n","      <th>match_balance_vix_5</th>\n","      <th>dow</th>\n","      <th>seconds</th>\n","      <th>minute</th>\n","      <th>global_median_size</th>\n","      <th>global_std_size</th>\n","      <th>global_ptp_size</th>\n","      <th>global_median_price</th>\n","      <th>global_std_price</th>\n","      <th>global_ptp_price</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3.180603e+06</td>\n","      <td>1</td>\n","      <td>0.999812</td>\n","      <td>13380277.00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.999812</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>42739.160156</td>\n","      <td>132986.921875</td>\n","      <td>5.898990e+06</td>\n","      <td>1.999695</td>\n","      <td>0.003353</td>\n","      <td>0.017414</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.666039e+05</td>\n","      <td>-1</td>\n","      <td>0.999896</td>\n","      <td>1642214.25</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.999896</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>25548.500000</td>\n","      <td>66444.906250</td>\n","      <td>6.938986e+05</td>\n","      <td>1.999827</td>\n","      <td>0.005588</td>\n","      <td>0.029370</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3.028799e+05</td>\n","      <td>-1</td>\n","      <td>0.999561</td>\n","      <td>1819368.00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.999403</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>26228.099609</td>\n","      <td>75674.656250</td>\n","      <td>1.069838e+06</td>\n","      <td>2.000200</td>\n","      <td>0.005333</td>\n","      <td>0.051622</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1.191768e+07</td>\n","      <td>-1</td>\n","      <td>1.000171</td>\n","      <td>18389746.00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.999999</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>41667.000000</td>\n","      <td>93875.773438</td>\n","      <td>1.928848e+06</td>\n","      <td>1.999980</td>\n","      <td>0.002903</td>\n","      <td>0.018551</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4.475500e+05</td>\n","      <td>-1</td>\n","      <td>0.999532</td>\n","      <td>17860614.00</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>0.999394</td>\n","      <td>...</td>\n","      <td>NaN</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>34014.578125</td>\n","      <td>80670.273438</td>\n","      <td>1.604066e+06</td>\n","      <td>1.999816</td>\n","      <td>0.003717</td>\n","      <td>0.017379</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5237697</th>\n","      <td>5</td>\n","      <td>480</td>\n","      <td>540</td>\n","      <td>1.063551e+06</td>\n","      <td>1</td>\n","      <td>0.998403</td>\n","      <td>11476997.00</td>\n","      <td>0.999396</td>\n","      <td>0.999396</td>\n","      <td>0.998119</td>\n","      <td>...</td>\n","      <td>0.011378</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>28287.939453</td>\n","      <td>108045.500000</td>\n","      <td>2.265457e+06</td>\n","      <td>2.000082</td>\n","      <td>0.006301</td>\n","      <td>0.072807</td>\n","    </tr>\n","    <tr>\n","      <th>5237698</th>\n","      <td>6</td>\n","      <td>480</td>\n","      <td>540</td>\n","      <td>9.985981e+05</td>\n","      <td>1</td>\n","      <td>1.000118</td>\n","      <td>13713463.00</td>\n","      <td>1.000495</td>\n","      <td>1.000469</td>\n","      <td>0.999984</td>\n","      <td>...</td>\n","      <td>0.018601</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>24720.609375</td>\n","      <td>109732.906250</td>\n","      <td>9.498386e+05</td>\n","      <td>1.999907</td>\n","      <td>0.004339</td>\n","      <td>0.018155</td>\n","    </tr>\n","    <tr>\n","      <th>5237699</th>\n","      <td>7</td>\n","      <td>480</td>\n","      <td>540</td>\n","      <td>2.469864e+06</td>\n","      <td>-1</td>\n","      <td>0.996621</td>\n","      <td>73054344.00</td>\n","      <td>0.996304</td>\n","      <td>0.996335</td>\n","      <td>0.996573</td>\n","      <td>...</td>\n","      <td>0.013453</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>29798.000000</td>\n","      <td>139523.984375</td>\n","      <td>1.383196e+06</td>\n","      <td>1.999987</td>\n","      <td>0.005259</td>\n","      <td>0.022833</td>\n","    </tr>\n","    <tr>\n","      <th>5237700</th>\n","      <td>8</td>\n","      <td>480</td>\n","      <td>540</td>\n","      <td>8.815596e+05</td>\n","      <td>-1</td>\n","      <td>1.000927</td>\n","      <td>84052704.00</td>\n","      <td>1.000055</td>\n","      <td>1.000602</td>\n","      <td>1.000703</td>\n","      <td>...</td>\n","      <td>0.002727</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>57259.949219</td>\n","      <td>387634.906250</td>\n","      <td>3.047677e+06</td>\n","      <td>1.999989</td>\n","      <td>0.004006</td>\n","      <td>0.017607</td>\n","    </tr>\n","    <tr>\n","      <th>5237701</th>\n","      <td>9</td>\n","      <td>480</td>\n","      <td>540</td>\n","      <td>0.000000e+00</td>\n","      <td>0</td>\n","      <td>1.001011</td>\n","      <td>12750417.00</td>\n","      <td>1.001011</td>\n","      <td>1.001011</td>\n","      <td>1.001011</td>\n","      <td>...</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>33052.000000</td>\n","      <td>69988.945312</td>\n","      <td>8.315606e+05</td>\n","      <td>1.999803</td>\n","      <td>0.003837</td>\n","      <td>0.021646</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>264550 rows × 127 columns</p>\n","</div>"],"text/plain":["         stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n","0               0        0                  0    3.180603e+06   \n","1               1        0                  0    1.666039e+05   \n","2               2        0                  0    3.028799e+05   \n","3               3        0                  0    1.191768e+07   \n","4               4        0                  0    4.475500e+05   \n","...           ...      ...                ...             ...   \n","5237697         5      480                540    1.063551e+06   \n","5237698         6      480                540    9.985981e+05   \n","5237699         7      480                540    2.469864e+06   \n","5237700         8      480                540    8.815596e+05   \n","5237701         9      480                540    0.000000e+00   \n","\n","         imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n","0                              1         0.999812   13380277.00        NaN   \n","1                             -1         0.999896    1642214.25        NaN   \n","2                             -1         0.999561    1819368.00        NaN   \n","3                             -1         1.000171   18389746.00        NaN   \n","4                             -1         0.999532   17860614.00        NaN   \n","...                          ...              ...           ...        ...   \n","5237697                        1         0.998403   11476997.00   0.999396   \n","5237698                        1         1.000118   13713463.00   1.000495   \n","5237699                       -1         0.996621   73054344.00   0.996304   \n","5237700                       -1         1.000927   84052704.00   1.000055   \n","5237701                        0         1.001011   12750417.00   1.001011   \n","\n","         near_price  bid_price  ...  match_balance_vix_5  dow  seconds  \\\n","0               NaN   0.999812  ...                  NaN    0        0   \n","1               NaN   0.999896  ...                  NaN    0        0   \n","2               NaN   0.999403  ...                  NaN    0        0   \n","3               NaN   0.999999  ...                  NaN    0        0   \n","4               NaN   0.999394  ...                  NaN    0        0   \n","...             ...        ...  ...                  ...  ...      ...   \n","5237697    0.999396   0.998119  ...             0.011378    0        0   \n","5237698    1.000469   0.999984  ...             0.018601    0        0   \n","5237699    0.996335   0.996573  ...             0.013453    0        0   \n","5237700    1.000602   1.000703  ...             0.002727    0        0   \n","5237701    1.001011   1.001011  ...             0.000000    0        0   \n","\n","         minute  global_median_size  global_std_size global_ptp_size  \\\n","0             0        42739.160156    132986.921875    5.898990e+06   \n","1             0        25548.500000     66444.906250    6.938986e+05   \n","2             0        26228.099609     75674.656250    1.069838e+06   \n","3             0        41667.000000     93875.773438    1.928848e+06   \n","4             0        34014.578125     80670.273438    1.604066e+06   \n","...         ...                 ...              ...             ...   \n","5237697       9        28287.939453    108045.500000    2.265457e+06   \n","5237698       9        24720.609375    109732.906250    9.498386e+05   \n","5237699       9        29798.000000    139523.984375    1.383196e+06   \n","5237700       9        57259.949219    387634.906250    3.047677e+06   \n","5237701       9        33052.000000     69988.945312    8.315606e+05   \n","\n","         global_median_price  global_std_price  global_ptp_price  \n","0                   1.999695          0.003353          0.017414  \n","1                   1.999827          0.005588          0.029370  \n","2                   2.000200          0.005333          0.051622  \n","3                   1.999980          0.002903          0.018551  \n","4                   1.999816          0.003717          0.017379  \n","...                      ...               ...               ...  \n","5237697             2.000082          0.006301          0.072807  \n","5237698             1.999907          0.004339          0.018155  \n","5237699             1.999987          0.005259          0.022833  \n","5237700             1.999989          0.004006          0.017607  \n","5237701             1.999803          0.003837          0.021646  \n","\n","[264550 rows x 127 columns]"]},"execution_count":136,"metadata":{},"output_type":"execute_result"}],"source":["df_train"]},{"cell_type":"markdown","metadata":{},"source":["# Model Training"]},{"cell_type":"code","execution_count":146,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:24.167604Z","iopub.status.busy":"2023-11-23T00:29:24.167007Z","iopub.status.idle":"2023-11-23T00:31:37.372490Z","shell.execute_reply":"2023-11-23T00:31:37.371543Z","shell.execute_reply.started":"2023-11-23T00:29:24.167568Z"},"trusted":true},"outputs":[],"source":["# 📦 Import necessary libraries\n","import numpy as np\n","import lightgbm as lgb\n","#import optuna.integration.lightgbm as lgb\n","from sklearn.metrics import mean_absolute_error\n","import gc\n","import os\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import KFold\n","import numpy as np\n","from dataclasses import dataclass\n","import optuna\n","import sys\n","import shutil\n","\n","import optuna.integration.lightgbm as optuna_lgb\n","optuna.logging.set_verbosity(optuna.logging.ERROR)\n","import lightgbm as lgb\n","\n","from warnings import simplefilter\n","simplefilter(\"ignore\", category=RuntimeWarning)\n","\n","@dataclass\n","class Model:\n","    model: lgb.Booster\n","    fold: int\n","    feature_importance: pd.DataFrame\n","    score: float\n","    best_iteration: int\n","    train_time: float = None\n","    weight: float = None\n","    mem_usage: float = None\n","\n","def train_model(train_x, train_y, val_x, val_y, best_params=None):\n","    print(\"------ Model training start ------\")\n","    trains = lgb.Dataset(train_x, train_y)\n","    valids = lgb.Dataset(val_x, val_y, reference=trains)\n","\n","    verbose_eval = 0\n","    if best_params is None:\n","        params = lgb_params\n","    else:\n","        params = best_params\n","\n","    print(\"Use params:\")\n","    print(params)\n","\n","    model = lgb.train(\n","        params,\n","        trains,\n","        valid_sets=[valids, trains], # 検証データ\n","        valid_names=['Train', 'Valid'],    # データセット名前\n","        num_boost_round=num_boost_round,\n","        callbacks=[\n","                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n","                lgb.log_evaluation(verbose_eval)\n","        ]\n","    )\n","\n","    print(\"------ Model training end ------\")\n","    return model\n","\n","def cross_train(df, key, n_splits, feature_name, valid_name, best_params=None):\n","    \"\"\" For Cross Train\n","\n","    Args:\n","        df (_type_): _description_\n","        n_splits (_type_): _description_\n","\n","    Returns:\n","        _type_: _description_\n","    \"\"\"\n","    print(\"----------------------------------------\")\n","    print(f\"Cross Train key id {key}: start, shape: {df.shape}, n_splits: {n_splits}\")\n","    print(f\"num_boost_round: {num_boost_round}, stopping_rounds: {stopping_rounds}, folds: {num_folds}\")\n","\n","    models = []\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","    df.reset_index(drop=True, inplace=True)\n","    \n","    for fold, (train_indices, valid_indices) in enumerate(kf.split(df)):\n","        print(f\"{key}: {fold} start\")\n","        now_time = time.time()\n","        X_train, X_valid = df[feature_name].iloc[train_indices], df[feature_name].iloc[valid_indices]\n","        y_train, y_valid = df[valid_name].loc[train_indices], df[valid_name].loc[valid_indices]\n","        print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}, y_train: {y_train.shape}, y_valid: {y_valid.shape}\")\n","\n","\n","        model = train_model(X_train, y_train, X_valid, y_valid, best_params)\n","\n","        y_valid_pred = model.predict(X_valid)\n","        \n","        # For Debug\n","        #y_valid_df = pd.DataFrame(y_valid, columns=[valid_name])\n","        #y_valid_df[\"pred\"] = y_valid_pred\n","        #y_valid_df.to_csv('filename.csv', index=False)\n","\n","        score = mean_absolute_error(y_valid, y_valid_pred)\n","        train_time = time.time() - now_time\n","        mem_usage = sys.getsizeof(model) / (1024 * 1024) # MB\n","        m = Model(model, fold, model.feature_importance(), score, model.best_iteration, train_time, weight= 1 / n_splits, mem_usage=mem_usage)\n","        print(f\"{key}: {fold} end, score: {score}, time: {m.train_time}, best_iteration: {m.best_iteration}, memory usage: {m.mem_usage}\")\n","        \n","        models.append(m)\n","        \n","        del X_train, X_valid, y_train, y_valid\n","        gc.collect()\n","\n","    print(f\"Cross train {key} model len {len(models)}\")\n","    print(\"----------------------------------------\")\n","    return key, models\n","\n","def optuna_tuning(df, n_splits, feature_name, valid_name):\n","    params = {\n","        'objective': 'regression',         # 回帰\n","        'metric': 'rmse',                  # 損失（誤差）\n","        'verbosity': -1\n","    }\n","\n","    # TBD: spint dataframe by length\n","    train_x = df[feature_name]\n","    train_y = df[valid_name] \n","    \n","    trains = optuna_lgb.Dataset(train_x, train_y)\n","    \n","    print(\"------- Optuna Tuning Start-------\")\n","    print(f\"num_boost_round: {num_boost_round}, stopping_rounds: {stopping_rounds}, folds: {num_folds}\")\n","\n","    folds = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","\n","    verbose_eval = 0\n","    tuner = optuna_lgb.LightGBMTunerCV(\n","        params,\n","        trains,\n","        num_boost_round=num_boost_round,\n","        folds=folds,\n","        callbacks=[\n","                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n","                lgb.log_evaluation(verbose_eval)\n","        ]\n","    )\n","    \n","    tuner.run()\n","    best_params = tuner.best_params\n","    print(\" Params: \")\n","    for key, value in best_params.items():\n","        print(\" {}: {}\".format(key, value))\n","    print(\"------- Optuna Tuning End-------\")\n","    return best_params"]},{"cell_type":"code","execution_count":147,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["------- Optuna Tuning Start-------\n","num_boost_round: 1, stopping_rounds: 1, folds: 5\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Training until validation scores don't improve for 1 rounds\n","Did not meet early stopping. Best iteration is:\n","[1]\tcv_agg's valid rmse: 8.82183 + 0.093477\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Training until validation scores don't improve for 1 rounds\n","Did not meet early stopping. Best iteration is:\n","[1]\tcv_agg's valid rmse: 8.82467 + 0.0932847\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Training until validation scores don't improve for 1 rounds\n","Did not meet early stopping. Best iteration is:\n","[1]\tcv_agg's valid rmse: 8.8245 + 0.0935752\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Training until validation scores don't improve for 1 rounds\n","Did not meet early stopping. Best iteration is:\n","[1]\tcv_agg's valid rmse: 8.82467 + 0.093521\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Training until validation scores don't improve for 1 rounds\n","Did not meet early stopping. Best iteration is:\n","[1]\tcv_agg's valid rmse: 8.84657 + 0.0929911\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Training until validation scores don't improve for 1 rounds\n","Did not meet early stopping. Best iteration is:\n","[1]\tcv_agg's valid rmse: 8.82272 + 0.0928293\n"]},{"name":"stderr","output_type":"stream","text":["feature_fraction, val_score: 8.821828: 100%|##########| 7/7 [00:06<00:00,  1.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Training until validation scores don't improve for 1 rounds\n","Did not meet early stopping. Best iteration is:\n","[1]\tcv_agg's valid rmse: 8.82456 + 0.0931777\n"]},{"name":"stderr","output_type":"stream","text":[]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<timed exec>:4\u001b[0m\n","Cell \u001b[0;32mIn[146], line 143\u001b[0m, in \u001b[0;36moptuna_tuning\u001b[0;34m(df, n_splits, feature_name, valid_name)\u001b[0m\n\u001b[1;32m    131\u001b[0m verbose_eval \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    132\u001b[0m tuner \u001b[39m=\u001b[39m optuna_lgb\u001b[39m.\u001b[39mLightGBMTunerCV(\n\u001b[1;32m    133\u001b[0m     params,\n\u001b[1;32m    134\u001b[0m     trains,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m     ]\n\u001b[1;32m    141\u001b[0m )\n\u001b[0;32m--> 143\u001b[0m tuner\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m    144\u001b[0m best_params \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mbest_params\n\u001b[1;32m    145\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m Params: \u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:497\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_train_set()\n\u001b[1;32m    496\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_feature_fraction()\n\u001b[0;32m--> 497\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtune_num_leaves()\n\u001b[1;32m    498\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_bagging()\n\u001b[1;32m    499\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtune_feature_fraction_stage2()\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:524\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner.tune_num_leaves\u001b[0;34m(self, n_trials)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtune_num_leaves\u001b[39m(\u001b[39mself\u001b[39m, n_trials: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 524\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tune_params(\n\u001b[1;32m    525\u001b[0m         [\u001b[39m\"\u001b[39;49m\u001b[39mnum_leaves\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    526\u001b[0m         n_trials,\n\u001b[1;32m    527\u001b[0m         optuna\u001b[39m.\u001b[39;49msamplers\u001b[39m.\u001b[39;49mTPESampler(seed\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optuna_seed),\n\u001b[1;32m    528\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mnum_leaves\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    529\u001b[0m     )\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:604\u001b[0m, in \u001b[0;36m_LightGBMBaseTuner._tune_params\u001b[0;34m(self, target_param_names, n_trials, sampler, step_name)\u001b[0m\n\u001b[1;32m    602\u001b[0m     _timeout \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39mif\u001b[39;00m _n_trials \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 604\u001b[0m     study\u001b[39m.\u001b[39;49moptimize(\n\u001b[1;32m    605\u001b[0m         objective,\n\u001b[1;32m    606\u001b[0m         n_trials\u001b[39m=\u001b[39;49m_n_trials,\n\u001b[1;32m    607\u001b[0m         timeout\u001b[39m=\u001b[39;49m_timeout,\n\u001b[1;32m    608\u001b[0m         catch\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    609\u001b[0m         callbacks\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optuna_callbacks,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    612\u001b[0m \u001b[39mif\u001b[39;00m pbar:\n\u001b[1;32m    613\u001b[0m     pbar\u001b[39m.\u001b[39mclose()\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    453\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/optuna/integration/_lightgbm_tuner/optimize.py:317\u001b[0m, in \u001b[0;36m_OptunaObjectiveCV.__call__\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    315\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    316\u001b[0m train_set \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_set)\n\u001b[0;32m--> 317\u001b[0m cv_results \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mcv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlgbm_params, train_set, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlgbm_kwargs)\n\u001b[1;32m    319\u001b[0m val_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_cv_scores(cv_results)\n\u001b[1;32m    320\u001b[0m val_score \u001b[39m=\u001b[39m val_scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/lightgbm/engine.py:722\u001b[0m, in \u001b[0;36mcv\u001b[0;34m(params, train_set, num_boost_round, folds, nfold, stratified, shuffle, metrics, feval, init_model, feature_name, categorical_feature, fpreproc, seed, callbacks, eval_train_metric, return_cvbooster)\u001b[0m\n\u001b[1;32m    716\u001b[0m train_set\u001b[39m.\u001b[39m_update_params(params) \\\n\u001b[1;32m    717\u001b[0m          \u001b[39m.\u001b[39m_set_predictor(predictor) \\\n\u001b[1;32m    718\u001b[0m          \u001b[39m.\u001b[39mset_feature_name(feature_name) \\\n\u001b[1;32m    719\u001b[0m          \u001b[39m.\u001b[39mset_categorical_feature(categorical_feature)\n\u001b[1;32m    721\u001b[0m results \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[0;32m--> 722\u001b[0m cvfolds \u001b[39m=\u001b[39m _make_n_folds(full_data\u001b[39m=\u001b[39;49mtrain_set, folds\u001b[39m=\u001b[39;49mfolds, nfold\u001b[39m=\u001b[39;49mnfold,\n\u001b[1;32m    723\u001b[0m                         params\u001b[39m=\u001b[39;49mparams, seed\u001b[39m=\u001b[39;49mseed, fpreproc\u001b[39m=\u001b[39;49mfpreproc,\n\u001b[1;32m    724\u001b[0m                         stratified\u001b[39m=\u001b[39;49mstratified, shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    725\u001b[0m                         eval_train_metric\u001b[39m=\u001b[39;49meval_train_metric)\n\u001b[1;32m    727\u001b[0m \u001b[39m# setup callbacks\u001b[39;00m\n\u001b[1;32m    728\u001b[0m \u001b[39mif\u001b[39;00m callbacks \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/lightgbm/engine.py:513\u001b[0m, in \u001b[0;36m_make_n_folds\u001b[0;34m(full_data, folds, nfold, params, seed, fpreproc, stratified, shuffle, eval_train_metric)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    512\u001b[0m     tparam \u001b[39m=\u001b[39m params\n\u001b[0;32m--> 513\u001b[0m booster_for_fold \u001b[39m=\u001b[39m Booster(tparam, train_set)\n\u001b[1;32m    514\u001b[0m \u001b[39mif\u001b[39;00m eval_train_metric:\n\u001b[1;32m    515\u001b[0m     booster_for_fold\u001b[39m.\u001b[39madd_valid(train_set, \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/lightgbm/basic.py:3204\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[0;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[1;32m   3202\u001b[0m params\u001b[39m.\u001b[39mupdate(train_set\u001b[39m.\u001b[39mget_params())\n\u001b[1;32m   3203\u001b[0m params_str \u001b[39m=\u001b[39m _param_dict_to_str(params)\n\u001b[0;32m-> 3204\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterCreate(\n\u001b[1;32m   3205\u001b[0m     train_set\u001b[39m.\u001b[39;49m_handle,\n\u001b[1;32m   3206\u001b[0m     _c_str(params_str),\n\u001b[1;32m   3207\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle)))\n\u001b[1;32m   3208\u001b[0m \u001b[39m# save reference to data\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_set \u001b[39m=\u001b[39m train_set\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["%%time\n","\n","# Train\n","best_params = None\n","if USE_OPTUNA:\n","    best_params = optuna_tuning(df=df_train, n_splits=num_folds, feature_name=feature_name, valid_name=\"target\")\n","\n","key_models = df_train.groupby(\"seconds_in_bucket\").apply(lambda x: cross_train(df=x, key=x.name, n_splits=num_folds, feature_name=feature_name, valid_name=\"target\", best_params=best_params))\n","model_dict = {key: model for key, model in key_models}\n","\n","if IS_USE_SAVED_MODEL:\n","    model_save_base_path = f\"{BASE_OUTPUT_PATH}/model\"\n","    if os.path.exists(model_save_base_path):\n","        print(f\"{model_save_base_path} already exists, clean up it.\")\n","        shutil.rmtree(model_save_base_path)\n","    os.makedirs(model_save_base_path)\n","\n","    key_model_paths = []\n","    for key, models in key_models:\n","        model_save_path = f\"{model_save_base_path}/{key}\"\n","        os.makedirs(model_save_path)\n","        model_paths = []\n","        for model in models:\n","            model_save_fullpath = f\"{model_save_path}/model_{key}_{model.fold}.txt\"\n","            model.model.save_model(model_save_fullpath)\n","            model_paths.append(model_save_fullpath)\n","        key_model_paths.append((key, model_paths))\n","\n","    model_dict_saved = {key: model_paths for key, model_paths in key_model_paths}\n","    print(model_dict_saved)\n","\n","del df_train\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Total model len 275\n","Total model mem usage 0.014687 MB\n","Model dict len 55\n"]}],"source":["# Show results\n","print(\"\")\n","print(f\"Total model len {(sum([len(models) for key, models in key_models]))}\")\n","print(f\"Total model mem usage {sum([sum([model.mem_usage for model in models]) for key, models in key_models]):2f} MB\")\n","print(f\"Model dict len {len(model_dict)}\")"]},{"cell_type":"code","execution_count":140,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>key</th>\n","      <th>fold</th>\n","      <th>score</th>\n","      <th>best_iteration</th>\n","      <th>train_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>275.000000</td>\n","      <td>275.000000</td>\n","      <td>275.000000</td>\n","      <td>275.0</td>\n","      <td>275.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>270.000000</td>\n","      <td>2.000000</td>\n","      <td>6.077108</td>\n","      <td>1.0</td>\n","      <td>0.069578</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>159.034496</td>\n","      <td>1.416792</td>\n","      <td>1.030951</td>\n","      <td>0.0</td>\n","      <td>0.014693</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>4.565445</td>\n","      <td>1.0</td>\n","      <td>0.060244</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>130.000000</td>\n","      <td>1.000000</td>\n","      <td>5.443119</td>\n","      <td>1.0</td>\n","      <td>0.063557</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>270.000000</td>\n","      <td>2.000000</td>\n","      <td>5.831177</td>\n","      <td>1.0</td>\n","      <td>0.065558</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>410.000000</td>\n","      <td>3.000000</td>\n","      <td>6.242689</td>\n","      <td>1.0</td>\n","      <td>0.069067</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>540.000000</td>\n","      <td>4.000000</td>\n","      <td>9.370074</td>\n","      <td>1.0</td>\n","      <td>0.238630</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              key        fold       score  best_iteration  train_time\n","count  275.000000  275.000000  275.000000           275.0  275.000000\n","mean   270.000000    2.000000    6.077108             1.0    0.069578\n","std    159.034496    1.416792    1.030951             0.0    0.014693\n","min      0.000000    0.000000    4.565445             1.0    0.060244\n","25%    130.000000    1.000000    5.443119             1.0    0.063557\n","50%    270.000000    2.000000    5.831177             1.0    0.065558\n","75%    410.000000    3.000000    6.242689             1.0    0.069067\n","max    540.000000    4.000000    9.370074             1.0    0.238630"]},"execution_count":140,"metadata":{},"output_type":"execute_result"}],"source":["# Check model quality\n","data = []\n","\n","for key, i_models in model_dict.items():\n","    for model in i_models:\n","        score = model.score\n","        best_iteration = model.best_iteration\n","        fold = model.fold\n","        train_time = model.train_time\n","        data.append({\"key\": key, \"fold\": fold, \"score\": score, \"best_iteration\": best_iteration, \"train_time\": train_time})\n","\n","df_model = pd.DataFrame(data)\n","df_model.describe()"]},{"cell_type":"code","execution_count":141,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>wap_vix_5</th>\n","      <td>7.065455</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_1</th>\n","      <td>6.425455</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price_wap_reference_price_imb2</th>\n","      <td>6.345455</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_shift_10</th>\n","      <td>5.785455</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_2</th>\n","      <td>5.778182</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_shift_3</th>\n","      <td>0.629091</td>\n","    </tr>\n","    <tr>\n","      <th>price_spread</th>\n","      <td>0.421818</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_shift_1</th>\n","      <td>0.287273</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_shift_2</th>\n","      <td>0.192727</td>\n","    </tr>\n","    <tr>\n","      <th>seconds_in_bucket</th>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>66 rows × 1 columns</p>\n","</div>"],"text/plain":["                                   importance\n","wap_vix_5                            7.065455\n","wap_diff_1                           6.425455\n","bid_price_wap_reference_price_imb2   6.345455\n","reference_price_shift_10             5.785455\n","wap_diff_2                           5.778182\n","...                                       ...\n","matched_size_shift_3                 0.629091\n","price_spread                         0.421818\n","matched_size_shift_1                 0.287273\n","matched_size_shift_2                 0.192727\n","seconds_in_bucket                         0.0\n","\n","[66 rows x 1 columns]"]},"execution_count":141,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize an empty DataFrame for aggregated importances\n","aggregated_importance = pd.DataFrame(index=feature_name, columns=['importance'])\n","\n","# Aggregate the importances from each model\n","for key, i_models in model_dict.items():\n","    for model in i_models:\n","        importance = pd.DataFrame({'feature': feature_name, 'importance': model.feature_importance})\n","        aggregated_importance = aggregated_importance.add(importance.set_index('feature'), fill_value=0)\n","\n","aggregated_importance['importance'] /= len(df_model)\n","\n","#pd_display_max()\n","# Sort the features by importance\n","aggregated_importance = aggregated_importance.sort_values(by='importance', ascending=False)\n","aggregated_importance"]},{"cell_type":"code","execution_count":142,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RAM memory GB usage = 1.827\n"]}],"source":["# Clean up\n","del key_models\n","if IS_USE_SAVED_MODEL:\n","    print(\"Delete model_dict\")\n","    del model_dict\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Infer"]},{"cell_type":"code","execution_count":143,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:31:37.373993Z","iopub.status.busy":"2023-11-23T00:31:37.373688Z","iopub.status.idle":"2023-11-23T00:31:38.383908Z","shell.execute_reply":"2023-11-23T00:31:38.382989Z","shell.execute_reply.started":"2023-11-23T00:31:37.373968Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Infer Local\n","------- counter 0 start -------\n","Memory usage of cache is 0.02 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 55.96%\n","cache len 200\n","Memory usage of features is 0.11 MB\n","Memory usage after optimization is: 0.09 MB\n","Decreased by 16.59%\n","df_feat len 200\n","Predict: 478, 0\n","Predictor target models len 5\n","------- counter 1 start -------\n","Memory usage of cache is 0.18 MB\n","Memory usage after optimization is: 0.14 MB\n","Decreased by 22.59%\n","cache len 400\n","Memory usage of features is 0.22 MB\n","Memory usage after optimization is: 0.19 MB\n","Decreased by 16.60%\n","df_feat len 200\n","Predict: 478, 10\n","Predictor target models len 5\n","------- counter 2 start -------\n","Memory usage of cache is 0.26 MB\n","Memory usage after optimization is: 0.20 MB\n","Decreased by 22.60%\n","cache len 600\n","Memory usage of features is 0.33 MB\n","Memory usage after optimization is: 0.28 MB\n","Decreased by 16.60%\n","df_feat len 200\n","Predict: 478, 20\n","Predictor target models len 5\n","------- counter 3 start -------\n","Memory usage of cache is 0.35 MB\n","Memory usage after optimization is: 0.27 MB\n","Decreased by 22.60%\n","cache len 800\n","Memory usage of features is 0.45 MB\n","Memory usage after optimization is: 0.37 MB\n","Decreased by 16.60%\n","df_feat len 200\n","Predict: 478, 30\n","Predictor target models len 5\n","------- counter 4 start -------\n","Memory usage of cache is 0.44 MB\n","Memory usage after optimization is: 0.34 MB\n","Decreased by 22.60%\n","cache len 1000\n","Memory usage of features is 0.56 MB\n","Memory usage after optimization is: 0.46 MB\n","Decreased by 16.61%\n","df_feat len 200\n","Predict: 478, 40\n","Predictor target models len 5\n","------- counter 5 start -------\n","Memory usage of cache is 0.53 MB\n","Memory usage after optimization is: 0.41 MB\n","Decreased by 22.60%\n","cache len 1200\n","Memory usage of features is 0.67 MB\n","Memory usage after optimization is: 0.56 MB\n","Decreased by 16.61%\n","df_feat len 200\n","Predict: 478, 50\n","Predictor target models len 5\n","------- counter 6 start -------\n","Memory usage of cache is 0.61 MB\n","Memory usage after optimization is: 0.48 MB\n","Decreased by 22.60%\n","cache len 1400\n","Memory usage of features is 0.78 MB\n","Memory usage after optimization is: 0.65 MB\n","Decreased by 16.61%\n","df_feat len 200\n","Predict: 478, 60\n","Predictor target models len 5\n","------- counter 7 start -------\n","Memory usage of cache is 0.70 MB\n","Memory usage after optimization is: 0.54 MB\n","Decreased by 22.60%\n","cache len 1600\n","Memory usage of features is 0.89 MB\n","Memory usage after optimization is: 0.74 MB\n","Decreased by 16.61%\n","df_feat len 200\n","Predict: 478, 70\n","Predictor target models len 5\n","------- counter 8 start -------\n","Memory usage of cache is 0.79 MB\n","Memory usage after optimization is: 0.61 MB\n","Decreased by 22.61%\n","cache len 1800\n","Memory usage of features is 1.00 MB\n","Memory usage after optimization is: 0.84 MB\n","Decreased by 16.61%\n","df_feat len 200\n","Predict: 478, 80\n","Predictor target models len 5\n","------- counter 9 start -------\n","Memory usage of cache is 0.88 MB\n","Memory usage after optimization is: 0.68 MB\n","Decreased by 22.61%\n","cache len 2000\n","Memory usage of features is 1.11 MB\n","Memory usage after optimization is: 0.93 MB\n","Decreased by 16.61%\n","df_feat len 200\n","Predict: 478, 90\n","Predictor target models len 5\n","------- counter 10 start -------\n","Memory usage of cache is 0.97 MB\n","Memory usage after optimization is: 0.75 MB\n","Decreased by 22.61%\n","cache len 2200\n","Memory usage of features is 1.23 MB\n","Memory usage after optimization is: 1.02 MB\n","Decreased by 16.61%\n","df_feat len 200\n","Predict: 478, 100\n","Predictor target models len 5\n","------- counter 11 start -------\n","Memory usage of cache is 1.05 MB\n","Memory usage after optimization is: 0.81 MB\n","Decreased by 22.61%\n","cache len 2400\n","Memory usage of features is 1.34 MB\n","Memory usage after optimization is: 1.11 MB\n","Decreased by 16.61%\n","df_feat len 200\n","Predict: 478, 110\n","Predictor target models len 5\n","------- counter 12 start -------\n","Memory usage of cache is 1.14 MB\n","Memory usage after optimization is: 0.88 MB\n","Decreased by 22.61%\n","cache len 2600\n","Memory usage of features is 1.45 MB\n","Memory usage after optimization is: 1.21 MB\n","Decreased by 16.61%\n","df_feat len 200\n","Predict: 478, 120\n","Predictor target models len 5\n","------- counter 13 start -------\n","Memory usage of cache is 1.23 MB\n","Memory usage after optimization is: 0.95 MB\n","Decreased by 22.39%\n","cache len 2800\n","Memory usage of features is 1.57 MB\n","Memory usage after optimization is: 1.30 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 130\n","Predictor target models len 5\n","------- counter 14 start -------\n","Memory usage of cache is 1.32 MB\n","Memory usage after optimization is: 1.02 MB\n","Decreased by 22.39%\n","cache len 3000\n","Memory usage of features is 1.68 MB\n","Memory usage after optimization is: 1.40 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 140\n","Predictor target models len 5\n","------- counter 15 start -------\n","Memory usage of cache is 1.40 MB\n","Memory usage after optimization is: 1.09 MB\n","Decreased by 22.39%\n","cache len 3200\n","Memory usage of features is 1.79 MB\n","Memory usage after optimization is: 1.49 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 150\n","Predictor target models len 5\n","------- counter 16 start -------\n","Memory usage of cache is 1.49 MB\n","Memory usage after optimization is: 1.16 MB\n","Decreased by 22.39%\n","cache len 3400\n","Memory usage of features is 1.90 MB\n","Memory usage after optimization is: 1.58 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 160\n","Predictor target models len 5\n","------- counter 17 start -------\n","Memory usage of cache is 1.58 MB\n","Memory usage after optimization is: 1.23 MB\n","Decreased by 22.39%\n","cache len 3600\n","Memory usage of features is 2.02 MB\n","Memory usage after optimization is: 1.68 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 170\n","Predictor target models len 5\n","------- counter 18 start -------\n","Memory usage of cache is 1.67 MB\n","Memory usage after optimization is: 1.29 MB\n","Decreased by 22.39%\n","cache len 3800\n","Memory usage of features is 2.13 MB\n","Memory usage after optimization is: 1.77 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 180\n","Predictor target models len 5\n","------- counter 19 start -------\n","Memory usage of cache is 1.75 MB\n","Memory usage after optimization is: 1.36 MB\n","Decreased by 22.39%\n","cache len 4000\n","Memory usage of features is 2.24 MB\n","Memory usage after optimization is: 1.86 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 190\n","Predictor target models len 5\n","------- counter 20 start -------\n","Memory usage of cache is 1.84 MB\n","Memory usage after optimization is: 1.43 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 200\n","Predictor target models len 5\n","------- counter 21 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 210\n","Predictor target models len 5\n","------- counter 22 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 220\n","Predictor target models len 5\n","------- counter 23 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 230\n","Predictor target models len 5\n","------- counter 24 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 240\n","Predictor target models len 5\n","------- counter 25 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 250\n","Predictor target models len 5\n","------- counter 26 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 260\n","Predictor target models len 5\n","------- counter 27 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 270\n","Predictor target models len 5\n","------- counter 28 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 280\n","Predictor target models len 5\n","------- counter 29 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 290\n","Predictor target models len 5\n","------- counter 30 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 300\n","Predictor target models len 5\n","------- counter 31 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 310\n","Predictor target models len 5\n","------- counter 32 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 320\n","Predictor target models len 5\n","------- counter 33 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 330\n","Predictor target models len 5\n","------- counter 34 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 340\n","Predictor target models len 5\n","------- counter 35 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 350\n","Predictor target models len 5\n","------- counter 36 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 360\n","Predictor target models len 5\n","------- counter 37 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 370\n","Predictor target models len 5\n","------- counter 38 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 380\n","Predictor target models len 5\n","------- counter 39 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 390\n","Predictor target models len 5\n","------- counter 40 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 400\n","Predictor target models len 5\n","------- counter 41 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 410\n","Predictor target models len 5\n","------- counter 42 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 420\n","Predictor target models len 5\n","------- counter 43 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 430\n","Predictor target models len 5\n","------- counter 44 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 440\n","Predictor target models len 5\n","------- counter 45 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 450\n","Predictor target models len 5\n","------- counter 46 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 460\n","Predictor target models len 5\n","------- counter 47 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 470\n","Predictor target models len 5\n","------- counter 48 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 480\n","Predictor target models len 5\n","------- counter 49 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 490\n","Predictor target models len 5\n","------- counter 50 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 500\n","Predictor target models len 5\n","------- counter 51 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 510\n","Predictor target models len 5\n","------- counter 52 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 520\n","Predictor target models len 5\n","------- counter 53 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 530\n","Predictor target models len 5\n","------- counter 54 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 478, 540\n","Predictor target models len 5\n","------- counter 55 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 0\n","Predictor target models len 5\n","------- counter 56 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 10\n","Predictor target models len 5\n","------- counter 57 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 20\n","Predictor target models len 5\n","------- counter 58 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 30\n","Predictor target models len 5\n","------- counter 59 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 40\n","Predictor target models len 5\n","------- counter 60 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 50\n","Predictor target models len 5\n","------- counter 61 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 60\n","Predictor target models len 5\n","------- counter 62 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 70\n","Predictor target models len 5\n","------- counter 63 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 80\n","Predictor target models len 5\n","------- counter 64 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 90\n","Predictor target models len 5\n","------- counter 65 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 100\n","Predictor target models len 5\n","------- counter 66 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 110\n","Predictor target models len 5\n","------- counter 67 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 120\n","Predictor target models len 5\n","------- counter 68 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 130\n","Predictor target models len 5\n","------- counter 69 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 140\n","Predictor target models len 5\n","------- counter 70 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 150\n","Predictor target models len 5\n","------- counter 71 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 160\n","Predictor target models len 5\n","------- counter 72 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 170\n","Predictor target models len 5\n","------- counter 73 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 180\n","Predictor target models len 5\n","------- counter 74 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 190\n","Predictor target models len 5\n","------- counter 75 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 200\n","Predictor target models len 5\n","------- counter 76 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 210\n","Predictor target models len 5\n","------- counter 77 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 220\n","Predictor target models len 5\n","------- counter 78 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 230\n","Predictor target models len 5\n","------- counter 79 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 240\n","Predictor target models len 5\n","------- counter 80 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 250\n","Predictor target models len 5\n","------- counter 81 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 260\n","Predictor target models len 5\n","------- counter 82 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 270\n","Predictor target models len 5\n","------- counter 83 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 280\n","Predictor target models len 5\n","------- counter 84 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 290\n","Predictor target models len 5\n","------- counter 85 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 300\n","Predictor target models len 5\n","------- counter 86 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 310\n","Predictor target models len 5\n","------- counter 87 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 320\n","Predictor target models len 5\n","------- counter 88 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 330\n","Predictor target models len 5\n","------- counter 89 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 340\n","Predictor target models len 5\n","------- counter 90 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 350\n","Predictor target models len 5\n","------- counter 91 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 360\n","Predictor target models len 5\n","------- counter 92 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 370\n","Predictor target models len 5\n","------- counter 93 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 380\n","Predictor target models len 5\n","------- counter 94 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 390\n","Predictor target models len 5\n","------- counter 95 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 400\n","Predictor target models len 5\n","------- counter 96 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 410\n","Predictor target models len 5\n","------- counter 97 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 420\n","Predictor target models len 5\n","------- counter 98 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 430\n","Predictor target models len 5\n","------- counter 99 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 440\n","Predictor target models len 5\n","------- counter 100 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 450\n","Predictor target models len 5\n","------- counter 101 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 460\n","Predictor target models len 5\n","------- counter 102 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 470\n","Predictor target models len 5\n","------- counter 103 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 480\n","Predictor target models len 5\n","------- counter 104 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 490\n","Predictor target models len 5\n","------- counter 105 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 500\n","Predictor target models len 5\n","------- counter 106 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 510\n","Predictor target models len 5\n","------- counter 107 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 520\n","Predictor target models len 5\n","------- counter 108 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 530\n","Predictor target models len 5\n","------- counter 109 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 479, 540\n","Predictor target models len 5\n","------- counter 110 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 0\n","Predictor target models len 5\n","------- counter 111 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 10\n","Predictor target models len 5\n","------- counter 112 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 20\n","Predictor target models len 5\n","------- counter 113 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 30\n","Predictor target models len 5\n","------- counter 114 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 40\n","Predictor target models len 5\n","------- counter 115 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 50\n","Predictor target models len 5\n","------- counter 116 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 60\n","Predictor target models len 5\n","------- counter 117 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 70\n","Predictor target models len 5\n","------- counter 118 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 80\n","Predictor target models len 5\n","------- counter 119 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 90\n","Predictor target models len 5\n","------- counter 120 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 100\n","Predictor target models len 5\n","------- counter 121 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 110\n","Predictor target models len 5\n","------- counter 122 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 120\n","Predictor target models len 5\n","------- counter 123 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 130\n","Predictor target models len 5\n","------- counter 124 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 140\n","Predictor target models len 5\n","------- counter 125 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 150\n","Predictor target models len 5\n","------- counter 126 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 160\n","Predictor target models len 5\n","------- counter 127 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 170\n","Predictor target models len 5\n","------- counter 128 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 180\n","Predictor target models len 5\n","------- counter 129 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 190\n","Predictor target models len 5\n","------- counter 130 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 200\n","Predictor target models len 5\n","------- counter 131 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 210\n","Predictor target models len 5\n","------- counter 132 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 220\n","Predictor target models len 5\n","------- counter 133 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 230\n","Predictor target models len 5\n","------- counter 134 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 240\n","Predictor target models len 5\n","------- counter 135 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 250\n","Predictor target models len 5\n","------- counter 136 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 260\n","Predictor target models len 5\n","------- counter 137 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 270\n","Predictor target models len 5\n","------- counter 138 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 280\n","Predictor target models len 5\n","------- counter 139 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 290\n","Predictor target models len 5\n","------- counter 140 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 300\n","Predictor target models len 5\n","------- counter 141 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 310\n","Predictor target models len 5\n","------- counter 142 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 320\n","Predictor target models len 5\n","------- counter 143 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 330\n","Predictor target models len 5\n","------- counter 144 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 340\n","Predictor target models len 5\n","------- counter 145 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 350\n","Predictor target models len 5\n","------- counter 146 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 360\n","Predictor target models len 5\n","------- counter 147 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 370\n","Predictor target models len 5\n","------- counter 148 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 380\n","Predictor target models len 5\n","------- counter 149 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 390\n","Predictor target models len 5\n","------- counter 150 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 400\n","Predictor target models len 5\n","------- counter 151 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 410\n","Predictor target models len 5\n","------- counter 152 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 420\n","Predictor target models len 5\n","------- counter 153 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 430\n","Predictor target models len 5\n","------- counter 154 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 440\n","Predictor target models len 5\n","------- counter 155 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 450\n","Predictor target models len 5\n","------- counter 156 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 460\n","Predictor target models len 5\n","------- counter 157 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 470\n","Predictor target models len 5\n","------- counter 158 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 480\n","Predictor target models len 5\n","------- counter 159 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 490\n","Predictor target models len 5\n","------- counter 160 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 500\n","Predictor target models len 5\n","------- counter 161 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 510\n","Predictor target models len 5\n","------- counter 162 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 520\n","Predictor target models len 5\n","------- counter 163 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 530\n","Predictor target models len 5\n","------- counter 164 start -------\n","Memory usage of cache is 1.93 MB\n","Memory usage after optimization is: 1.50 MB\n","Decreased by 22.39%\n","cache len 4200\n","Memory usage of features is 2.35 MB\n","Memory usage after optimization is: 1.95 MB\n","Decreased by 16.86%\n","df_feat len 200\n","Predict: 480, 540\n","Predictor target models len 5\n","RAM memory GB usage = 0.876\n","CPU times: user 2min 16s, sys: 1.52 s, total: 2min 17s\n","Wall time: 2min 17s\n"]}],"source":["%%time\n","\n","# 📉 Define a function to adjust prices based on volumes\n","def zero_sum(prices, volumes):\n","    std_error = np.sqrt(volumes)  # 🧮 Calculate standard error based on volumes\n","    step = np.sum(prices) / np.sum(std_error)  # 🧮 Calculate the step size based on prices and standard error\n","    out = prices - std_error * step  # 💰 Adjust prices by subtracting the standardized step size\n","    return out\n","\n","def model_infer(key, df_feat):\n","    def predictor(models):\n","        print(f\"Predictor target models len {len(models)}\")\n","        #print(f\"Predictor Feat len {len(df_feat)}\")\n","        predictions = np.zeros(len(df_feat))\n","        predictions = np.mean([model.predict(df_feat[feature_name]) for model in models], 0)\n","        return predictions\n","    \n","    if IS_USE_SAVED_MODEL:\n","        model_paths = model_dict_saved[key]\n","        models = [lgb.Booster(model_file=model_path) for model_path in model_paths]\n","        predictions = predictor(models)\n","        del models\n","    else:\n","        models = [m.model for m in model_dict[key]]\n","        predictions = predictor(models)\n","    collect()\n","    return predictions\n","\n","\n","y_min, y_max = -64, 64\n","predictions = []\n","cache = pd.DataFrame()\n","counter = 0\n","\n","if IS_INFER:\n","    if IS_LOCAL:\n","        print(\"Infer Local\")\n","        env = make_env()\n","    else:\n","        print(\"Infer Submission\")\n","        import optiver2023\n","        env = optiver2023.make_env()\n","    iter_test = env.iter_test()\n","\n","    for (test, revealed_targets, sample_prediction) in iter_test:\n","        now_time = time.time()\n","        print(f\"------- counter {counter} start -------\")\n","\n","        # It faults due to test is iterator\n","        #seconds_in_bucket = test['seconds_in_bucket'][0]\n","        #print(f\"prdict: {test['date_id'][0]}, {seconds_in_bucket}\")\n","        \n","        # Generate cahce\n","        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n","        cache = reduce_mem_usage(cache, 'cache')\n","        if counter > 0:\n","            # 🔄 If not the first iteration, limit the cache to the last 21 rows for each stock\n","            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n","        print(f\"cache len {len(cache)}\")\n","\n","        # 📊 Generate features\n","        df_test, generated_feature_name = generate_all_features(cache, False)\n","        df_test = df_test[-len(test):].reset_index(drop=True)\n","        df_feat = df_test[feature_name]\n","        print(f\"df_feat len {len(df_feat)}\")\n","\n","        # Get seconds_in_bucket and date_id\n","        seconds_in_bucket = df_test['seconds_in_bucket'][0]\n","        date_id = df_test['date_id'][0]\n","        print(f\"Predict: {date_id}, {seconds_in_bucket}\")\n","        \n","        # Predict\n","        predictions = model_infer(seconds_in_bucket, df_feat)\n","\n","        # Adjust the predictions based on the order book imbalance\n","        zerosum_predictions = zero_sum(predictions, test['bid_size'] + test['ask_size'])\n","        clipped_predictions = np.clip(zerosum_predictions, y_min, y_max)\n","        clipped_predictions.replace([np.nan, np.inf, -np.inf], 0, inplace=True)\n","\n","        # Submit\n","        sample_prediction['target'] = clipped_predictions\n","        env.predict(sample_prediction)\n","        \n","        # Clean up\n","        del df_feat, df_test,predictions, zerosum_predictions, clipped_predictions\n","        collect()\n","        counter += 1\n","\n","collect()\n","print(GetMemUsage())"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7056235,"sourceId":57891,"sourceType":"competition"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
