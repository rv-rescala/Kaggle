{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":57891,"databundleVersionId":6640818,"sourceType":"competition"}],"dockerImageVersionId":30553,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Enhanced Explained singel model\nThis origin comming from https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model/notebook","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport os\nimport warnings\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport gc  # Garbage collection for memory management\nimport os  # Operating system-related functions\nimport time  # Time-related functions\nimport warnings  # Handling warnings\nfrom itertools import combinations  # For creating combinations of elements\nfrom warnings import simplefilter  # Simplifying warning handling\n\n# ğŸ“¦ Importing machine learning libraries\nimport joblib  # For saving and loading models\nimport lightgbm as lgb  # LightGBM gradient boosting framework\nimport numpy as np  # Numerical operations\nimport pandas as pd  # Data manipulation and analysis\nfrom sklearn.metrics import mean_absolute_error  # Metric for evaluation\nfrom sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n\n# ğŸ¤ Disable warnings to keep the code clean\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n\nmax_lookback = np.nan  # Maximum lookback (not specified)\nsplit_day = 435  # Split day for time series data\n\nwarnings.filterwarnings(\"ignore\")\n\nPREV_MAX = 80\nIS_DEBUG = True\n# For kaggle environment\nif os.environ.get(\"KAGGLE_DATA_PROXY_TOKEN\") != None:\n    BASE_OUTPUT_PATH = Path(f'/kaggle/working')\n    BASE_INPUT_PATH = Path(f'/kaggle/input/optiver-trading-at-the-close')\n    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n    TEST_FILE = Path(f'{BASE_INPUT_PATH}/test.csv')\n    IS_LOCAL = False\n    IS_OFFLINE = False\n    IS_TRAIN = True\n    IS_INFER = True\n    DEVICE = \"gpu\"\n\n# For local environment\nelse:\n    BASE_OUTPUT_PATH = Path(f'../output')\n    BASE_INPUT_PATH = Path(f'../kaggle/input/optiver-trading-at-the-close')\n    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n    TEST_FILE = Path(f'{BASE_INPUT_PATH}/test.csv')\n    SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/sample_submission.csv')\n    REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/revealed_targets.csv')\n    IS_LOCAL = True\n    IS_OFFLINE = False\n    IS_TRAIN = True\n    IS_INFER = True\n    DEVICE = \"cpu\"\n\nprint(f\"BASE_OUTPUT_PATH: {BASE_OUTPUT_PATH}\")\nprint(f\"BASE_INPUT_PATH: {BASE_INPUT_PATH}\")\nprint(f\"TRAIN_FILE: {TRAIN_FILE}\")\nprint(f\"TEST_FILE: {TEST_FILE}\")\nprint(f\"IS_OFFLINE: {IS_OFFLINE}\")\nprint(f\"IS_LOCAL: {IS_LOCAL}\")\nprint(f\"IS_TRAIN: {IS_TRAIN}\")\nprint(f\"IS_INFER: {IS_INFER}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:53:12.128562Z","iopub.execute_input":"2023-11-14T23:53:12.129567Z","iopub.status.idle":"2023-11-14T23:53:12.143728Z","shell.execute_reply.started":"2023-11-14T23:53:12.129529Z","shell.execute_reply":"2023-11-14T23:53:12.142617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions","metadata":{}},{"cell_type":"code","source":"from typing import Sequence, Tuple\n\nimport pandas as pd\n\n# for local execution\nclass MockApi:\n    def __init__(self):\n        '''\n        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n        They've been intentionally left in an invalid state.\n\n        Variables to set:\n            input_paths: a list of two or more paths to the csv files to be served\n            group_id_column: the column that identifies which groups of rows the API should serve.\n                A call to iter_test serves all rows of all dataframes with the current group ID value.\n            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n        '''\n        self.input_paths: Sequence[str] = [TEST_FILE, REVEALED_TARGETS_FILE, SAMPLE_SUBMISSION_FILE]\n        self.group_id_column: str = 'time_id'\n        self.export_group_id_column: bool = True\n        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n        assert len(self.input_paths) >= 2\n\n        self._status = 'initialized'\n        self.predictions = []\n\n    def iter_test(self) -> Tuple[pd.DataFrame]:\n        '''\n        Loads all of the dataframes specified in self.input_paths,\n        then yields all rows in those dataframes that equal the current self.group_id_column value.\n        '''\n        if self._status != 'initialized':\n\n            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n\n        dataframes = []\n        for pth in self.input_paths:\n            dataframes.append(pd.read_csv(pth, low_memory=False))\n        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n\n        for group_id in group_order:\n            self._status = 'prediction_needed'\n            current_data = []\n            for df in dataframes:\n                cur_df = df.loc[group_id].copy()\n                # returning single line dataframes from df.loc requires special handling\n                if not isinstance(cur_df, pd.DataFrame):\n                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n                    cur_df.index.name = self.group_id_column\n                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n                current_data.append(cur_df)\n            yield tuple(current_data)\n\n            while self._status != 'prediction_received':\n                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n                yield None\n\n        with open('submission.csv', 'w') as f_open:\n            pd.concat(self.predictions).to_csv(f_open, index=False)\n        self._status = 'finished'\n\n    def predict(self, user_predictions: pd.DataFrame):\n        '''\n        Accepts and stores the user's predictions and unlocks iter_test once that is done\n        '''\n        if self._status == 'finished':\n            raise Exception('You have already made predictions for the full test set.')\n        if self._status != 'prediction_needed':\n            raise Exception('You must get the next test sample from `iter_test()` first.')\n        if not isinstance(user_predictions, pd.DataFrame):\n            raise Exception('You must provide a DataFrame.')\n\n        self.predictions.append(user_predictions)\n        self._status = 'prediction_received'\n\n\ndef make_env():\n    return MockApi()","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:53:12.145975Z","iopub.execute_input":"2023-11-14T23:53:12.146403Z","iopub.status.idle":"2023-11-14T23:53:12.165555Z","shell.execute_reply.started":"2023-11-14T23:53:12.146334Z","shell.execute_reply":"2023-11-14T23:53:12.164563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ğŸ§¹ Function to reduce memory usage of a Pandas DataFrame\ndef reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    \n    # ğŸ“ Calculate the initial memory usage of the DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    # ğŸ”„ Iterate through each column in the DataFrame\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        # Check if the column's data type is not 'object' (i.e., numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # Check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # Check if the column's data type is a float\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n\n    # â„¹ï¸ Provide memory optimization information if 'verbose' is True\n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    # ğŸ”„ Return the DataFrame with optimized memory usage\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:53:12.166781Z","iopub.execute_input":"2023-11-14T23:53:12.167111Z","iopub.status.idle":"2023-11-14T23:53:12.182784Z","shell.execute_reply.started":"2023-11-14T23:53:12.167086Z","shell.execute_reply":"2023-11-14T23:53:12.181757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ğŸï¸ Import Numba for just-in-time (JIT) compilation and parallel processing\nfrom numba import njit, prange\n\n# ğŸ“Š Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # ğŸ” Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # ğŸ” Loop through rows of the DataFrame\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            \n            # ğŸš« Prevent division by zero\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\n# ğŸ“ˆ Function to calculate triplet imbalance for given price data and a DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance using the Numba-optimized function\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:53:12.317417Z","iopub.execute_input":"2023-11-14T23:53:12.317830Z","iopub.status.idle":"2023-11-14T23:53:12.330719Z","shell.execute_reply.started":"2023-11-14T23:53:12.317797Z","shell.execute_reply":"2023-11-14T23:53:12.329850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ğŸ“Š Function to generate imbalance features\ndef imbalance_features(df):\n    import cudf\n    df = cudf.from_pandas(df)\n    \n    # Define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n\n    # V1 features\n    # Calculate various features using Pandas eval function\n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    # Create features for pairwise price imbalances\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n        \n    # V2 features\n    # Calculate additional features\n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    \n    # Calculate various statistical aggregation features\n    \n        \n    # V3 features\n    # Calculate shifted and return features for specific columns\n    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n        for window in [1, 2, 3, 10]:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n    \n    # Calculate diff features for specific columns\n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n        for window in [1, 2, 3, 10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n    df = df.to_pandas()\n    # Replace infinite values with 0\n    return df.replace([np.inf, -np.inf], 0)\n\ndef numba_imb_features(df):\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n    # Calculate triplet imbalance features using the Numba-optimized function\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    return df\n\n# ğŸ“… Function to generate time and stock-related features\ndef other_features(df):\n    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n\n    # Map global features to the DataFrame\n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    return df\n\n# ğŸ“ˆ Function to generate normalized features\ndef generate_normalized_features(df, is_train):\n    print(\"generate_normalized_features\")\n    df['match_balance'] = ( df['matched_size']  + (df['imbalance_buy_sell_flag'] * df['imbalance_size'])) / df['matched_size']\n    if is_train:\n        global_population_wap = df['wap'].describe()\n        global_population_mathch_balance = df['match_balance'].describe()\n    df['normalized_wap'] = (df['wap'] - global_population_wap['mean']) / global_population_wap['std']\n    df['normalized_match_balance'] = (df['match_balance'] - global_population_mathch_balance['mean']) / global_population_mathch_balance['std']\n    return df\n\ndef cal_vix(df, group_key, target_col, period):\n    return df.groupby(group_key)[target_col].transform(lambda x: np.log(x).diff().rolling(period).std())\n\ndef generate_historical_features(df, is_train):\n    print(\"generate_historical_features\")\n\n    for col in ['wap', 'match_balance']:\n        for window in [1, 2, 3, 10]:\n            col_name = f\"{col}_diff_{window}\"\n            df[col_name] = df.groupby(\"stock_id\")[col].diff(window)\n            #df[col_name] = df[col_name].fillna(0)  # NaNã‚’0ã§ç½®ãæ›ãˆã‚‹\n        for period in [5]:\n            col_name = f\"{col}_vix_{period}\"\n            df[col_name] = cal_vix(df, ['stock_id', 'date_id'], col, period)\n\n    df = df.replace([np.inf, -np.inf], 0)\n    return df\n\n# ğŸš€ Function to generate all features by combining imbalance and other features\ndef generate_all_features(df, is_train):\n    # Select relevant columns for feature generation\n    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n    df = df[cols]\n    \n    # Generate imbalance features\n    df = imbalance_features(df)\n    df = numba_imb_features(df)\n    df = generate_normalized_features(df, is_train)\n    df = generate_historical_features(df, is_train)\n    # Generate time and stock-related features\n    df = other_features(df)\n    gc.collect()  # Perform garbage collection to free up memory\n    \n    # Select and return the generated features\n    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n    \n    return df[feature_name]","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:53:12.333148Z","iopub.execute_input":"2023-11-14T23:53:12.333545Z","iopub.status.idle":"2023-11-14T23:53:12.362782Z","shell.execute_reply.started":"2023-11-14T23:53:12.333513Z","shell.execute_reply":"2023-11-14T23:53:12.361652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generationg train dataset","metadata":{}},{"cell_type":"code","source":"def load_train_dataset():\n    df = pd.read_csv(TRAIN_FILE)\n    # ğŸ§¹ Remove rows with missing values in the \"target\" column\n    df = df.dropna(subset=[\"target\"])\n    # ğŸ” Reset the index of the DataFrame and apply the changes in place\n    df.reset_index(drop=True, inplace=True)\n    return df\n","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:53:12.364082Z","iopub.execute_input":"2023-11-14T23:53:12.364508Z","iopub.status.idle":"2023-11-14T23:53:12.378029Z","shell.execute_reply.started":"2023-11-14T23:53:12.364477Z","shell.execute_reply":"2023-11-14T23:53:12.376857Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if the code is running in offline or online mode\ndf_tmp_train = load_train_dataset()\nif IS_OFFLINE:\n    # In offline mode, split the data into training and validation sets based on the split_day\n    df_train = df_tmp_train[df_tmp_train[\"date_id\"] <= split_day]\n    df_valid = df_tmp_train[df_tmp_train[\"date_id\"] > split_day]\n    \n    # Display a message indicating offline mode and the shapes of the training and validation sets\n    print(\"Offline mode\")\n    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\nelse:\n    # In online mode, use the entire dataset for training\n    df_train = df_tmp_train\n    \n    # Display a message indicating online mode\n    print(\"Online mode\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:53:12.379229Z","iopub.execute_input":"2023-11-14T23:53:12.379569Z","iopub.status.idle":"2023-11-14T23:53:25.762298Z","shell.execute_reply.started":"2023-11-14T23:53:12.379535Z","shell.execute_reply":"2023-11-14T23:53:25.761087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if IS_TRAIN:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    }\n    if IS_OFFLINE:\n        df_train_feats = generate_all_features(df_train, True)\n        print(\"Build Train Feats Finished.\")\n        df_valid_feats = generate_all_features(df_valid, True)\n        print(\"Build Valid Feats Finished.\")\n        df_valid_feats = reduce_mem_usage(df_valid_feats, True)\n    else:\n        df_train_feats = generate_all_features(df_train, True)\n        print(\"Build Online Train Feats Finished.\")\n\n    df_train_feats = reduce_mem_usage(df_train_feats)","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:53:25.765487Z","iopub.execute_input":"2023-11-14T23:53:25.765830Z","iopub.status.idle":"2023-11-14T23:56:43.025328Z","shell.execute_reply.started":"2023-11-14T23:53:25.765788Z","shell.execute_reply":"2023-11-14T23:56:43.024483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## global_stock_id_feats","metadata":{}},{"cell_type":"code","source":"df_global_stock_id_feats = pd.DataFrame(global_stock_id_feats)\ndf_global_stock_id_feats.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:56:43.027023Z","iopub.execute_input":"2023-11-14T23:56:43.027499Z","iopub.status.idle":"2023-11-14T23:56:43.050203Z","shell.execute_reply.started":"2023-11-14T23:56:43.027461Z","shell.execute_reply":"2023-11-14T23:56:43.049042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## df_train_feats","metadata":{}},{"cell_type":"code","source":"df_train_feats.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:56:43.051869Z","iopub.execute_input":"2023-11-14T23:56:43.052165Z","iopub.status.idle":"2023-11-14T23:56:43.082286Z","shell.execute_reply.started":"2023-11-14T23:56:43.052139Z","shell.execute_reply":"2023-11-14T23:56:43.081006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"# ğŸ“¦ Import necessary libraries\nimport numpy as np\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_absolute_error\nimport gc\nimport os\n\n# ğŸ“Š Set LightGBM parameters\n\nlgb_params = {\n    \"objective\": \"mae\",\n    \"n_estimators\": 6000,\n    \"num_leaves\": 256,\n    \"subsample\": 0.6,\n    \"colsample_bytree\": 0.8,\n    \"learning_rate\": 0.00871,\n    'max_depth': 11,\n    \"n_jobs\": 4,\n    \"device\": DEVICE,\n    \"verbosity\": -1,\n    \"importance_type\": \"gain\",\n}\n\n# ğŸ“‹ Get feature names\nfeature_name = list(df_train_feats.columns)\nprint(f\"Feature length = {len(feature_name)}\")\nprint(f\"Features: {feature_name}\")\n\n# ğŸ”„ Set up cross-validation parameters\nnum_folds = 5\nfold_size = 480 // num_folds\ngap = 5\n\n# ğŸ“Š Initialize lists to store models and scores\nmodels = []\nscores = []\n\n# ğŸ’¾ Set model save path\nmodel_save_path = 'modelitos_para_despues' \nif not os.path.exists(model_save_path):\n    os.makedirs(model_save_path)\n\n# ğŸ“… Get date IDs from the training data\ndate_ids = df_train['date_id'].values\n\n# ğŸ”„ Loop over folds for cross-validation\nfor i in range(num_folds):\n    start = i * fold_size\n    end = start + fold_size\n    if i < num_folds - 1:  # No need to purge after the last fold\n        purged_start = end - 2\n        purged_end = end + gap + 2\n        train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n    else:\n        train_indices = (date_ids >= start) & (date_ids < end)\n\n    test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n\n    # ğŸ“Š Create fold-specific training and validation sets\n    df_fold_train = df_train_feats[train_indices]\n    df_fold_train_target = df_train['target'][train_indices]\n    df_fold_valid = df_train_feats[test_indices]\n    df_fold_valid_target = df_train['target'][test_indices]\n\n    print(f\"Fold {i+1} Model Training\")\n    print(f\"Start: {start}\")\n    print(f\"End: {end}\")\n    print(f\"train_indices: {train_indices}\")\n    print(f\"test_indices: {test_indices}\")\n\n    # ğŸš‚ Train a LightGBM model for the current fold\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(\n        df_fold_train[feature_name],\n        df_fold_train_target,\n        eval_set=[(df_fold_valid[feature_name], df_fold_valid_target)],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=100),\n            lgb.callback.log_evaluation(period=100),\n        ],\n    )\n\n    models.append(lgb_model)\n\n    # ğŸ’¾ Save the model to a file\n    model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n    lgb_model.booster_.save_model(model_filename)\n    print(f\"Model for fold {i+1} saved to {model_filename}\")\n\n    # ğŸ“Š Evaluate model performance on the validation set\n    fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n    scores.append(fold_score)\n    print(f\"Fold {i+1} MAE: {fold_score}\")\n\n    # ğŸ”„ Free up memory by deleting fold-specific variables\n    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n    gc.collect()\n\n# ğŸ“ˆ Calculate the average best iteration from all regular folds\naverage_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n\n# ğŸ”„ Update the lgb_params with the average best iteration\nfinal_model_params = lgb_params.copy()\nfinal_model_params['n_estimators'] = average_best_iteration\n\nprint(f\"Training final model with average best iteration: {average_best_iteration}\")\n\n# ğŸš‚ Train the final model on the entire dataset\nfinal_model = lgb.LGBMRegressor(**final_model_params)\nfinal_model.fit(\n    df_train_feats[feature_name],\n    df_train['target'],\n    callbacks=[\n        lgb.callback.log_evaluation(period=100),\n    ],\n)\n\n# ğŸ“„ Append the final model to the list of models\nmodels.append(final_model)\n\n# ğŸ’¾ Save the final model to a file\nfinal_model_filename = os.path.join(model_save_path, 'doblez-conjunto.txt')\nfinal_model.booster_.save_model(final_model_filename)\nprint(f\"Final model saved to {final_model_filename}\")\n\n# â„¹ï¸ Now 'models' holds the trained models for each fold and 'scores' holds the validation scores\nprint(f\"Average MAE across all folds: {np.mean(scores)}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-14T23:56:43.085852Z","iopub.execute_input":"2023-11-14T23:56:43.086168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Draw result\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer","metadata":{}},{"cell_type":"code","source":"# ğŸ“‰ Define a function to adjust prices based on volumes\ndef zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)  # ğŸ§® Calculate standard error based on volumes\n    step = np.sum(prices) / np.sum(std_error)  # ğŸ§® Calculate the step size based on prices and standard error\n    out = prices - std_error * step  # ğŸ’° Adjust prices by subtracting the standardized step size\n    return out\n\n# â„¹ï¸ If in inference mode, use the Optiver 2023 environment\nif IS_INFER:\n    if IS_OFFLINE:\n        env = make_env()\n    else:\n        import optiver2023\n        env = optiver2023.make_env()\n    iter_test = env.iter_test()\n    counter = 0\n    y_min, y_max = -64, 64\n    qps, predictions = [], []\n    cache = pd.DataFrame()\n\n    # ğŸš‚ Define weights for each fold model\n    model_weights = [1/len(models)] * len(models) \n    \n    # ğŸ”„ Loop through each test scenario\n    for (test, revealed_targets, sample_prediction) in iter_test:\n        now_time = time.time()\n        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n        \n        # ğŸ”„ If not the first iteration, limit the cache to the last 21 rows for each stock\n        if counter > 0:\n            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n        \n        # ğŸ“Š Generate features based on the updated cache\n        feat = generate_all_features(cache, False)[-len(test):]\n\n        # ğŸ“Š Generate predictions for each model and calculate the weighted average\n        lgb_predictions = np.zeros(len(test))\n        for model, weight in zip(models, model_weights):\n            lgb_predictions += weight * model.predict(feat)\n\n        # ğŸ§® Adjust predictions using the zero_sum function\n        lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n        clipped_predictions = np.clip(lgb_predictions, y_min, y_max)  # ğŸ“ Clip predictions within a specified range\n        sample_prediction['target'] = clipped_predictions\n        env.predict(sample_prediction)  # ğŸ“ˆ Submit predictions to the environment\n        counter += 1\n        qps.append(time.time() - now_time)\n        \n        # ğŸ”„ Print the average queries per second every 10 iterations\n        if counter % 10 == 0:\n            print(counter, 'qps:', np.mean(qps))\n\n    time_cost = 1.146 * np.mean(qps)\n    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}