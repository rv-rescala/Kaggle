{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce76a20c",
   "metadata": {
    "papermill": {
     "duration": 0.013915,
     "end_time": "2023-12-17T03:01:29.700259",
     "exception": false,
     "start_time": "2023-12-17T03:01:29.686344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Incremental Learning Model\n",
    "This origin comming from https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd2fb21",
   "metadata": {
    "papermill": {
     "duration": 0.013272,
     "end_time": "2023-12-17T03:01:29.726705",
     "exception": false,
     "start_time": "2023-12-17T03:01:29.713433",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030d3223",
   "metadata": {
    "papermill": {
     "duration": 0.012668,
     "end_time": "2023-12-17T03:01:29.752466",
     "exception": false,
     "start_time": "2023-12-17T03:01:29.739798",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e454eb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:01:29.780129Z",
     "iopub.status.busy": "2023-12-17T03:01:29.779782Z",
     "iopub.status.idle": "2023-12-17T03:01:43.625301Z",
     "shell.execute_reply": "2023-12-17T03:01:43.624332Z"
    },
    "papermill": {
     "duration": 13.862082,
     "end_time": "2023-12-17T03:01:43.627643",
     "exception": false,
     "start_time": "2023-12-17T03:01:29.765561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc  # Garbage collection for memory management\n",
    "import os  # Operating system-related functions\n",
    "import time  # Time-related functions\n",
    "import warnings  # Handling warnings\n",
    "from itertools import combinations  # For creating combinations of elements\n",
    "from warnings import simplefilter  # Simplifying warning handling\n",
    "import joblib  # For saving and loading models\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from numba import njit, prange  # Compiling Python code for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e368cbcc",
   "metadata": {
    "papermill": {
     "duration": 0.012737,
     "end_time": "2023-12-17T03:01:43.653888",
     "exception": false,
     "start_time": "2023-12-17T03:01:43.641151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b70936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:01:43.682563Z",
     "iopub.status.busy": "2023-12-17T03:01:43.681936Z",
     "iopub.status.idle": "2023-12-17T03:01:43.706040Z",
     "shell.execute_reply": "2023-12-17T03:01:43.705211Z"
    },
    "papermill": {
     "duration": 0.040806,
     "end_time": "2023-12-17T03:01:43.708123",
     "exception": false,
     "start_time": "2023-12-17T03:01:43.667317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_OUTPUT_PATH: /kaggle/working\n",
      "BASE_INPUT_PATH: /kaggle/input/optiver-trading-at-the-close\n",
      "TRAIN_FILE: /kaggle/input/optiver-trading-at-the-close/train.csv\n",
      "TEST_FILE: /kaggle/input/optiver-trading-at-the-close/example_test_files/test.csv\n",
      "IS_LOCAL: False\n",
      "IS_INFER: True\n",
      "IS_USE_SAVED_MODEL: False\n",
      "IS_MIN_LEARN: False\n",
      "USE_OPTUNA: False\n",
      "USE_CONTINUOUS_UPDATE: True\n",
      "USE_ALL_FEATUTES: True\n",
      "USE_REVEALED_TARGETS: True\n",
      "USE_INDEX: True\n"
     ]
    }
   ],
   "source": [
    "# Disable warnings to keep the code clean\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "seed = 2023\n",
    "DATA_COUNT_IN_SAME_BUCKET = 55 # 同じbucket内のデータ数\n",
    "\n",
    "# For kaggle environment\n",
    "if os.environ.get(\"KAGGLE_DATA_PROXY_TOKEN\") != None:\n",
    "    BASE_OUTPUT_PATH = Path(f'/kaggle/working')\n",
    "    BASE_INPUT_PATH = Path(f'/kaggle/input/optiver-trading-at-the-close')\n",
    "    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n",
    "    TEST_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/test.csv')\n",
    "    \n",
    "    IS_LOCAL = False # If kaggle environment, set False\n",
    "    IS_INFER = True # If kaggle environment, set True\n",
    "    IS_USE_SAVED_MODEL = False # Use saved model or not\n",
    "    IS_MIN_LEARN = False # Use min learning or not\n",
    "    USE_OPTUNA = False # Use optuna or not\n",
    "    USE_CONTINUOUS_UPDATE = True # Use test date on train or not\n",
    "    USE_ALL_FEATUTES = True # Use all features or not\n",
    "    USE_REVEALED_TARGETS = True # Use revealed targets or not\n",
    "    USE_INDEX = True # Use index or not\n",
    "    IS_DEBUG = True\n",
    "    USE_ADDITIONAL_TRAIN = True\n",
    "    NUM_THREADS = 4\n",
    "\n",
    "    SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/sample_submission.csv')\n",
    "    REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/revealed_targets.csv')\n",
    "\n",
    "    stopping_rounds = 100 # early_stopping用コールバック関数\n",
    "    num_boost_round = 5000 # 計算回数\n",
    "    update_num_boost_round = 5000 # 再学習の計算回数\n",
    "    num_folds = 5 # クロスバリデーションの分割数\n",
    "    continuos_dataset_span = 20 # DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span が更新の使用する対象のデータ\n",
    "    continuos_train_span = 3 # DATA_COUNT_IN_SAME_BUCKET * continuos_train_span が更新の頻度\n",
    "\n",
    "    DEVICE = 'gpu' # cpu or gpu\n",
    "    OPTUNA_TIME_BUDGET = 60 * 60 * 4 # 1 hours\n",
    "    TARGET_STOCK_IDS = [0, 1]\n",
    "\n",
    "    optuna_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',         # 回帰\n",
    "        'metric': 'rmse',                  # 損失（誤差）\n",
    "        'verbosity': -1,\n",
    "        'deterministic':True, #再現性確保用のパラメータ\n",
    "        'force_row_wise':True,  #再現性確保用のパラメータ\n",
    "        'device': DEVICE\n",
    "    }\n",
    "\n",
    "    # サンプルのパラメータ\n",
    "    \"\"\"\n",
    "    lgb_params = {\n",
    "        \"objective\": \"mae\",\n",
    "        \"n_estimators\": 5500,\n",
    "        \"num_leaves\": 128,\n",
    "        \"subsample\": 0.6,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"learning_rate\": 0.00005,\n",
    "        'max_depth': 11,\n",
    "        \"n_jobs\": 4,\n",
    "        \"device\": DEVICE,\n",
    "        \"verbosity\": -1,\n",
    "        \"importance_type\": \"gain\",\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression', \n",
    "        'metric': 'rmse', \n",
    "        'verbosity': -1, \n",
    "        'device': DEVICE,\n",
    "        'feature_pre_filter': False, \n",
    "        'lambda_l1': 0.0,\n",
    "        'lambda_l2': 0.0,\n",
    "        'num_leaves': 31, \n",
    "        'feature_fraction': 0.8, \n",
    "        'bagging_fraction': 1.0, \n",
    "        'bagging_freq': 0, \n",
    "        'min_child_samples': 20,\n",
    "        'seed': seed,                       # シード値\n",
    "    }\n",
    "\n",
    "    \n",
    "   # こっちのパラメータの方が、計算時間がかかる\n",
    "    \"\"\"\n",
    "    lgb_params = {\n",
    "        'task': 'train',                   # 学習\n",
    "        'objective': 'regression',                # 目的関数の種類。ここでは回帰タスクを指定\n",
    "        'metric': 'rmse',                          # 評価指標\n",
    "        'boosting_type': 'gbdt',                  # ブースティングタイプ。勾配ブースティング決定木\n",
    "        \"n_estimators\": 32,                        # ブースティングに使用する木の数。多いほど性能が向上するが計算コストが増加\n",
    "        \"num_leaves\": 64,                         # 木に存在する最大の葉の数。大きい値は精度を向上させるが過学習のリスクが増加\n",
    "        \"subsample\": 0.8,                         # 各木のトレーニングに使用されるデータの割合。過学習を防ぐために一部のデータをサンプリング\n",
    "        \"colsample_bytree\": 0.8,                  # 木を構築する際に使用される特徴の割合。特徴のサブセットを使用し過学習を防ぐ\n",
    "        \"learning_rate\": 0.01,                 # 学習率。小さい値は堅牢なモデルを生成するが収束に時間がかかる\n",
    "        'max_depth': 32,                           # 木の最大の深さ。深い木は複雑なモデルを作成するが過学習のリスクがある\n",
    "        \"device\": DEVICE,                         # トレーニングに使用するデバイス（CPUまたはGPU）\n",
    "        \"verbosity\": -1,                          # LightGBMのログ出力のレベル。-1はログを出力しないことを意味する\n",
    "       # \"importance_type\": \"gain\",                # 特徴重要度を計算する際の指標。\"gain\"は分割による平均情報利得\n",
    "        'lambda_l1': 0.5,                         # L1正則化項の係数。過学習を防ぐためにモデルの複雑さにペナルティを課す\n",
    "        'lambda_l2': 0.5,                         # L2正則化項の係数。同じく過学習を防ぐ\n",
    "        'bagging_freq': 5,                 # バギング実施頻度\n",
    "        'min_child_samples': 10,           # 葉に含まれる最小データ数\n",
    "        'seed': seed,                       # シード値\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "# For local environment\n",
    "else:\n",
    "    BASE_OUTPUT_PATH = Path(f'../output')\n",
    "    BASE_INPUT_PATH = Path(f'../kaggle/input/optiver-trading-at-the-close')\n",
    "    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n",
    "    TEST_FILE = Path(f'{BASE_INPUT_PATH}/test.csv')\n",
    "\n",
    "    SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/sample_submission.csv')\n",
    "    REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/revealed_targets.csv')\n",
    "\n",
    "    IS_LOCAL = True\n",
    "    IS_INFER = True\n",
    "    IS_USE_SAVED_MODEL = False # Use saved model or not\n",
    "    IS_MIN_LEARN = True\n",
    "    USE_OPTUNA = False # Use optuna or not\n",
    "    USE_ALL_FEATUTES = True # Use all features or not\n",
    "    USE_CONTINUOUS_UPDATE = True # Use test date on train or not\n",
    "    USE_REVEALED_TARGETS = True # Use revealed targets or not \n",
    "    USE_INDEX = True # Use index or not\n",
    "    IS_DEBUG = True\n",
    "    USE_ADDITIONAL_TRAIN = True\n",
    "    TARGET_STOCK_IDS = [0]\n",
    "    NUM_THREADS = 2\n",
    "\n",
    "    # For training\n",
    "    stopping_rounds = 1 # early_stopping用コールバック関数\n",
    "    num_boost_round = 1 # 計算回数\n",
    "    update_num_boost_round = 1\n",
    "    num_folds = 2 # クロスバリデーションの分割数\n",
    "    continuos_dataset_span = 3 # DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span が更新の使用する対象のデータ\n",
    "    continuos_train_span = 2 # DATA_COUNT_IN_SAME_BUCKET * continuos_train_span が更新の頻度\n",
    "\n",
    "    DEVICE = 'cpu' # cpu or gpu\n",
    "    OPTUNA_TIME_BUDGET = 60 # 1 min\n",
    "\n",
    "    optuna_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',         # 回帰\n",
    "        'metric': 'rmse',                  # 損失（誤差）\n",
    "        'verbosity': -1,\n",
    "        'deterministic':True, #再現性確保用のパラメータ\n",
    "        'force_row_wise':True,  #再現性確保用のパラメータ\n",
    "        'device': DEVICE\n",
    "    }\n",
    "\n",
    "    lgb_params = {\n",
    "        'task': 'train',                   # 学習\n",
    "        'boosting_type': 'gbdt',           # GBDT\n",
    "        'objective': 'regression',         # 回帰\n",
    "        'metric': 'rmse',                  # 損失（誤差）\n",
    "        'learning_rate': 0.01,             # 学習率\n",
    "        'lambda_l1': 0.5,                  # L1正則化項の係数\n",
    "        'lambda_l2': 0.5,                  # L2正則化項の係数\n",
    "        'num_leaves': 10,                  # 最大葉枚数\n",
    "        'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n",
    "        'bagging_fraction': 0.5,           # ランダムに抽出される標本の割合\n",
    "        'bagging_freq': 5,                 # バギング実施頻度\n",
    "        'min_child_samples': 10,           # 葉に含まれる最小データ数\n",
    "        'seed': seed,                       # シード値\n",
    "        \"device\": DEVICE,\n",
    "        'verbosity': -1\n",
    "    }\n",
    "\n",
    "\n",
    "print(f\"BASE_OUTPUT_PATH: {BASE_OUTPUT_PATH}\")\n",
    "print(f\"BASE_INPUT_PATH: {BASE_INPUT_PATH}\")\n",
    "print(f\"TRAIN_FILE: {TRAIN_FILE}\")\n",
    "print(f\"TEST_FILE: {TEST_FILE}\")\n",
    "print(f\"IS_LOCAL: {IS_LOCAL}\")\n",
    "print(f\"IS_INFER: {IS_INFER}\")\n",
    "print(f\"IS_USE_SAVED_MODEL: {IS_USE_SAVED_MODEL}\")\n",
    "print(f\"IS_MIN_LEARN: {IS_MIN_LEARN}\")\n",
    "print(f\"USE_OPTUNA: {USE_OPTUNA}\")\n",
    "print(f\"USE_CONTINUOUS_UPDATE: {USE_CONTINUOUS_UPDATE}\")\n",
    "print(f\"USE_ALL_FEATUTES: {USE_ALL_FEATUTES}\")\n",
    "print(f\"USE_REVEALED_TARGETS: {USE_REVEALED_TARGETS}\")\n",
    "print(f\"USE_INDEX: {USE_INDEX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29347f4",
   "metadata": {
    "papermill": {
     "duration": 0.013323,
     "end_time": "2023-12-17T03:01:43.734900",
     "exception": false,
     "start_time": "2023-12-17T03:01:43.721577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065e02ee",
   "metadata": {
    "papermill": {
     "duration": 0.012832,
     "end_time": "2023-12-17T03:01:43.760859",
     "exception": false,
     "start_time": "2023-12-17T03:01:43.748027",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Memory Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24317fca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:01:43.788164Z",
     "iopub.status.busy": "2023-12-17T03:01:43.787873Z",
     "iopub.status.idle": "2023-12-17T03:01:43.989585Z",
     "shell.execute_reply": "2023-12-17T03:01:43.988562Z"
    },
    "papermill": {
     "duration": 0.217726,
     "end_time": "2023-12-17T03:01:43.991551",
     "exception": false,
     "start_time": "2023-12-17T03:01:43.773825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory GB usage = 0.727\n",
      "CPU times: user 196 ms, sys: 999 µs, total: 197 ms\n",
      "Wall time: 195 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from gc import collect;\n",
    "from psutil import Process;\n",
    "from os import system, getpid, walk;\n",
    "\n",
    "# Defining global configurations and functions:-\n",
    "\n",
    "    \n",
    "def GetMemUsage():\n",
    "    \"\"\"\n",
    "    This function defines the memory usage across the kernel. \n",
    "    Source-\n",
    "    https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\n",
    "    \"\"\";\n",
    "    \n",
    "    pid = getpid();\n",
    "    py = Process(pid);\n",
    "    memory_use = py.memory_info()[0] / 2. ** 30;\n",
    "    return f\"RAM memory GB usage = {memory_use :.4}\";\n",
    "\n",
    "\n",
    "collect();\n",
    "print(GetMemUsage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93afe7c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:01:44.022357Z",
     "iopub.status.busy": "2023-12-17T03:01:44.022014Z",
     "iopub.status.idle": "2023-12-17T03:01:44.034890Z",
     "shell.execute_reply": "2023-12-17T03:01:44.034039Z"
    },
    "papermill": {
     "duration": 0.029064,
     "end_time": "2023-12-17T03:01:44.036863",
     "exception": false,
     "start_time": "2023-12-17T03:01:44.007799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 🧹 Function to reduce memory usage of a Pandas DataFrame\n",
    "def reduce_mem_usage(df, name: str, show_optimization: bool = False):\n",
    "    \"\"\"\n",
    "    Iterate through all numeric columns of a dataframe and modify the data type\n",
    "    to reduce memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 📏 Calculate the initial memory usage of the DataFrame\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "\n",
    "    # 🔄 Iterate through each column in the DataFrame\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "\n",
    "        # Check if the column's data type is not 'object' (i.e., numeric)\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            # Check if the column's data type is an integer\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                # Check if the column's data type is a float\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "\n",
    "    if show_optimization:\n",
    "        print(f\"Memory usage of {name} is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        print(f\"Decreased by {decrease:.2f}%\")\n",
    "\n",
    "    # 🔄 Return the DataFrame with optimized memory usage\n",
    "\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e165787d",
   "metadata": {
    "papermill": {
     "duration": 0.013395,
     "end_time": "2023-12-17T03:01:44.063245",
     "exception": false,
     "start_time": "2023-12-17T03:01:44.049850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## API Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64dc1e40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:01:44.147705Z",
     "iopub.status.busy": "2023-12-17T03:01:44.147009Z",
     "iopub.status.idle": "2023-12-17T03:01:44.161317Z",
     "shell.execute_reply": "2023-12-17T03:01:44.160468Z"
    },
    "papermill": {
     "duration": 0.082653,
     "end_time": "2023-12-17T03:01:44.163286",
     "exception": false,
     "start_time": "2023-12-17T03:01:44.080633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 83 µs, sys: 0 ns, total: 83 µs\n",
      "Wall time: 87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "from typing import Sequence, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# for local execution\n",
    "class MockApi:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n",
    "        They've been intentionally left in an invalid state.\n",
    "\n",
    "        Variables to set:\n",
    "            input_paths: a list of two or more paths to the csv files to be served\n",
    "            group_id_column: the column that identifies which groups of rows the API should serve.\n",
    "                A call to iter_test serves all rows of all dataframes with the current group ID value.\n",
    "            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n",
    "        '''\n",
    "        self.input_paths: Sequence[str] = [TEST_FILE, REVEALED_TARGETS_FILE, SAMPLE_SUBMISSION_FILE]\n",
    "        self.group_id_column: str = 'time_id'\n",
    "        self.export_group_id_column: bool = True\n",
    "        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n",
    "        assert len(self.input_paths) >= 2\n",
    "\n",
    "        self._status = 'initialized'\n",
    "        self.predictions = []\n",
    "\n",
    "    def iter_test(self) -> Tuple[pd.DataFrame]:\n",
    "        '''\n",
    "        Loads all of the dataframes specified in self.input_paths,\n",
    "        then yields all rows in those dataframes that equal the current self.group_id_column value.\n",
    "        '''\n",
    "        if self._status != 'initialized':\n",
    "\n",
    "            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n",
    "\n",
    "        dataframes = []\n",
    "        for pth in self.input_paths:\n",
    "            dataframes.append(pd.read_csv(pth, low_memory=False))\n",
    "        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n",
    "        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n",
    "\n",
    "        for group_id in group_order:\n",
    "            self._status = 'prediction_needed'\n",
    "            current_data = []\n",
    "            for df in dataframes:\n",
    "                cur_df = df.loc[group_id].copy()\n",
    "                # returning single line dataframes from df.loc requires special handling\n",
    "                if not isinstance(cur_df, pd.DataFrame):\n",
    "                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n",
    "                    cur_df.index.name = self.group_id_column\n",
    "                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n",
    "                current_data.append(cur_df)\n",
    "            yield tuple(current_data)\n",
    "\n",
    "            while self._status != 'prediction_received':\n",
    "                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n",
    "                yield None\n",
    "\n",
    "        with open('submission.csv', 'w') as f_open:\n",
    "            pd.concat(self.predictions).to_csv(f_open, index=False)\n",
    "        self._status = 'finished'\n",
    "\n",
    "    def predict(self, user_predictions: pd.DataFrame):\n",
    "        '''\n",
    "        Accepts and stores the user's predictions and unlocks iter_test once that is done\n",
    "        '''\n",
    "        if self._status == 'finished':\n",
    "            raise Exception('You have already made predictions for the full test set.')\n",
    "        if self._status != 'prediction_needed':\n",
    "            raise Exception('You must get the next test sample from `iter_test()` first.')\n",
    "        if not isinstance(user_predictions, pd.DataFrame):\n",
    "            raise Exception('You must provide a DataFrame.')\n",
    "\n",
    "        self.predictions.append(user_predictions)\n",
    "        self._status = 'prediction_received'\n",
    "\n",
    "def make_env():\n",
    "    return MockApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984a5d0",
   "metadata": {
    "papermill": {
     "duration": 0.013045,
     "end_time": "2023-12-17T03:01:44.189672",
     "exception": false,
     "start_time": "2023-12-17T03:01:44.176627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pandas Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a20243e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:01:44.217743Z",
     "iopub.status.busy": "2023-12-17T03:01:44.217412Z",
     "iopub.status.idle": "2023-12-17T03:01:44.223038Z",
     "shell.execute_reply": "2023-12-17T03:01:44.222248Z"
    },
    "papermill": {
     "duration": 0.021849,
     "end_time": "2023-12-17T03:01:44.224901",
     "exception": false,
     "start_time": "2023-12-17T03:01:44.203052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pd_display_max():\n",
    "    pd.set_option('display.max_rows', None)  # 行の最大表示数を無制限に設定\n",
    "    pd.set_option('display.max_columns', None)  # 列の最大表示数を無制限に設定\n",
    "    pd.set_option('display.width', None)  # 表示幅を拡張\n",
    "    pd.set_option('display.max_colwidth', None)  # 列の幅を最大に設定\n",
    "\n",
    "def pd_clear_display_max():\n",
    "    pd.set_option('display.max_rows', 10)\n",
    "    pd.set_option('display.max_columns', 10)\n",
    "    pd.set_option('display.width', None)  # 表示幅を拡張\n",
    "    pd.set_option('display.max_colwidth', None)  # 列の幅を最大に設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea2ea3",
   "metadata": {
    "papermill": {
     "duration": 0.012942,
     "end_time": "2023-12-17T03:01:44.251043",
     "exception": false,
     "start_time": "2023-12-17T03:01:44.238101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sorting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b2fbe84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:01:44.279286Z",
     "iopub.status.busy": "2023-12-17T03:01:44.278332Z",
     "iopub.status.idle": "2023-12-17T03:01:44.282921Z",
     "shell.execute_reply": "2023-12-17T03:01:44.282114Z"
    },
    "papermill": {
     "duration": 0.020504,
     "end_time": "2023-12-17T03:01:44.284802",
     "exception": false,
     "start_time": "2023-12-17T03:01:44.264298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def default_sort(df):\n",
    "    return df.sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22752ea",
   "metadata": {
    "papermill": {
     "duration": 0.012925,
     "end_time": "2023-12-17T03:01:44.310889",
     "exception": false,
     "start_time": "2023-12-17T03:01:44.297964",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generationg train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5a11bd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:01:44.339269Z",
     "iopub.status.busy": "2023-12-17T03:01:44.338618Z",
     "iopub.status.idle": "2023-12-17T03:01:44.349090Z",
     "shell.execute_reply": "2023-12-17T03:01:44.348299Z"
    },
    "papermill": {
     "duration": 0.026648,
     "end_time": "2023-12-17T03:01:44.350901",
     "exception": false,
     "start_time": "2023-12-17T03:01:44.324253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_train_dataset():\n",
    "    df = pd.read_csv(TRAIN_FILE)\n",
    "    # 🧹 Remove rows with missing values in the \"target\" column\n",
    "    df = df.dropna(subset=[\"target\"])\n",
    "    # 🔁 Reset the index of the DataFrame and apply the changes in place\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "def load_test_dataset():\n",
    "    df_test = pd.read_csv(TEST_FILE)\n",
    "    \n",
    "    df_revealed_targets = pd.read_csv(REVEALED_TARGETS_FILE)\n",
    "    df_revealed_targets = df_revealed_targets.dropna(subset=['date_id', 'seconds_in_bucket', 'stock_id'])\n",
    "    df_revealed_targets['revealed_date_id'] = df_revealed_targets['revealed_date_id'].astype(int).astype(str)\n",
    "    df_revealed_targets['seconds_in_bucket'] = df_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n",
    "    df_revealed_targets['stock_id'] = df_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n",
    "    df_revealed_targets['date_id'] = df_revealed_targets['date_id'].astype(int).astype(str)\n",
    "    df_revealed_targets['row_id'] = df_revealed_targets['date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n",
    "    df_revealed_targets['revealed_row_id'] = df_revealed_targets['revealed_date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n",
    "\n",
    "    # USE_CONTINUOUS_UPDATEが有効の時、cacheのrow_idとdf_revealed_targetsのrevealed_row_idをleft joinする\n",
    "    if USE_CONTINUOUS_UPDATE:\n",
    "        df_r = df_revealed_targets[['revealed_row_id', 'revealed_target']]\n",
    "        df_r.rename(columns={'revealed_target': 'target'}, inplace=True)\n",
    "        df_r.rename(columns={'revealed_row_id': 'row_id'}, inplace=True)\n",
    "        df_test = pd.merge(df_test, df_r, how='left', on='row_id')\n",
    "        \n",
    "    # USE_REVEALED_TARGETSが有効の時、cacheのrow_idとdf_revealed_targetsのrow_idをleft joinする\n",
    "    if USE_REVEALED_TARGETS:\n",
    "        df_r = df_revealed_targets[['row_id', 'revealed_target']]\n",
    "        df_test = pd.merge(df_test, df_r, how='left', on='row_id')\n",
    "        \n",
    "    df_test = df_test.dropna(subset=[\"target\"])\n",
    "    df_test.reset_index(drop=True, inplace=True)\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f333a07b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:01:44.378519Z",
     "iopub.status.busy": "2023-12-17T03:01:44.378221Z",
     "iopub.status.idle": "2023-12-17T03:02:02.251589Z",
     "shell.execute_reply": "2023-12-17T03:02:02.250673Z"
    },
    "papermill": {
     "duration": 17.889534,
     "end_time": "2023-12-17T03:02:02.253680",
     "exception": false,
     "start_time": "2023-12-17T03:01:44.364146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load train dataset\n",
      "['stock_id', 'seconds_in_bucket', 'imbalance_size', 'imbalance_buy_sell_flag', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap']\n",
      "RAM memory GB usage = 2.004\n",
      "CPU times: user 12.3 s, sys: 1.38 s, total: 13.6 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Check if the code is running in offline or online mode\n",
    "print(\"Load train dataset\")\n",
    "\n",
    "df_train = load_train_dataset()\n",
    "\n",
    "if IS_MIN_LEARN:\n",
    "    print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n",
    "    # In local mode, stock id TARGET_STOCK_ID is used for training\n",
    "    df_train = df_train[df_train[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n",
    "    \n",
    "features = [c for c in df_train.columns if c not in [\"row_id\", \"target\", \"time_id\", \"row_id\", \"date_id\", \"currently_scored\"]]\n",
    "print(features)\n",
    "\n",
    "collect();\n",
    "print(GetMemUsage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f374342",
   "metadata": {
    "papermill": {
     "duration": 0.0133,
     "end_time": "2023-12-17T03:02:02.280887",
     "exception": false,
     "start_time": "2023-12-17T03:02:02.267587",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Generate Featuers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c96685",
   "metadata": {
    "papermill": {
     "duration": 0.013224,
     "end_time": "2023-12-17T03:02:02.307507",
     "exception": false,
     "start_time": "2023-12-17T03:02:02.294283",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step1. Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "320f55f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:02:02.336292Z",
     "iopub.status.busy": "2023-12-17T03:02:02.335899Z",
     "iopub.status.idle": "2023-12-17T03:02:02.445197Z",
     "shell.execute_reply": "2023-12-17T03:02:02.444447Z"
    },
    "papermill": {
     "duration": 0.126378,
     "end_time": "2023-12-17T03:02:02.447270",
     "exception": false,
     "start_time": "2023-12-17T03:02:02.320892",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to compute triplet imbalance in parallel using Numba\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "\n",
    "    # 🔁 Loop through all combinations of triplets\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        \n",
    "        # 🔁 Loop through rows of the DataFrame\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            # 🚫 Prevent division by zero\n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "# 📈 Function to calculate triplet imbalance for given price data and a DataFrame\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    # Convert DataFrame to numpy array for Numba compatibility\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "\n",
    "    # Calculate the triplet imbalance using the Numba-optimized function\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Function to generate imbalance features\n",
    "def imbalance_features(df):\n",
    "    def __imbalance_features(df):\n",
    "        # Define lists of price and size-related column names\n",
    "        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "\n",
    "        # V1 features\n",
    "        # Calculate various features using Pandas eval function\n",
    "        df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "        df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n",
    "        df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "        df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n",
    "        df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "        \n",
    "        # Create features for pairwise price imbalances\n",
    "        for c in combinations(prices, 2):\n",
    "            df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "            \n",
    "        # V2 features\n",
    "        # Calculate additional features\n",
    "        df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "        df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "        df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "        df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "        df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "        df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "        # Calculate the imbalance ratio\n",
    "        df['match_balance'] = ( df['matched_size']  + (df['imbalance_buy_sell_flag'] * df['imbalance_size'])) / df['matched_size']\n",
    "        return df\n",
    "\n",
    "    if DEVICE == 'gpu':\n",
    "        import cudf\n",
    "        df = cudf.from_pandas(df)\n",
    "        df = __imbalance_features(df)\n",
    "        df = df.to_pandas()\n",
    "    else:\n",
    "        df = __imbalance_features(df)\n",
    "    # Replace infinite values with 0\n",
    "    return df.replace([np.inf, -np.inf], 0)\n",
    "\n",
    "def numba_imb_features(df):\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "    # Calculate triplet imbalance features using the Numba-optimized function\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    return df\n",
    "\n",
    "# 📅 Function to generate time and stock-related features\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n",
    "\n",
    "    # Map global features to the DataFrame\n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "# 🚀 Function to generate all features by combining imbalance and other features\n",
    "def generate_basic_features(df):\n",
    "    prev_cols = list(df.columns)\n",
    "\n",
    "    # Generate imbalance features\n",
    "    df = imbalance_features(df)\n",
    "    df = numba_imb_features(df)\n",
    "    df = other_features(df)\n",
    "\n",
    "    df = default_sort(df)    \n",
    "    \n",
    "    df = reduce_mem_usage(df, \"generate_basic_features\")\n",
    "    collect()  # Perform garbage collection to free up memory\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d0bf1e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:02:02.475635Z",
     "iopub.status.busy": "2023-12-17T03:02:02.475334Z",
     "iopub.status.idle": "2023-12-17T03:02:54.186102Z",
     "shell.execute_reply": "2023-12-17T03:02:54.185156Z"
    },
    "papermill": {
     "duration": 51.740816,
     "end_time": "2023-12-17T03:02:54.201798",
     "exception": false,
     "start_time": "2023-12-17T03:02:02.460982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1. Generate general Global Stock ID Features and basic features\n",
      "['all_sizes_skew', 'spread_intensity', 'reference_price_near_price_imb', 'global_ptp_price', 'global_std_price', 'all_prices_mean', 'reference_price_bid_price_imb', 'matched_size_bid_size_imbalance_size_imb2', 'far_price_ask_price_imb', 'price_spread', 'match_balance', 'far_price_near_price_imb', 'global_median_price', 'reference_price_ask_price_imb', 'bid_price_wap_imb', 'global_median_size', 'near_price_bid_price_imb', 'ask_price_wap_imb', 'seconds', 'all_sizes_kurt', 'dow', 'near_price_ask_price_imb', 'bid_size_ask_size_imbalance_size_imb2', 'ask_price_bid_price_wap_imb2', 'minute', 'depth_pressure', 'size_imbalance', 'market_urgency', 'global_std_size', 'all_sizes_mean', 'ask_price_bid_price_reference_price_imb2', 'near_price_wap_imb', 'global_ptp_size', 'all_sizes_std', 'far_price_bid_price_imb', 'bid_price_wap_reference_price_imb2', 'far_price_wap_imb', 'matched_imbalance', 'matched_size_ask_size_imbalance_size_imb2', 'ask_price_bid_price_imb', 'liquidity_imbalance', 'mid_price', 'price_pressure', 'ask_price_wap_reference_price_imb2', 'imbalance_momentum', 'reference_price_wap_imb', 'volume', 'matched_size_bid_size_ask_size_imb2', 'reference_price_far_price_imb', 'all_prices_skew', 'all_prices_std', 'all_prices_kurt']\n",
      "RAM memory GB usage = 5.899\n",
      "CPU times: user 34.7 s, sys: 12.6 s, total: 47.3 s\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Step1. Generate general Global Stock ID Features and basic features\")\n",
    "prev_cols = list(df_train.columns)\n",
    "global_stock_id_feats = {\n",
    "    \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "    \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "    \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "    \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "    \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "    \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "}\n",
    "\n",
    "df_train = generate_basic_features(df_train)\n",
    "\n",
    "generated_feature_name = list(set(df_train.columns) - set(prev_cols))\n",
    "features += generated_feature_name\n",
    "print(generated_feature_name)\n",
    "\n",
    "collect()\n",
    "print(GetMemUsage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d58dcb9",
   "metadata": {
    "papermill": {
     "duration": 0.013903,
     "end_time": "2023-12-17T03:02:54.229620",
     "exception": false,
     "start_time": "2023-12-17T03:02:54.215717",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Step2. Enhance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27a60a5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:02:54.259834Z",
     "iopub.status.busy": "2023-12-17T03:02:54.259473Z",
     "iopub.status.idle": "2023-12-17T03:02:54.809053Z",
     "shell.execute_reply": "2023-12-17T03:02:54.807875Z"
    },
    "papermill": {
     "duration": 0.56732,
     "end_time": "2023-12-17T03:02:54.811144",
     "exception": false,
     "start_time": "2023-12-17T03:02:54.243824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step2. Generate enhanced features\n",
      "CPU times: user 383 ms, sys: 31.7 ms, total: 414 ms\n",
      "Wall time: 525 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Step2. Generate enhanced features\")\n",
    "prev_cols = list(df_train.columns)\n",
    "\n",
    "@njit()\n",
    "def cal_diff(x, window):\n",
    "    # pands diffより遅い\n",
    "    # xの長さと同じ大きさの配列を作成し、初期値をNaNに設定\n",
    "    log_diff = np.full(x.shape, np.nan)\n",
    "    # 指定されたwindowに基づいて差分を計算\n",
    "    for i in range(window, len(x)):\n",
    "        log_diff[i] = x[i] - x[i - window]\n",
    "\n",
    "    return log_diff\n",
    "\n",
    "#@njit()\n",
    "#@njit(parallel=True)\n",
    "@njit()\n",
    "def cal_vix(x, window, offset=0):\n",
    "    log_x = np.log(x + offset)\n",
    "    log_diff = np.empty(log_x.shape)\n",
    "    roll_std = np.empty(log_diff.shape)\n",
    "\n",
    "    for i in prange(1, len(log_x)):\n",
    "        log_diff[i] = log_x[i] - log_x[i - 1]\n",
    "    \n",
    "    # ローリング標準偏差を計算\n",
    "    # jitを使わない場合、roll_std[i] = np.std(log_diff[i-window+1:i+1], ddof=1)と書ける(不偏推定量を使うためddof=1)\n",
    "    # jitを使う場合、ddof=1は使えないので、標準偏差の計算を自分で実装する\n",
    "    for i in prange(window, len(log_diff)):\n",
    "        window_values = log_diff[i-window+1:i+1]\n",
    "        mean = np.mean(window_values)\n",
    "        sum_sq_diff = np.sum((window_values - mean) ** 2)\n",
    "        roll_std[i] = np.sqrt(sum_sq_diff / (window - 1))\n",
    "\n",
    "    return roll_std\n",
    "\n",
    "USE_DASK = False\n",
    "import dask.dataframe as dd\n",
    "def generate_historical_features(df):\n",
    "    def __generate_historical_features(df):\n",
    "        print(\"generate_historical_features\")\n",
    "        target_cols = ['wap', 'match_balance']\n",
    "        if USE_INDEX:\n",
    "            target_cols.append('index_mean_wap')\n",
    "            target_cols.append('index_mean_match_balance')\n",
    "        if USE_REVEALED_TARGETS:\n",
    "            target_cols.append('revealed_target')\n",
    "\n",
    "        grouped = df.groupby(['stock_id', 'date_id'])\n",
    "\n",
    "        for col in target_cols:\n",
    "            for window in [3, 5, 7]:\n",
    "                col_diff_name = f\"{col}_diff_{window}\"\n",
    "                df[col_diff_name] = grouped[col].diff(window)\n",
    "                #df[col_diff_name] = grouped[col].transform(lambda x: cal_diff(x.values, window))\n",
    "\n",
    "                col_vix_name = f\"{col}_vix_{window}\"\n",
    "\n",
    "                if col == 'revealed_target':\n",
    "                    offset = 10\n",
    "                else:\n",
    "                    offset = 0\n",
    "                \n",
    "                #df[col_vix_name] = grouped[col].transform(lambda x: np.log(x).diff().rolling(window).std())\n",
    "                df[col_vix_name] = grouped[col].transform(lambda x: cal_vix(x.values, window))\n",
    "                #df[col_vix_name] = grouped[col].apply(lambda x: np.log(x + 100).diff().rolling(2).std()).reset_index()[col]\n",
    "\n",
    "        return df\n",
    "\n",
    "    # gpu, dskでも速度が出ないので、cpuで実行\n",
    "    \"\"\"\n",
    "    if DEVICE == 'gpu':\n",
    "        import cudf\n",
    "        df = cudf.from_pandas(df)\n",
    "        df = __generate_historical_features(df)\n",
    "        df = df.to_pandas()\n",
    "    else:\n",
    "        if USE_DASK:\n",
    "            df = dd.from_pandas(df, npartitions=4)  # npartitionsは使用するコアの数に応じて調整\n",
    "            df = df.set_index('stock_id')\n",
    "            df = __generate_historical_features(df)\n",
    "            df = df.compute()\n",
    "        else:\n",
    "            df = __generate_historical_features(df)\n",
    "    \"\"\"\n",
    "    df = __generate_historical_features(df)\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], 0)\n",
    "    return df\n",
    "\n",
    "# サブセットを処理する関数\n",
    "def subset_generate_historical_features(df_subset):\n",
    "    return generate_historical_features(df_subset)\n",
    "\n",
    "# 並列処理を実行する関数\n",
    "def parallel_generate_historical_features(df, num_threads=NUM_THREADS):\n",
    "    # DataFrameを 'stock_id' でグループ化\n",
    "    grouped = df.groupby('stock_id')\n",
    "\n",
    "    # 並列処理の実行\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        results = executor.map(subset_generate_historical_features, [group for _, group in grouped])\n",
    "\n",
    "    # 結果の統合\n",
    "    results = pd.concat(results)\n",
    "    return results\n",
    "\n",
    "def generate_index_features(df):\n",
    "     # Calculating mean and std for 'wap' and 'match_balance'\n",
    "    wap_stats = df.groupby(['date_id', 'seconds_in_bucket'])['wap'].agg(['mean', 'std']).reset_index()\n",
    "    match_balance_stats = df.groupby(['date_id', 'seconds_in_bucket'])['match_balance'].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "    # Adding prefix and suffix\n",
    "    wap_stats = wap_stats.add_prefix('index_').add_suffix('_wap')\n",
    "    match_balance_stats = match_balance_stats.add_prefix('index_').add_suffix('_match_balance')\n",
    "\n",
    "    # Adjusting column names for merging\n",
    "    wap_stats = wap_stats.rename(columns={'index_date_id_wap': 'date_id', 'index_seconds_in_bucket_wap': 'seconds_in_bucket'})\n",
    "    match_balance_stats = match_balance_stats.rename(columns={'index_date_id_match_balance': 'date_id', 'index_seconds_in_bucket_match_balance': 'seconds_in_bucket'})\n",
    "\n",
    "    # Merging with the original dataframe\n",
    "    df = df.merge(wap_stats, on=['date_id', 'seconds_in_bucket'], how='left')\n",
    "    df = df.merge(match_balance_stats, on=['date_id', 'seconds_in_bucket'], how='left')\n",
    "\n",
    "    return df\n",
    "\n",
    "def generate_normalized_features(df, is_train):\n",
    "    print(\"generate_normalized_features\")\n",
    "    if is_train:\n",
    "        df['n_target'] = (df['target'] - global_target['mean']) / global_target['std']\n",
    "    df['n_wap'] = (df['wap'] - global_wap['mean']) / global_wap['std']\n",
    "    df['n_match_balance'] = (df['match_balance'] - global_mathch_balance['mean']) / global_mathch_balance['std']\n",
    "    df['n_reference_price'] = (df['reference_price'] - global_reference_price['mean']) / global_reference_price['std']\n",
    "    \n",
    "    df = reduce_mem_usage(df, \"generate_normalized_features\")\n",
    "    return df\n",
    "\n",
    "def generate_enhance_features(df, is_train=False):\n",
    "    print(\"generate_enhance_features\")\n",
    "    if is_train:\n",
    "        if USE_REVEALED_TARGETS:\n",
    "            print(\"Use revealed targets\")\n",
    "            df[f\"revealed_target\"] = df.groupby(['stock_id', 'seconds_in_bucket'])['target'].shift(1)\n",
    "            df = df.dropna(subset=[\"revealed_target\"])\n",
    "            df = default_sort(df)\n",
    "        else:\n",
    "            print(\"Dosent't use revealed targets\")\n",
    "    if USE_INDEX:\n",
    "        print(\"Use index\")\n",
    "        current_time = time.time()\n",
    "        df = generate_index_features(df)\n",
    "        print(f\"generate_index_features {time.time() - current_time:.2f} [sec]\")\n",
    "    current_time = time.time()\n",
    "    \n",
    "    if is_train:\n",
    "        df = parallel_generate_historical_features(df)\n",
    "    else:\n",
    "        df = generate_historical_features(df)\n",
    "    print(f\"generate_historical_features {time.time() - current_time:.2f} [sec]\")\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    df = default_sort(df)\n",
    "    df = reduce_mem_usage(df, \"generate_enhance_features\")\n",
    "    collect()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1859881b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:02:54.841613Z",
     "iopub.status.busy": "2023-12-17T03:02:54.840978Z",
     "iopub.status.idle": "2023-12-17T03:06:48.487043Z",
     "shell.execute_reply": "2023-12-17T03:06:48.485921Z"
    },
    "papermill": {
     "duration": 233.663729,
     "end_time": "2023-12-17T03:06:48.489410",
     "exception": false,
     "start_time": "2023-12-17T03:02:54.825681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_enhance_features\n",
      "Use revealed targets\n",
      "Use index\n",
      "generate_index_features 3.16 [sec]\n",
      "generate_historical_featuresgenerate_historical_features\n",
      "\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_featuresgenerate_historical_features\n",
      "\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features\n",
      "generate_historical_features 215.15 [sec]\n",
      "['revealed_target_diff_3', 'index_std_wap', 'revealed_target_vix_3', 'index_mean_wap_diff_5', 'match_balance_vix_5', 'revealed_target_diff_7', 'index_mean_match_balance_vix_5', 'index_mean_wap_vix_3', 'index_mean_match_balance_diff_7', 'index_mean_match_balance_diff_3', 'match_balance_diff_3', 'index_mean_wap_vix_7', 'index_mean_match_balance_vix_7', 'revealed_target', 'index_mean_wap_diff_3', 'index_mean_match_balance', 'revealed_target_vix_7', 'match_balance_diff_5', 'match_balance_vix_7', 'revealed_target_diff_5', 'index_mean_match_balance_vix_3', 'revealed_target_vix_5', 'wap_vix_3', 'index_std_match_balance', 'wap_diff_3', 'index_mean_wap_vix_5', 'index_mean_match_balance_diff_5', 'wap_diff_5', 'match_balance_diff_7', 'index_mean_wap', 'wap_vix_7', 'wap_vix_5', 'match_balance_vix_3', 'index_mean_wap_diff_7', 'wap_diff_7']\n",
      "RAM memory GB usage = 8.396\n"
     ]
    }
   ],
   "source": [
    "df_train = generate_enhance_features(df_train, is_train=True)\n",
    "generated_feature_name = list(set(df_train.columns) - set(prev_cols))\n",
    "features += generated_feature_name\n",
    "print(generated_feature_name)\n",
    "\n",
    "collect()\n",
    "print(GetMemUsage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e503ff20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:06:48.545130Z",
     "iopub.status.busy": "2023-12-17T03:06:48.544711Z",
     "iopub.status.idle": "2023-12-17T03:17:31.964019Z",
     "shell.execute_reply": "2023-12-17T03:17:31.963134Z"
    },
    "papermill": {
     "duration": 643.450099,
     "end_time": "2023-12-17T03:17:31.966764",
     "exception": false,
     "start_time": "2023-12-17T03:06:48.516665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_DEBUG:\n",
    "    df_train.to_csv(f\"{BASE_OUTPUT_PATH}/df_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aac67dc",
   "metadata": {
    "papermill": {
     "duration": 0.029105,
     "end_time": "2023-12-17T03:17:32.026615",
     "exception": false,
     "start_time": "2023-12-17T03:17:31.997510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "104d1cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:17:32.086664Z",
     "iopub.status.busy": "2023-12-17T03:17:32.085904Z",
     "iopub.status.idle": "2023-12-17T03:17:32.095832Z",
     "shell.execute_reply": "2023-12-17T03:17:32.094869Z"
    },
    "papermill": {
     "duration": 0.04289,
     "end_time": "2023-12-17T03:17:32.098525",
     "exception": false,
     "start_time": "2023-12-17T03:17:32.055635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stock_id',\n",
       " 'seconds_in_bucket',\n",
       " 'imbalance_size',\n",
       " 'imbalance_buy_sell_flag',\n",
       " 'reference_price',\n",
       " 'matched_size',\n",
       " 'far_price',\n",
       " 'near_price',\n",
       " 'bid_price',\n",
       " 'bid_size',\n",
       " 'ask_price',\n",
       " 'ask_size',\n",
       " 'wap',\n",
       " 'all_sizes_skew',\n",
       " 'spread_intensity',\n",
       " 'reference_price_near_price_imb',\n",
       " 'global_ptp_price',\n",
       " 'global_std_price',\n",
       " 'all_prices_mean',\n",
       " 'reference_price_bid_price_imb',\n",
       " 'matched_size_bid_size_imbalance_size_imb2',\n",
       " 'far_price_ask_price_imb',\n",
       " 'price_spread',\n",
       " 'match_balance',\n",
       " 'far_price_near_price_imb',\n",
       " 'global_median_price',\n",
       " 'reference_price_ask_price_imb',\n",
       " 'bid_price_wap_imb',\n",
       " 'global_median_size',\n",
       " 'near_price_bid_price_imb',\n",
       " 'ask_price_wap_imb',\n",
       " 'seconds',\n",
       " 'all_sizes_kurt',\n",
       " 'dow',\n",
       " 'near_price_ask_price_imb',\n",
       " 'bid_size_ask_size_imbalance_size_imb2',\n",
       " 'ask_price_bid_price_wap_imb2',\n",
       " 'minute',\n",
       " 'depth_pressure',\n",
       " 'size_imbalance',\n",
       " 'market_urgency',\n",
       " 'global_std_size',\n",
       " 'all_sizes_mean',\n",
       " 'ask_price_bid_price_reference_price_imb2',\n",
       " 'near_price_wap_imb',\n",
       " 'global_ptp_size',\n",
       " 'all_sizes_std',\n",
       " 'far_price_bid_price_imb',\n",
       " 'bid_price_wap_reference_price_imb2',\n",
       " 'far_price_wap_imb',\n",
       " 'matched_imbalance',\n",
       " 'matched_size_ask_size_imbalance_size_imb2',\n",
       " 'ask_price_bid_price_imb',\n",
       " 'liquidity_imbalance',\n",
       " 'mid_price',\n",
       " 'price_pressure',\n",
       " 'ask_price_wap_reference_price_imb2',\n",
       " 'imbalance_momentum',\n",
       " 'reference_price_wap_imb',\n",
       " 'volume',\n",
       " 'matched_size_bid_size_ask_size_imb2',\n",
       " 'reference_price_far_price_imb',\n",
       " 'all_prices_skew',\n",
       " 'all_prices_std',\n",
       " 'all_prices_kurt',\n",
       " 'revealed_target_diff_3',\n",
       " 'index_std_wap',\n",
       " 'revealed_target_vix_3',\n",
       " 'index_mean_wap_diff_5',\n",
       " 'match_balance_vix_5',\n",
       " 'revealed_target_diff_7',\n",
       " 'index_mean_match_balance_vix_5',\n",
       " 'index_mean_wap_vix_3',\n",
       " 'index_mean_match_balance_diff_7',\n",
       " 'index_mean_match_balance_diff_3',\n",
       " 'match_balance_diff_3',\n",
       " 'index_mean_wap_vix_7',\n",
       " 'index_mean_match_balance_vix_7',\n",
       " 'revealed_target',\n",
       " 'index_mean_wap_diff_3',\n",
       " 'index_mean_match_balance',\n",
       " 'revealed_target_vix_7',\n",
       " 'match_balance_diff_5',\n",
       " 'match_balance_vix_7',\n",
       " 'revealed_target_diff_5',\n",
       " 'index_mean_match_balance_vix_3',\n",
       " 'revealed_target_vix_5',\n",
       " 'wap_vix_3',\n",
       " 'index_std_match_balance',\n",
       " 'wap_diff_3',\n",
       " 'index_mean_wap_vix_5',\n",
       " 'index_mean_match_balance_diff_5',\n",
       " 'wap_diff_5',\n",
       " 'match_balance_diff_7',\n",
       " 'index_mean_wap',\n",
       " 'wap_vix_7',\n",
       " 'wap_vix_5',\n",
       " 'match_balance_vix_3',\n",
       " 'index_mean_wap_diff_7',\n",
       " 'wap_diff_7']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature selection\n",
    "if not USE_ALL_FEATUTES:\n",
    "    features  = [\n",
    "        \"revealed_target\",\n",
    "        \"wap_diff_1\",\n",
    "        \"index_mean_wap_diff_1\",\n",
    "        \"seconds_in_bucket\",\n",
    "        \"stock_id\",\n",
    "    ]\n",
    "#df_valid = df_train[\"target\"]\n",
    "#df_train = df_train[features]\n",
    "#if USE_REVEALED_TARGETS:\n",
    "#    features.remove(\"revealed_target\")\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc5264ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:17:32.158918Z",
     "iopub.status.busy": "2023-12-17T03:17:32.158550Z",
     "iopub.status.idle": "2023-12-17T03:17:34.610500Z",
     "shell.execute_reply": "2023-12-17T03:17:34.609475Z"
    },
    "papermill": {
     "duration": 2.48515,
     "end_time": "2023-12-17T03:17:34.613038",
     "exception": false,
     "start_time": "2023-12-17T03:17:32.127888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "revealed_target_vix_7                        4375930\n",
       "revealed_target_vix_5                        4109444\n",
       "revealed_target_vix_3                        3694728\n",
       "reference_price_far_price_imb                2888134\n",
       "far_price                                    2888134\n",
       "far_price_wap_imb                            2888134\n",
       "far_price_bid_price_imb                      2888134\n",
       "depth_pressure                               2888134\n",
       "far_price_near_price_imb                     2888134\n",
       "far_price_ask_price_imb                      2888134\n",
       "near_price                                   2851092\n",
       "near_price_wap_imb                           2851092\n",
       "reference_price_near_price_imb               2851092\n",
       "near_price_ask_price_imb                     2851092\n",
       "near_price_bid_price_imb                     2851092\n",
       "wap_diff_7                                    665356\n",
       "match_balance_diff_7                          665356\n",
       "index_mean_wap_diff_7                         665245\n",
       "index_mean_match_balance_diff_7               665245\n",
       "revealed_target_diff_7                        665245\n",
       "match_balance_diff_5                          475292\n",
       "wap_diff_5                                    475292\n",
       "revealed_target_diff_5                        475175\n",
       "index_mean_wap_diff_5                         475175\n",
       "index_mean_match_balance_diff_5               475175\n",
       "bid_price_wap_reference_price_imb2            443062\n",
       "ask_price_bid_price_reference_price_imb2      437812\n",
       "wap_diff_3                                    285228\n",
       "match_balance_diff_3                          285228\n",
       "index_mean_match_balance_diff_3               285105\n",
       "revealed_target_diff_3                        285105\n",
       "index_mean_wap_diff_3                         285105\n",
       "match_balance_vix_7                           194178\n",
       "match_balance_vix_5                           180616\n",
       "match_balance_vix_3                           166860\n",
       "wap_vix_3                                      40361\n",
       "wap_vix_5                                      33594\n",
       "wap_vix_7                                      31314\n",
       "index_mean_wap_vix_3                           22792\n",
       "index_mean_wap_vix_5                           21688\n",
       "index_mean_wap_vix_7                           21381\n",
       "index_mean_match_balance_vix_3                 19468\n",
       "index_mean_match_balance_vix_5                 18772\n",
       "index_mean_match_balance_vix_7                 18503\n",
       "ask_price_wap_reference_price_imb2              6436\n",
       "ask_price_bid_price_wap_imb2                    3382\n",
       "spread_intensity                                 135\n",
       "imbalance_momentum                               135\n",
       "bid_size_ask_size_imbalance_size_imb2            133\n",
       "all_prices_mean                                  132\n",
       "match_balance                                    132\n",
       "price_spread                                     132\n",
       "matched_size_bid_size_imbalance_size_imb2        132\n",
       "reference_price_bid_price_imb                    132\n",
       "all_prices_kurt                                  132\n",
       "bid_price_wap_imb                                132\n",
       "all_sizes_skew                                   132\n",
       "wap                                              132\n",
       "ask_price                                        132\n",
       "bid_price                                        132\n",
       "matched_size                                     132\n",
       "reference_price_ask_price_imb                    132\n",
       "all_sizes_kurt                                   132\n",
       "ask_price_wap_imb                                132\n",
       "all_prices_std                                   132\n",
       "market_urgency                                   132\n",
       "matched_imbalance                                132\n",
       "matched_size_ask_size_imbalance_size_imb2        132\n",
       "ask_price_bid_price_imb                          132\n",
       "mid_price                                        132\n",
       "price_pressure                                   132\n",
       "reference_price                                  132\n",
       "reference_price_wap_imb                          132\n",
       "matched_size_bid_size_ask_size_imb2              132\n",
       "all_prices_skew                                  132\n",
       "imbalance_size                                   132\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_display_max()\n",
    "nan_count = df_train[features].isna().sum()\n",
    "#df_train[features].to_csv('train.csv', index=False)\n",
    "nan_count = nan_count[nan_count > 0].sort_values(ascending=False)\n",
    "nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9233f96d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:17:34.675394Z",
     "iopub.status.busy": "2023-12-17T03:17:34.674473Z",
     "iopub.status.idle": "2023-12-17T03:17:34.679089Z",
     "shell.execute_reply": "2023-12-17T03:17:34.678223Z"
    },
    "papermill": {
     "duration": 0.038565,
     "end_time": "2023-12-17T03:17:34.681136",
     "exception": false,
     "start_time": "2023-12-17T03:17:34.642571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd_clear_display_max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81492b85",
   "metadata": {
    "papermill": {
     "duration": 0.029419,
     "end_time": "2023-12-17T03:17:34.739787",
     "exception": false,
     "start_time": "2023-12-17T03:17:34.710368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train function (lightgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a820e16f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:17:34.800498Z",
     "iopub.status.busy": "2023-12-17T03:17:34.799757Z",
     "iopub.status.idle": "2023-12-17T03:17:35.267511Z",
     "shell.execute_reply": "2023-12-17T03:17:35.266491Z"
    },
    "papermill": {
     "duration": 0.500752,
     "end_time": "2023-12-17T03:17:35.269736",
     "exception": false,
     "start_time": "2023-12-17T03:17:34.768984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 📦 Import necessary libraries\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import gc\n",
    "import os\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "import shutil\n",
    "import lightgbm as lgb\n",
    "\n",
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "@dataclass\n",
    "class Model:\n",
    "    booster: lgb.Booster\n",
    "    fold: int\n",
    "    feature_importance: pd.DataFrame\n",
    "    score: float\n",
    "    best_iteration: int\n",
    "    train_time: float = None\n",
    "    weight: float = None\n",
    "    mem_usage: float = None\n",
    "    train_func: str = None\n",
    "    is_latest: bool = False\n",
    "\n",
    "def train_model(train_x, train_y, val_x, val_y, best_params=None):\n",
    "    trains = lgb.Dataset(train_x, train_y)\n",
    "    valids = lgb.Dataset(val_x, val_y, reference=trains)\n",
    "\n",
    "    verbose_eval = -1\n",
    "    if best_params is None:\n",
    "        params = lgb_params\n",
    "    else:\n",
    "        params = best_params\n",
    "\n",
    "    print(\"Use params:\")\n",
    "    print(params)\n",
    "\n",
    "    booster = lgb.train(\n",
    "        params,\n",
    "        trains,\n",
    "        valid_sets=valids, # 検証データ\n",
    "        num_boost_round=num_boost_round,\n",
    "        keep_training_booster=True,\n",
    "        callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n",
    "                lgb.log_evaluation(verbose_eval)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    del trains, valids\n",
    "    return booster\n",
    "\n",
    "def cross_train(df, key, n_splits, features, valid_name, best_params=None):\n",
    "    \"\"\" For Cross Train\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        n_splits (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    print(\"----------------------------------------\")\n",
    "    print(f\"Cross Train key id {key}: start, shape: {df_train.shape}, n_splits: {n_splits}\")\n",
    "    print(f\"num_boost_round: {num_boost_round}, stopping_rounds: {stopping_rounds}, folds: {num_folds}\")\n",
    "\n",
    "    models = []\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for fold, (train_indices, valid_indices) in enumerate(kf.split(df)):\n",
    "        print(f\"----- Train {key}: {fold} start -----\")\n",
    "        now_time = time.time()\n",
    "\n",
    "        print(f\"train_indices: {train_indices}, valid_indices: {valid_indices}\")\n",
    "        X_train, X_valid = df[features].iloc[train_indices], df[features].iloc[valid_indices]\n",
    "        y_train, y_valid = df[valid_name].loc[train_indices], df[valid_name].loc[valid_indices]\n",
    "        print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}, y_train: {y_train.shape}, y_valid: {y_valid.shape}\")\n",
    "\n",
    "        booster = train_model(X_train, y_train, X_valid, y_valid, best_params)\n",
    "        print(booster.best_score)\n",
    "        y_valid_pred = booster.predict(X_valid)\n",
    "        \n",
    "        score = mean_absolute_error(y_valid, y_valid_pred)\n",
    "        train_time = time.time() - now_time\n",
    "        mem_usage = sys.getsizeof(booster) / (1024 * 1024) # MB\n",
    "        model = Model(booster, fold, booster.feature_importance(), score, booster.best_iteration, train_time, weight= 1 / n_splits, mem_usage=mem_usage, train_func=\"lightgbm\", is_latest=True)\n",
    "        print(f\"{key}: {fold} end, score: {score}, time: {model.train_time}, best_iteration: {model.best_iteration}, memory usage: {model.mem_usage}\")\n",
    "        \n",
    "        models.append(model)\n",
    "        \n",
    "        del X_train, X_valid, y_train, y_valid\n",
    "        gc.collect()\n",
    "        print(GetMemUsage())\n",
    "        print(f\"----- Train {key}: {fold} end -----\")\n",
    "\n",
    "    print(f\"Cross train {key} model len {len(models)}\")\n",
    "    print(\"----------------------------------------\")\n",
    "    return key, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89890ce3",
   "metadata": {
    "papermill": {
     "duration": 0.029018,
     "end_time": "2023-12-17T03:17:35.328839",
     "exception": false,
     "start_time": "2023-12-17T03:17:35.299821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Train function (optuna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07519123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:17:35.389080Z",
     "iopub.status.busy": "2023-12-17T03:17:35.388231Z",
     "iopub.status.idle": "2023-12-17T03:17:35.412022Z",
     "shell.execute_reply": "2023-12-17T03:17:35.410903Z"
    },
    "papermill": {
     "duration": 0.056045,
     "end_time": "2023-12-17T03:17:35.414328",
     "exception": false,
     "start_time": "2023-12-17T03:17:35.358283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.3 ms, sys: 0 ns, total: 4.3 ms\n",
      "Wall time: 7.01 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import optuna.integration.lightgbm as optuna_lgb\n",
    "import optuna\n",
    "import lightgbm\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "class TunerCVCheckpointCallback(object):\n",
    "    \"\"\"Optuna の LightGBMTunerCV から学習済みモデルを取り出すためのコールバック\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Models\n",
    "        self.models = []\n",
    "        self.counter = 0\n",
    "\n",
    "    def get_models(self):\n",
    "        # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster\n",
    "        return self.models\n",
    "\n",
    "    def __call__(self, env: lightgbm.callback.CallbackEnv):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            env (lightgbm.callback.CallbackEnv): _description_\n",
    "            \"model\",\n",
    "            \"params\",\n",
    "            \"iteration\",\n",
    "            \"begin_iteration\",\n",
    "            \"end_iteration\",\n",
    "            \"evaluation_result_list\"\n",
    "        \"\"\"\n",
    "        print(\"\")\n",
    "\n",
    "        self.counter += 1\n",
    "        print(\"-------------------\")\n",
    "        print(f\"Counter: {self.counter}\")\n",
    "        print(f\"Iteration: {env.iteration}\")\n",
    "        print(f\"Begin_iteration: {env.begin_iteration}\")\n",
    "        print(f\"End_iteration: {env.end_iteration}\")\n",
    "        print(f\"Evaluation_result_list: {env.evaluation_result_list}\")\n",
    "        print(f\"Model best_iteration: {env.model.best_iteration}\")\n",
    "        print(\"Params: \", env.params)\n",
    "        #self.models.append(env.model)\n",
    "        del env\n",
    "\n",
    "        collect();\n",
    "        print(GetMemUsage())\n",
    "\n",
    "def optuna_tuning(df, n_splits, features, valid_name, model_save_path):\n",
    "    df_train = df[features]\n",
    "    df_valid = df[valid_name]\n",
    "    \n",
    "    trains = optuna_lgb.Dataset(df_train, df_valid)\n",
    "    \n",
    "    print(\"------- Optuna Tuning Start -------\")\n",
    "    now_time = time.time()\n",
    "    print(f\"num_boost_round: {num_boost_round}, stopping_rounds: {stopping_rounds}, folds: {num_folds}\")\n",
    "\n",
    "    folds = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    checkpoint_cb = TunerCVCheckpointCallback()\n",
    "    \n",
    "    verbose_eval = 0\n",
    "    # https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.lightgbm.LightGBMTunerCV.html\n",
    "    tuner = optuna_lgb.LightGBMTunerCV(\n",
    "        optuna_params,\n",
    "        trains,\n",
    "        num_boost_round=num_boost_round,\n",
    "        folds=folds,\n",
    "        show_progress_bar=False,\n",
    "        return_cvbooster=True,\n",
    "        verbosity=-1,\n",
    "        model_dir=model_save_path,\n",
    "        optuna_seed=seed,\n",
    "        time_budget=OPTUNA_TIME_BUDGET,\n",
    "        callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n",
    "                lgb.log_evaluation(verbose_eval),\n",
    "                checkpoint_cb\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    tuner.run()\n",
    "    best_params = tuner.best_params\n",
    "    \n",
    "    print(\"Params: \")\n",
    "    for key, value in best_params.items():\n",
    "        print(\" {}: {}\".format(key, value))\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"len(tuner.study.trials): \", len(tuner.study.trials))\n",
    "    #print(\"len(checkpoint_cb.cv_boosters): \", len(checkpoint_cb.models))\n",
    "    print(\"Tuner best_params\", tuner.best_params)\n",
    "    print(\"Tuner best score: \", tuner.best_score)\n",
    "   \n",
    "    # 最も良かったパラメータをキーにして学習済みモデルを取り出す\n",
    "    best_booster = tuner.get_best_booster()\n",
    "    score = -1\n",
    "    train_time = time.time() - now_time\n",
    "    mem_usage = sys.getsizeof(best_booster) / (1024 * 1024) # MB\n",
    "    feature_importance = np.mean(best_booster.feature_importance(), axis=0)\n",
    "\n",
    "    best_model = Model(best_booster, 1, feature_importance, score, best_booster.best_iteration, train_time, weight= 1, mem_usage=mem_usage, train_func=\"optuna_lgb\")\n",
    "    print(\"------- Optuna Tuning End -------\")\n",
    "    return best_params, best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801cd78c",
   "metadata": {
    "papermill": {
     "duration": 0.028788,
     "end_time": "2023-12-17T03:17:35.472782",
     "exception": false,
     "start_time": "2023-12-17T03:17:35.443994",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f2e79c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:17:35.535635Z",
     "iopub.status.busy": "2023-12-17T03:17:35.535237Z",
     "iopub.status.idle": "2023-12-17T03:55:52.545458Z",
     "shell.execute_reply": "2023-12-17T03:55:52.544488Z"
    },
    "papermill": {
     "duration": 2297.045241,
     "end_time": "2023-12-17T03:55:52.547665",
     "exception": false,
     "start_time": "2023-12-17T03:17:35.502424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Cross Train key id -1: start, shape: (5226892, 104), n_splits: 5\n",
      "num_boost_round: 5000, stopping_rounds: 100, folds: 5\n",
      "----- Train -1: 0 start -----\n",
      "train_indices: [      1       2       4 ... 5226889 5226890 5226891], valid_indices: [      0       3       8 ... 5226873 5226877 5226885]\n",
      "X_train: (4181513, 100), X_valid: (1045379, 100), y_train: (4181513,), y_valid: (1045379,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.61912\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.61912268081489)])})\n",
      "-1: 0 end, score: 5.984168869076591, time: 469.47164940834045, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 12.74\n",
      "----- Train -1: 0 end -----\n",
      "----- Train -1: 1 start -----\n",
      "train_indices: [      0       1       3 ... 5226889 5226890 5226891], valid_indices: [      2       5       9 ... 5226874 5226881 5226884]\n",
      "X_train: (4181513, 100), X_valid: (1045379, 100), y_train: (4181513,), y_valid: (1045379,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.60711\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.607114724608635)])})\n",
      "-1: 1 end, score: 5.976397120278592, time: 452.20588207244873, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 12.88\n",
      "----- Train -1: 1 end -----\n",
      "----- Train -1: 2 start -----\n",
      "train_indices: [      0       2       3 ... 5226887 5226889 5226890], valid_indices: [      1       7      19 ... 5226882 5226888 5226891]\n",
      "X_train: (4181514, 100), X_valid: (1045378, 100), y_train: (4181514,), y_valid: (1045378,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.64784\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.647836719529979)])})\n",
      "-1: 2 end, score: 5.986117024481044, time: 455.56006932258606, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 12.95\n",
      "----- Train -1: 2 end -----\n",
      "----- Train -1: 3 start -----\n",
      "train_indices: [      0       1       2 ... 5226888 5226889 5226891], valid_indices: [     14      18      25 ... 5226886 5226887 5226890]\n",
      "X_train: (4181514, 100), X_valid: (1045378, 100), y_train: (4181514,), y_valid: (1045378,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.63399\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.633988165193085)])})\n",
      "-1: 3 end, score: 5.994566684278234, time: 457.10699009895325, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 13.05\n",
      "----- Train -1: 3 end -----\n",
      "----- Train -1: 4 start -----\n",
      "train_indices: [      0       1       2 ... 5226888 5226890 5226891], valid_indices: [      4       6      10 ... 5226872 5226880 5226889]\n",
      "X_train: (4181514, 100), X_valid: (1045378, 100), y_train: (4181514,), y_valid: (1045378,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.64689\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.646894459987836)])})\n",
      "-1: 4 end, score: 6.001732469650849, time: 460.1423017978668, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 13.84\n",
      "----- Train -1: 4 end -----\n",
      "Cross train -1 model len 5\n",
      "----------------------------------------\n",
      "RAM memory GB usage = 13.8\n",
      "CPU times: user 2h 26min 29s, sys: 38.8 s, total: 2h 27min 7s\n",
      "Wall time: 38min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "KEY = \"-1\"\n",
    "\n",
    "# Train\n",
    "best_params = None\n",
    "key_models = None\n",
    "if USE_OPTUNA:\n",
    "    model_save_base_path = f\"{BASE_OUTPUT_PATH}/model\"\n",
    "    if os.path.exists(model_save_base_path):\n",
    "        print(f\"{model_save_base_path} already exists, clean up it.\")\n",
    "        shutil.rmtree(model_save_base_path)\n",
    "    os.makedirs(model_save_base_path)\n",
    "    print(f\"model_save_base_path: {model_save_base_path}\")\n",
    "\n",
    "    best_params, best_model = optuna_tuning(df=df_train, n_splits=num_folds, features=features, valid_name=\"target\", model_save_path=model_save_base_path)\n",
    "    key_models = [(KEY, [best_model])]\n",
    "else:\n",
    "    #key_models = df_train.groupby(\"seconds_in_bucket\").apply(lambda x: cross_train(df=x, key=x.name, n_splits=num_folds, feature_name=feature_name, valid_name=\"target\", best_params=best_params))\n",
    "    key_models = [cross_train(df_train, key=KEY, n_splits=num_folds, features=features, valid_name=\"target\", best_params=best_params)]\n",
    "    if IS_USE_SAVED_MODEL:\n",
    "        model_save_base_path = f\"{BASE_OUTPUT_PATH}/model\"\n",
    "        if os.path.exists(model_save_base_path):\n",
    "            print(f\"{model_save_base_path} already exists, clean up it.\")\n",
    "            shutil.rmtree(model_save_base_path)\n",
    "        os.makedirs(model_save_base_path)\n",
    "\n",
    "        key_model_paths = []\n",
    "        for key, models in key_models:\n",
    "            model_save_path = f\"{model_save_base_path}/{key}\"\n",
    "            os.makedirs(model_save_path)\n",
    "            model_paths = []\n",
    "            for model in models:\n",
    "                model_save_fullpath = f\"{model_save_path}/model_{key}_{model.fold}.txt\"\n",
    "                model.model.save_model(model_save_fullpath)\n",
    "                model_paths.append(model_save_fullpath)\n",
    "            key_model_paths.append((key, model_paths))\n",
    "\n",
    "        model_dict_saved = {key: model_paths for key, model_paths in key_model_paths}\n",
    "        print(model_dict_saved)\n",
    "\n",
    "\n",
    "model_dict = {key: model for key, model in key_models}\n",
    "collect()\n",
    "print(GetMemUsage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683832dd",
   "metadata": {
    "papermill": {
     "duration": 0.030616,
     "end_time": "2023-12-17T03:55:52.608388",
     "exception": false,
     "start_time": "2023-12-17T03:55:52.577772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Additional training by important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c9649b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T03:55:52.668595Z",
     "iopub.status.busy": "2023-12-17T03:55:52.668237Z",
     "iopub.status.idle": "2023-12-17T05:01:38.189297Z",
     "shell.execute_reply": "2023-12-17T05:01:38.188168Z"
    },
    "papermill": {
     "duration": 3945.553762,
     "end_time": "2023-12-17T05:01:38.191490",
     "exception": false,
     "start_time": "2023-12-17T03:55:52.637728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Train\n",
      "----------------------------------------\n",
      "Cross Train key id -1: start, shape: (5226892, 104), n_splits: 5\n",
      "num_boost_round: 5000, stopping_rounds: 100, folds: 5\n",
      "----- Train -1: 0 start -----\n",
      "train_indices: [      1       2       4 ... 5226889 5226890 5226891], valid_indices: [      0       3       8 ... 5226873 5226877 5226885]\n",
      "X_train: (4181513, 100), X_valid: (1045379, 100), y_train: (4181513,), y_valid: (1045379,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.61706\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.617057389795058)])})\n",
      "-1: 0 end, score: 5.983229319348763, time: 447.59299898147583, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 16.25\n",
      "----- Train -1: 0 end -----\n",
      "----- Train -1: 1 start -----\n",
      "train_indices: [      0       1       3 ... 5226889 5226890 5226891], valid_indices: [      2       5       9 ... 5226874 5226881 5226884]\n",
      "X_train: (4181513, 100), X_valid: (1045379, 100), y_train: (4181513,), y_valid: (1045379,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.60717\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.60717253381433)])})\n",
      "-1: 1 end, score: 5.976441633428617, time: 461.82292222976685, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 16.32\n",
      "----- Train -1: 1 end -----\n",
      "----- Train -1: 2 start -----\n",
      "train_indices: [      0       2       3 ... 5226887 5226889 5226890], valid_indices: [      1       7      19 ... 5226882 5226888 5226891]\n",
      "X_train: (4181514, 100), X_valid: (1045378, 100), y_train: (4181514,), y_valid: (1045378,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.64681\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.646806279439556)])})\n",
      "-1: 2 end, score: 5.985263261976791, time: 462.4707124233246, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 16.44\n",
      "----- Train -1: 2 end -----\n",
      "----- Train -1: 3 start -----\n",
      "train_indices: [      0       1       2 ... 5226888 5226889 5226891], valid_indices: [     14      18      25 ... 5226886 5226887 5226890]\n",
      "X_train: (4181514, 100), X_valid: (1045378, 100), y_train: (4181514,), y_valid: (1045378,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.63507\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.635074546593135)])})\n",
      "-1: 3 end, score: 5.995127767765736, time: 460.8615560531616, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 17.28\n",
      "----- Train -1: 3 end -----\n",
      "----- Train -1: 4 start -----\n",
      "train_indices: [      0       1       2 ... 5226888 5226890 5226891], valid_indices: [      4       6      10 ... 5226872 5226880 5226889]\n",
      "X_train: (4181514, 100), X_valid: (1045378, 100), y_train: (4181514,), y_valid: (1045378,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.64832\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.648316628132646)])})\n",
      "-1: 4 end, score: 6.002896980588425, time: 474.82264614105225, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 18.11\n",
      "----- Train -1: 4 end -----\n",
      "Cross train -1 model len 5\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Cross Train key id -1: start, shape: (5226892, 104), n_splits: 5\n",
      "num_boost_round: 5000, stopping_rounds: 100, folds: 5\n",
      "----- Train -1: 0 start -----\n",
      "train_indices: [      1       2       4 ... 5226889 5226890 5226891], valid_indices: [      0       3       8 ... 5226873 5226877 5226885]\n",
      "X_train: (4181513, 24), X_valid: (1045379, 24), y_train: (4181513,), y_valid: (1045379,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[4999]\tvalid_0's rmse: 8.86293\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.86292897335461)])})\n",
      "-1: 0 end, score: 6.133121417236165, time: 324.3733694553375, best_iteration: 4999, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 18.21\n",
      "----- Train -1: 0 end -----\n",
      "----- Train -1: 1 start -----\n",
      "train_indices: [      0       1       3 ... 5226889 5226890 5226891], valid_indices: [      2       5       9 ... 5226874 5226881 5226884]\n",
      "X_train: (4181513, 24), X_valid: (1045379, 24), y_train: (4181513,), y_valid: (1045379,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.86182\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.861817288315395)])})\n",
      "-1: 1 end, score: 6.130692388830125, time: 325.5052251815796, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 18.32\n",
      "----- Train -1: 1 end -----\n",
      "----- Train -1: 2 start -----\n",
      "train_indices: [      0       2       3 ... 5226887 5226889 5226890], valid_indices: [      1       7      19 ... 5226882 5226888 5226891]\n",
      "X_train: (4181514, 24), X_valid: (1045378, 24), y_train: (4181514,), y_valid: (1045378,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.8962\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.896199085464866)])})\n",
      "-1: 2 end, score: 6.1358475596060185, time: 325.3401508331299, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 18.48\n",
      "----- Train -1: 2 end -----\n",
      "----- Train -1: 3 start -----\n",
      "train_indices: [      0       1       2 ... 5226888 5226889 5226891], valid_indices: [     14      18      25 ... 5226886 5226887 5226890]\n",
      "X_train: (4181514, 24), X_valid: (1045378, 24), y_train: (4181514,), y_valid: (1045378,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.88421\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.884213871389312)])})\n",
      "-1: 3 end, score: 6.145433767115097, time: 332.1529264450073, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 18.86\n",
      "----- Train -1: 3 end -----\n",
      "----- Train -1: 4 start -----\n",
      "train_indices: [      0       1       2 ... 5226888 5226890 5226891], valid_indices: [      4       6      10 ... 5226872 5226880 5226889]\n",
      "X_train: (4181514, 24), X_valid: (1045378, 24), y_train: (4181514,), y_valid: (1045378,)\n",
      "Use params:\n",
      "{'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'verbosity': -1, 'device': 'gpu', 'feature_pre_filter': False, 'lambda_l1': 0.0, 'lambda_l2': 0.0, 'num_leaves': 31, 'feature_fraction': 0.8, 'bagging_fraction': 1.0, 'bagging_freq': 0, 'min_child_samples': 20, 'seed': 2023}\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[5000]\tvalid_0's rmse: 8.89423\n",
      "defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 8.894231448271738)])})\n",
      "-1: 4 end, score: 6.154397559131187, time: 325.9159400463104, best_iteration: 5000, memory usage: 4.57763671875e-05\n",
      "RAM memory GB usage = 19.3\n",
      "----- Train -1: 4 end -----\n",
      "Cross train -1 model len 5\n",
      "----------------------------------------\n",
      "RAM memory GB usage = 19.3\n",
      "CPU times: user 4h 11min 54s, sys: 1min 36s, total: 4h 13min 30s\n",
      "Wall time: 1h 5min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "additional_features = ['reference_price', 'match_balance_diff_5', 'wap', 'global_std_price',\n",
    "       'all_sizes_skew', 'matched_size_bid_size_ask_size_imb2', 'ask_price',\n",
    "       'index_mean_wap_diff_7', 'seconds_in_bucket', 'mid_price',\n",
    "       'ask_price_bid_price_reference_price_imb2', 'wap_vix_7', 'wap_vix_3',\n",
    "       'all_sizes_std', 'global_median_price', 'volume', 'all_sizes_mean',\n",
    "       'revealed_target', 'wap_diff_7', 'global_ptp_size',\n",
    "       'reference_price_wap_imb', 'bid_price_wap_reference_price_imb2',\n",
    "       'stock_id', 'global_median_size']\n",
    "additional_models = []\n",
    "\n",
    "if USE_ADDITIONAL_TRAIN:\n",
    "   print(\"Additional Train\")\n",
    "   key_models = [cross_train(df_train, key=KEY, n_splits=num_folds, features=features, valid_name=\"target\", best_params=best_params)]\n",
    "   key_additional_models = [cross_train(df_train, key=KEY, n_splits=num_folds, features=additional_features, valid_name=\"target\", best_params=best_params)]\n",
    "   additional_model_dict = {key: model for key, model in key_additional_models}\n",
    "\n",
    "collect()\n",
    "print(GetMemUsage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc65f9fa",
   "metadata": {
    "papermill": {
     "duration": 0.033807,
     "end_time": "2023-12-17T05:01:38.260078",
     "exception": false,
     "start_time": "2023-12-17T05:01:38.226271",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Update model using test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02696bd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:01:38.330758Z",
     "iopub.status.busy": "2023-12-17T05:01:38.330381Z",
     "iopub.status.idle": "2023-12-17T05:02:23.354235Z",
     "shell.execute_reply": "2023-12-17T05:02:23.353305Z"
    },
    "papermill": {
     "duration": 45.092063,
     "end_time": "2023-12-17T05:02:23.386120",
     "exception": false,
     "start_time": "2023-12-17T05:01:38.294057",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_duration 1100\n",
      "Update model with test date\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.62 [sec]\n",
      "update_global_train_cache\n",
      "Updated global_train_cache, len:  220000\n",
      "220000\n",
      "        date_id  seconds_in_bucket  stock_id  origin  revealed_target  \\\n",
      "0           461                  0         0       0        -2.830029   \n",
      "1           461                  0         1       0        11.060238   \n",
      "2           461                  0         2       0         2.080202   \n",
      "3           461                  0         3       0         2.720356   \n",
      "4           461                  0         4       0        -5.499721   \n",
      "...         ...                ...       ...     ...              ...   \n",
      "219995      480                540       195       0         1.599789   \n",
      "219996      480                540       196       0        -8.440018   \n",
      "219997      480                540       197       0         5.149841   \n",
      "219998      480                540       198       0        -0.249743   \n",
      "219999      480                540       199       0        -7.609725   \n",
      "\n",
      "          target  \n",
      "0       2.900362  \n",
      "1       4.279613  \n",
      "2      -6.049871  \n",
      "3      -4.360080  \n",
      "4       2.360344  \n",
      "...          ...  \n",
      "219995  2.310276  \n",
      "219996 -8.220077  \n",
      "219997  1.169443  \n",
      "219998 -1.540184  \n",
      "219999 -6.530285  \n",
      "\n",
      "[220000 rows x 6 columns]\n",
      "Update model with test date end\n",
      "RAM memory GB usage = 19.3\n",
      "CPU times: user 11.2 s, sys: 34.1 s, total: 45.2 s\n",
      "Wall time: 45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# global train cache for continuous update\n",
    "global_train_cache = df_train.copy()\n",
    "# origin 0 is train, 1 is test, 2 is revaled\n",
    "global_train_cache['origin'] = 0\n",
    "date_duration = DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span\n",
    "print(\"date_duration\", date_duration)\n",
    "\n",
    "def update_global_train_cache(df, origin, valid_key: str = 'target'):\n",
    "    global global_train_cache\n",
    "    df['origin'] = origin\n",
    "    print(\"update_global_train_cache\")\n",
    "    global_train_cache = pd.concat([global_train_cache, df], axis=0)\n",
    "    global_train_cache = global_train_cache.dropna(subset=['target'])\n",
    "    global_train_cache = global_train_cache.sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id', 'origin'])\n",
    "    global_train_cache = global_train_cache.drop_duplicates(['date_id', 'seconds_in_bucket', 'stock_id'], keep='last')\n",
    "    global_train_cache = global_train_cache.reset_index(drop=True)\n",
    "    global_train_cache = global_train_cache.groupby(['stock_id']).tail(date_duration).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "\n",
    "    global_train_cache = reduce_mem_usage(global_train_cache, 'global_train_cache')\n",
    "    print(f\"Updated global_train_cache, len: \", len(global_train_cache))\n",
    "    if IS_DEBUG:\n",
    "        print(len(global_train_cache))\n",
    "        if USE_REVEALED_TARGETS:\n",
    "            cdf = global_train_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'origin', 'revealed_target', 'target']]\n",
    "        else:\n",
    "            cdf = global_train_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'origin', 'target']]\n",
    "        print(cdf)\n",
    "\n",
    "def update_models(df, models, features, valid_name):\n",
    "    \"\"\" For Update Model\n",
    "\n",
    "    Args:\n",
    "        df (_type_): _description_\n",
    "        n_splits (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    for fold, (train_indices, valid_indices) in enumerate(kf.split(df)):\n",
    "        print(f\"{key}: {fold} update\")\n",
    "        now_time = time.time()\n",
    "        X_train, X_valid = df[features].iloc[train_indices], df[features].iloc[valid_indices]\n",
    "        y_train, y_valid = df[valid_name].loc[train_indices], df[valid_name].loc[valid_indices]\n",
    "        trains = lgb.Dataset(X_train, y_train, free_raw_data=False)\n",
    "        valids = lgb.Dataset(X_valid, y_valid, reference=trains)\n",
    "\n",
    "        print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}, y_train: {y_train.shape}, y_valid: {y_valid.shape}\")\n",
    "        models[fold].booster.update(trains)\n",
    "\n",
    "def keep_train_models(df, models, features, valid_name, is_append=False):\n",
    "    print(f\"----------------- keep_train_models, is_append: {is_append}, model len: {len(models)}, df len: {len(df)} ---------------------\")\n",
    "    train_x = df[features]\n",
    "    train_y = df[valid_name]\n",
    "    trains = lgb.Dataset(train_x, train_y, free_raw_data=False)\n",
    "    verbose_eval = -1\n",
    "\n",
    "    counter = 0\n",
    "    r_models = []\n",
    "    if IS_DEBUG:\n",
    "        print(f\"Re-train dataset:\")\n",
    "        if USE_REVEALED_TARGETS:\n",
    "            print(df[['date_id', 'seconds_in_bucket', 'stock_id', 'target', 'revealed_target']])\n",
    "        else:\n",
    "            print(df[['date_id', 'seconds_in_bucket', 'stock_id', 'target']])\n",
    "    for model in models:\n",
    "        print(f\"---- train start, counter: {counter} ----\")\n",
    "        if model.is_latest:\n",
    "            print(\"Update latest model\")\n",
    "            now_time = time.time()\n",
    "            booster = lgb.train(\n",
    "                lgb_params,\n",
    "                trains,\n",
    "                num_boost_round=update_num_boost_round,\n",
    "                keep_training_booster=True,\n",
    "                init_model=model.booster,\n",
    "            )\n",
    "            train_time = time.time() - now_time\n",
    "            updated_model = Model(\n",
    "                booster=booster,\n",
    "                fold=1,\n",
    "                best_iteration=booster.best_iteration, \n",
    "                feature_importance=booster.feature_importance(),\n",
    "                score=-1, \n",
    "                train_time=train_time, \n",
    "                weight=-1, \n",
    "                mem_usage=-1,\n",
    "                is_latest=True,\n",
    "                train_func=\"lightgbm update by test\")\n",
    "            r_models.append(updated_model)\n",
    "            if is_append:\n",
    "                print(\"Adding previous model\")\n",
    "                model.is_latest = False\n",
    "                r_models.append(model)\n",
    "        else:\n",
    "            print(\"Dose not latest, just append\")\n",
    "            r_models.append(model)\n",
    "        counter = counter + 1\n",
    "    print(f\"---- train end, train time: {train_time}, updated model len {len(r_models)} ----\")\n",
    "    return r_models\n",
    "\n",
    "if USE_CONTINUOUS_UPDATE:\n",
    "    try:\n",
    "        print(\"Update model with test date\")\n",
    "        df_test = load_test_dataset()\n",
    "        if IS_MIN_LEARN:\n",
    "            print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n",
    "            df_test = df_test[df_test[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n",
    "        df_test = generate_basic_features(df_test)\n",
    "        df_test = generate_enhance_features(df_test)\n",
    "        update_global_train_cache(df_test, 1)\n",
    "        #model_dict[KEY] = keep_train_models(global_train_cache, model_dict[KEY], features, \"target\", is_append=True)\n",
    "        print(\"Update model with test date end\")\n",
    "    except Exception as e:\n",
    "        print(\"Cannot get test date\", e)\n",
    "\n",
    "collect()\n",
    "print(GetMemUsage())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98b13786",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:02:23.457122Z",
     "iopub.status.busy": "2023-12-17T05:02:23.456241Z",
     "iopub.status.idle": "2023-12-17T05:02:23.476397Z",
     "shell.execute_reply": "2023-12-17T05:02:23.475344Z"
    },
    "papermill": {
     "duration": 0.057989,
     "end_time": "2023-12-17T05:02:23.478470",
     "exception": false,
     "start_time": "2023-12-17T05:02:23.420481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: -1, model len: 5\n",
      "           fold     score  best_iteration  train_time\n",
      "count  5.000000  5.000000             5.0    5.000000\n",
      "mean   2.000000  5.988596          5000.0  458.897379\n",
      "std    1.581139  0.009782             0.0    6.566664\n",
      "min    0.000000  5.976397          5000.0  452.205882\n",
      "25%    1.000000  5.984169          5000.0  455.560069\n",
      "50%    2.000000  5.986117          5000.0  457.106990\n",
      "75%    3.000000  5.994567          5000.0  460.142302\n",
      "max    4.000000  6.001732          5000.0  469.471649\n"
     ]
    }
   ],
   "source": [
    "# Show results\n",
    "\n",
    "for key, models in model_dict.items():\n",
    "    print(f\"Key: {key}, model len: {len(models)}\")\n",
    "    data = []\n",
    "    for model in models:\n",
    "        score = model.score\n",
    "        best_iteration = model.best_iteration\n",
    "        fold = model.fold\n",
    "        train_time = model.train_time\n",
    "        data.append({\"key\": key, \"fold\": fold, \"score\": score, \"best_iteration\": best_iteration, \"train_time\": train_time})\n",
    "\n",
    "    df_model = pd.DataFrame(data)\n",
    "    print(df_model.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "582da7f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:02:23.550789Z",
     "iopub.status.busy": "2023-12-17T05:02:23.550433Z",
     "iopub.status.idle": "2023-12-17T05:02:23.575662Z",
     "shell.execute_reply": "2023-12-17T05:02:23.574660Z"
    },
    "papermill": {
     "duration": 0.0637,
     "end_time": "2023-12-17T05:02:23.577986",
     "exception": false,
     "start_time": "2023-12-17T05:02:23.514286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fold</th>\n",
       "      <th>score</th>\n",
       "      <th>best_iteration</th>\n",
       "      <th>train_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.988596</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>458.897379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.581139</td>\n",
       "      <td>0.009782</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.566664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.976397</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>452.205882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.984169</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>455.560069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.986117</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>457.106990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.994567</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>460.142302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.001732</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>469.471649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           fold     score  best_iteration  train_time\n",
       "count  5.000000  5.000000             5.0    5.000000\n",
       "mean   2.000000  5.988596          5000.0  458.897379\n",
       "std    1.581139  0.009782             0.0    6.566664\n",
       "min    0.000000  5.976397          5000.0  452.205882\n",
       "25%    1.000000  5.984169          5000.0  455.560069\n",
       "50%    2.000000  5.986117          5000.0  457.106990\n",
       "75%    3.000000  5.994567          5000.0  460.142302\n",
       "max    4.000000  6.001732          5000.0  469.471649"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check model quality\n",
    "data = []\n",
    "\n",
    "for key, i_models in model_dict.items():\n",
    "    for model in i_models:\n",
    "        score = model.score\n",
    "        best_iteration = model.best_iteration\n",
    "        fold = model.fold\n",
    "        train_time = model.train_time\n",
    "        data.append({\"key\": key, \"fold\": fold, \"score\": score, \"best_iteration\": best_iteration, \"train_time\": train_time})\n",
    "\n",
    "df_model = pd.DataFrame(data)\n",
    "df_model.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8dfaab3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:02:23.650982Z",
     "iopub.status.busy": "2023-12-17T05:02:23.650226Z",
     "iopub.status.idle": "2023-12-17T05:02:23.674131Z",
     "shell.execute_reply": "2023-12-17T05:02:23.673154Z"
    },
    "papermill": {
     "duration": 0.062477,
     "end_time": "2023-12-17T05:02:23.676557",
     "exception": false,
     "start_time": "2023-12-17T05:02:23.614080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>index_mean_match_balance</th>\n",
       "      <td>4405.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_std_price</th>\n",
       "      <td>4187.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_wap</th>\n",
       "      <td>4014.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_std_wap</th>\n",
       "      <td>3808.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_std_match_balance</th>\n",
       "      <td>3754.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_sizes_mean</th>\n",
       "      <td>2915.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matched_size</th>\n",
       "      <td>2745.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_ptp_price</th>\n",
       "      <td>2705.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_balance</th>\n",
       "      <td>2636.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_median_price</th>\n",
       "      <td>2606.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_median_size</th>\n",
       "      <td>2586.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wap_vix_7</th>\n",
       "      <td>2525.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_sizes_std</th>\n",
       "      <td>2426.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <td>2388.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_balance_vix_7</th>\n",
       "      <td>2322.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revealed_target</th>\n",
       "      <td>2317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wap_diff_7</th>\n",
       "      <td>2304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imbalance_size</th>\n",
       "      <td>2243.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_wap_vix_7</th>\n",
       "      <td>2243.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_ptp_size</th>\n",
       "      <td>2222.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>global_std_size</th>\n",
       "      <td>2163.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_match_balance_vix_7</th>\n",
       "      <td>2096.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_balance_diff_7</th>\n",
       "      <td>2089.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stock_id</th>\n",
       "      <td>2038.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_wap_diff_7</th>\n",
       "      <td>2026.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price</th>\n",
       "      <td>1989.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_match_balance_diff_7</th>\n",
       "      <td>1989.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price</th>\n",
       "      <td>1931.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wap_diff_5</th>\n",
       "      <td>1849.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wap_vix_5</th>\n",
       "      <td>1842.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reference_price</th>\n",
       "      <td>1826.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revealed_target_diff_7</th>\n",
       "      <td>1815.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_prices_mean</th>\n",
       "      <td>1798.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_pressure</th>\n",
       "      <td>1789.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_wap_vix_5</th>\n",
       "      <td>1645.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_bid_price_reference_price_imb2</th>\n",
       "      <td>1644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_balance_vix_5</th>\n",
       "      <td>1615.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wap_diff_3</th>\n",
       "      <td>1576.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_wap_diff_5</th>\n",
       "      <td>1524.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revealed_target_diff_5</th>\n",
       "      <td>1515.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_match_balance_vix_5</th>\n",
       "      <td>1489.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_prices_std</th>\n",
       "      <td>1486.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reference_price_bid_price_imb</th>\n",
       "      <td>1479.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wap</th>\n",
       "      <td>1387.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revealed_target_diff_3</th>\n",
       "      <td>1376.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_wap_diff_3</th>\n",
       "      <td>1361.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mid_price</th>\n",
       "      <td>1349.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wap_vix_3</th>\n",
       "      <td>1347.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matched_imbalance</th>\n",
       "      <td>1322.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_match_balance_diff_5</th>\n",
       "      <td>1312.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reference_price_ask_price_imb</th>\n",
       "      <td>1304.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dow</th>\n",
       "      <td>1302.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_balance_diff_5</th>\n",
       "      <td>1299.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_balance_vix_3</th>\n",
       "      <td>1295.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>price_spread</th>\n",
       "      <td>1291.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_wap_vix_3</th>\n",
       "      <td>1223.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_match_balance_vix_3</th>\n",
       "      <td>1222.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>near_price</th>\n",
       "      <td>1195.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index_mean_match_balance_diff_3</th>\n",
       "      <td>1186.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reference_price_wap_imb</th>\n",
       "      <td>1139.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matched_size_bid_size_imbalance_size_imb2</th>\n",
       "      <td>1136.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>match_balance_diff_3</th>\n",
       "      <td>1135.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_size</th>\n",
       "      <td>1127.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matched_size_ask_size_imbalance_size_imb2</th>\n",
       "      <td>1121.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>volume</th>\n",
       "      <td>1118.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_size</th>\n",
       "      <td>1105.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_bid_price_imb</th>\n",
       "      <td>1098.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>matched_size_bid_size_ask_size_imb2</th>\n",
       "      <td>1078.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_prices_kurt</th>\n",
       "      <td>1073.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_prices_skew</th>\n",
       "      <td>1046.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_size_ask_size_imbalance_size_imb2</th>\n",
       "      <td>1002.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_wap_reference_price_imb2</th>\n",
       "      <td>989.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far_price</th>\n",
       "      <td>975.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_sizes_kurt</th>\n",
       "      <td>969.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market_urgency</th>\n",
       "      <td>942.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_wap_imb</th>\n",
       "      <td>932.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spread_intensity</th>\n",
       "      <td>924.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bid_price_wap_imb</th>\n",
       "      <td>914.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>all_sizes_skew</th>\n",
       "      <td>910.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imbalance_momentum</th>\n",
       "      <td>894.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revealed_target_vix_3</th>\n",
       "      <td>779.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revealed_target_vix_7</th>\n",
       "      <td>734.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_wap_reference_price_imb2</th>\n",
       "      <td>730.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>revealed_target_vix_5</th>\n",
       "      <td>707.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>near_price_bid_price_imb</th>\n",
       "      <td>633.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far_price_near_price_imb</th>\n",
       "      <td>606.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>near_price_ask_price_imb</th>\n",
       "      <td>567.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>near_price_wap_imb</th>\n",
       "      <td>494.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>minute</th>\n",
       "      <td>416.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far_price_bid_price_imb</th>\n",
       "      <td>410.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far_price_ask_price_imb</th>\n",
       "      <td>394.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reference_price_near_price_imb</th>\n",
       "      <td>382.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>far_price_wap_imb</th>\n",
       "      <td>343.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ask_price_bid_price_wap_imb2</th>\n",
       "      <td>334.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>depth_pressure</th>\n",
       "      <td>320.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reference_price_far_price_imb</th>\n",
       "      <td>304.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>size_imbalance</th>\n",
       "      <td>267.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seconds</th>\n",
       "      <td>248.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>liquidity_imbalance</th>\n",
       "      <td>238.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <td>90.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          importance\n",
       "index_mean_match_balance                      4405.4\n",
       "global_std_price                              4187.8\n",
       "index_mean_wap                                4014.6\n",
       "index_std_wap                                 3808.4\n",
       "index_std_match_balance                       3754.6\n",
       "all_sizes_mean                                2915.4\n",
       "matched_size                                  2745.2\n",
       "global_ptp_price                              2705.8\n",
       "match_balance                                 2636.4\n",
       "global_median_price                           2606.8\n",
       "global_median_size                            2586.0\n",
       "wap_vix_7                                     2525.2\n",
       "all_sizes_std                                 2426.8\n",
       "seconds_in_bucket                             2388.2\n",
       "match_balance_vix_7                           2322.2\n",
       "revealed_target                               2317.0\n",
       "wap_diff_7                                    2304.0\n",
       "imbalance_size                                2243.8\n",
       "index_mean_wap_vix_7                          2243.4\n",
       "global_ptp_size                               2222.6\n",
       "global_std_size                               2163.4\n",
       "index_mean_match_balance_vix_7                2096.6\n",
       "match_balance_diff_7                          2089.6\n",
       "stock_id                                      2038.6\n",
       "index_mean_wap_diff_7                         2026.4\n",
       "ask_price                                     1989.6\n",
       "index_mean_match_balance_diff_7               1989.2\n",
       "bid_price                                     1931.2\n",
       "wap_diff_5                                    1849.8\n",
       "wap_vix_5                                     1842.6\n",
       "reference_price                               1826.2\n",
       "revealed_target_diff_7                        1815.0\n",
       "all_prices_mean                               1798.2\n",
       "price_pressure                                1789.0\n",
       "index_mean_wap_vix_5                          1645.8\n",
       "ask_price_bid_price_reference_price_imb2      1644.0\n",
       "match_balance_vix_5                           1615.4\n",
       "wap_diff_3                                    1576.2\n",
       "index_mean_wap_diff_5                         1524.0\n",
       "revealed_target_diff_5                        1515.8\n",
       "index_mean_match_balance_vix_5                1489.0\n",
       "all_prices_std                                1486.6\n",
       "reference_price_bid_price_imb                 1479.0\n",
       "wap                                           1387.0\n",
       "revealed_target_diff_3                        1376.6\n",
       "index_mean_wap_diff_3                         1361.8\n",
       "mid_price                                     1349.0\n",
       "wap_vix_3                                     1347.2\n",
       "matched_imbalance                             1322.8\n",
       "index_mean_match_balance_diff_5               1312.8\n",
       "reference_price_ask_price_imb                 1304.2\n",
       "dow                                           1302.6\n",
       "match_balance_diff_5                          1299.4\n",
       "match_balance_vix_3                           1295.6\n",
       "price_spread                                  1291.2\n",
       "index_mean_wap_vix_3                          1223.6\n",
       "index_mean_match_balance_vix_3                1222.4\n",
       "near_price                                    1195.4\n",
       "index_mean_match_balance_diff_3               1186.8\n",
       "reference_price_wap_imb                       1139.4\n",
       "matched_size_bid_size_imbalance_size_imb2     1136.8\n",
       "match_balance_diff_3                          1135.6\n",
       "bid_size                                      1127.4\n",
       "matched_size_ask_size_imbalance_size_imb2     1121.8\n",
       "volume                                        1118.6\n",
       "ask_size                                      1105.2\n",
       "ask_price_bid_price_imb                       1098.6\n",
       "matched_size_bid_size_ask_size_imb2           1078.2\n",
       "all_prices_kurt                               1073.4\n",
       "all_prices_skew                               1046.4\n",
       "bid_size_ask_size_imbalance_size_imb2         1002.6\n",
       "bid_price_wap_reference_price_imb2             989.6\n",
       "far_price                                      975.6\n",
       "all_sizes_kurt                                 969.0\n",
       "market_urgency                                 942.4\n",
       "ask_price_wap_imb                              932.4\n",
       "spread_intensity                               924.2\n",
       "bid_price_wap_imb                              914.2\n",
       "all_sizes_skew                                 910.2\n",
       "imbalance_momentum                             894.6\n",
       "revealed_target_vix_3                          779.4\n",
       "revealed_target_vix_7                          734.2\n",
       "ask_price_wap_reference_price_imb2             730.4\n",
       "revealed_target_vix_5                          707.6\n",
       "near_price_bid_price_imb                       633.4\n",
       "far_price_near_price_imb                       606.2\n",
       "near_price_ask_price_imb                       567.8\n",
       "near_price_wap_imb                             494.6\n",
       "minute                                         416.0\n",
       "far_price_bid_price_imb                        410.6\n",
       "far_price_ask_price_imb                        394.4\n",
       "reference_price_near_price_imb                 382.2\n",
       "far_price_wap_imb                              343.4\n",
       "ask_price_bid_price_wap_imb2                   334.0\n",
       "depth_pressure                                 320.4\n",
       "reference_price_far_price_imb                  304.4\n",
       "size_imbalance                                 267.8\n",
       "seconds                                        248.6\n",
       "liquidity_imbalance                            238.4\n",
       "imbalance_buy_sell_flag                         90.8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty DataFrame for aggregated importances\n",
    "aggregated_importance = pd.DataFrame(index=features, columns=['importance'])\n",
    "\n",
    "# Aggregate the importances from each model\n",
    "for key, i_models in model_dict.items():\n",
    "    for model in i_models:\n",
    "        importance = pd.DataFrame({'feature': features, 'importance': model.feature_importance})\n",
    "        aggregated_importance = aggregated_importance.add(importance.set_index('feature'), fill_value=0)\n",
    "\n",
    "aggregated_importance['importance'] /= len(df_model)\n",
    "\n",
    "pd_display_max()\n",
    "# Sort the features by importance\n",
    "aggregated_importance = aggregated_importance.sort_values(by='importance', ascending=False)\n",
    "aggregated_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f63bd",
   "metadata": {
    "papermill": {
     "duration": 0.03587,
     "end_time": "2023-12-17T05:02:23.749379",
     "exception": false,
     "start_time": "2023-12-17T05:02:23.713509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa94bd1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:02:23.822894Z",
     "iopub.status.busy": "2023-12-17T05:02:23.822515Z",
     "iopub.status.idle": "2023-12-17T05:02:25.526310Z",
     "shell.execute_reply": "2023-12-17T05:02:25.525343Z"
    },
    "papermill": {
     "duration": 1.742723,
     "end_time": "2023-12-17T05:02:25.528369",
     "exception": false,
     "start_time": "2023-12-17T05:02:23.785646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stock_id                                       int16\n",
       "seconds_in_bucket                              int16\n",
       "imbalance_size                               float32\n",
       "imbalance_buy_sell_flag                         int8\n",
       "reference_price                              float32\n",
       "matched_size                                 float32\n",
       "far_price                                    float32\n",
       "near_price                                   float32\n",
       "bid_price                                    float32\n",
       "bid_size                                     float32\n",
       "ask_price                                    float32\n",
       "ask_size                                     float32\n",
       "wap                                          float32\n",
       "all_sizes_skew                               float32\n",
       "spread_intensity                             float32\n",
       "reference_price_near_price_imb               float32\n",
       "global_ptp_price                             float32\n",
       "global_std_price                             float32\n",
       "all_prices_mean                              float32\n",
       "reference_price_bid_price_imb                float32\n",
       "matched_size_bid_size_imbalance_size_imb2    float32\n",
       "far_price_ask_price_imb                      float32\n",
       "price_spread                                 float32\n",
       "match_balance                                float32\n",
       "far_price_near_price_imb                     float32\n",
       "global_median_price                          float32\n",
       "reference_price_ask_price_imb                float32\n",
       "bid_price_wap_imb                            float32\n",
       "global_median_size                           float32\n",
       "near_price_bid_price_imb                     float32\n",
       "ask_price_wap_imb                            float32\n",
       "seconds                                         int8\n",
       "all_sizes_kurt                               float32\n",
       "dow                                             int8\n",
       "near_price_ask_price_imb                     float32\n",
       "bid_size_ask_size_imbalance_size_imb2        float32\n",
       "ask_price_bid_price_wap_imb2                 float32\n",
       "minute                                          int8\n",
       "depth_pressure                               float32\n",
       "size_imbalance                               float32\n",
       "market_urgency                               float32\n",
       "global_std_size                              float32\n",
       "all_sizes_mean                               float32\n",
       "ask_price_bid_price_reference_price_imb2     float32\n",
       "near_price_wap_imb                           float32\n",
       "global_ptp_size                              float32\n",
       "all_sizes_std                                float32\n",
       "far_price_bid_price_imb                      float32\n",
       "bid_price_wap_reference_price_imb2           float32\n",
       "far_price_wap_imb                            float32\n",
       "matched_imbalance                            float32\n",
       "matched_size_ask_size_imbalance_size_imb2    float32\n",
       "ask_price_bid_price_imb                      float32\n",
       "liquidity_imbalance                          float32\n",
       "mid_price                                    float32\n",
       "price_pressure                               float32\n",
       "ask_price_wap_reference_price_imb2           float32\n",
       "imbalance_momentum                           float32\n",
       "reference_price_wap_imb                      float32\n",
       "volume                                       float32\n",
       "matched_size_bid_size_ask_size_imb2          float32\n",
       "reference_price_far_price_imb                float32\n",
       "all_prices_skew                              float32\n",
       "all_prices_std                               float32\n",
       "all_prices_kurt                              float32\n",
       "revealed_target_diff_3                       float32\n",
       "index_std_wap                                float32\n",
       "revealed_target_vix_3                        float32\n",
       "index_mean_wap_diff_5                        float32\n",
       "match_balance_vix_5                          float32\n",
       "revealed_target_diff_7                       float32\n",
       "index_mean_match_balance_vix_5               float32\n",
       "index_mean_wap_vix_3                         float32\n",
       "index_mean_match_balance_diff_7              float32\n",
       "index_mean_match_balance_diff_3              float32\n",
       "match_balance_diff_3                         float32\n",
       "index_mean_wap_vix_7                         float32\n",
       "index_mean_match_balance_vix_7               float32\n",
       "revealed_target                              float32\n",
       "index_mean_wap_diff_3                        float32\n",
       "index_mean_match_balance                     float32\n",
       "revealed_target_vix_7                        float32\n",
       "match_balance_diff_5                         float32\n",
       "match_balance_vix_7                          float32\n",
       "revealed_target_diff_5                       float32\n",
       "index_mean_match_balance_vix_3               float32\n",
       "revealed_target_vix_5                        float32\n",
       "wap_vix_3                                    float32\n",
       "index_std_match_balance                      float32\n",
       "wap_diff_3                                   float32\n",
       "index_mean_wap_vix_5                         float32\n",
       "index_mean_match_balance_diff_5              float32\n",
       "wap_diff_5                                   float32\n",
       "match_balance_diff_7                         float32\n",
       "index_mean_wap                               float32\n",
       "wap_vix_7                                    float32\n",
       "wap_vix_5                                    float32\n",
       "match_balance_vix_3                          float32\n",
       "index_mean_wap_diff_7                        float32\n",
       "wap_diff_7                                   float32\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_types = df_train[features].dtypes\n",
    "features_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f67b7e89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:02:25.602983Z",
     "iopub.status.busy": "2023-12-17T05:02:25.602266Z",
     "iopub.status.idle": "2023-12-17T05:02:25.608551Z",
     "shell.execute_reply": "2023-12-17T05:02:25.607614Z"
    },
    "papermill": {
     "duration": 0.046144,
     "end_time": "2023-12-17T05:02:25.610700",
     "exception": false,
     "start_time": "2023-12-17T05:02:25.564556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_dtypes(df):\n",
    "    df_types = df[features].dtypes\n",
    "    different_types = [col for col in df_types.index if col in features_types and df_types[col] != features_types[col]]\n",
    "    print(f\"Different Types: {different_types}\")\n",
    "    return different_types\n",
    "\n",
    "def update_dtypes_by_origin(df):\n",
    "    diff_types = convert_dtypes(df)\n",
    "    for col in diff_types:\n",
    "        df[col] = df[col].astype(features_types[col])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22947906",
   "metadata": {
    "papermill": {
     "duration": 0.036416,
     "end_time": "2023-12-17T05:02:25.684292",
     "exception": false,
     "start_time": "2023-12-17T05:02:25.647876",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Clear trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4e42532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:02:25.757662Z",
     "iopub.status.busy": "2023-12-17T05:02:25.756756Z",
     "iopub.status.idle": "2023-12-17T05:02:26.458866Z",
     "shell.execute_reply": "2023-12-17T05:02:26.457844Z"
    },
    "papermill": {
     "duration": 0.741092,
     "end_time": "2023-12-17T05:02:26.461082",
     "exception": false,
     "start_time": "2023-12-17T05:02:25.719990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAM memory GB usage = 18.81\n"
     ]
    }
   ],
   "source": [
    "# Clean up\n",
    "pd_clear_display_max()\n",
    "del key_models\n",
    "if IS_USE_SAVED_MODEL:\n",
    "    print(\"Delete model_dict\")\n",
    "    del model_dict\n",
    "del df_train\n",
    "collect()\n",
    "print(GetMemUsage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9905c2",
   "metadata": {
    "papermill": {
     "duration": 0.036341,
     "end_time": "2023-12-17T05:02:26.534127",
     "exception": false,
     "start_time": "2023-12-17T05:02:26.497786",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe5d3dbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:02:26.608470Z",
     "iopub.status.busy": "2023-12-17T05:02:26.607737Z",
     "iopub.status.idle": "2023-12-17T05:02:26.621432Z",
     "shell.execute_reply": "2023-12-17T05:02:26.620504Z"
    },
    "papermill": {
     "duration": 0.053007,
     "end_time": "2023-12-17T05:02:26.623434",
     "exception": false,
     "start_time": "2023-12-17T05:02:26.570427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 µs, sys: 0 ns, total: 8 µs\n",
      "Wall time: 12.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y_min, y_max = -64, 64\n",
    "\n",
    "# 📉 Define a function to adjust prices based on volumes\n",
    "def zero_sum(prices, volumes):\n",
    "    std_error = np.sqrt(volumes)  # 🧮 Calculate standard error based on volumes\n",
    "    step = np.sum(prices) / np.sum(std_error)  # 🧮 Calculate the step size based on prices and standard error\n",
    "    out = prices - std_error * step  # 💰 Adjust prices by subtracting the standardized step size\n",
    "    return out\n",
    "\n",
    "def zero_clip(df, predictions):\n",
    "    # Adjust the predictions based on the order book imbalance\n",
    "    zerosum_predictions = zero_sum(predictions, df['bid_size'] + df['ask_size'])\n",
    "    clipped_predictions = np.clip(zerosum_predictions, y_min, y_max)\n",
    "    clipped_predictions.replace([np.nan, np.inf, -np.inf], 0, inplace=True)\n",
    "    clipped_predictions = clipped_predictions.astype('float64').values  \n",
    "    return clipped_predictions\n",
    "\n",
    "def predict_scaler(df):\n",
    "    pred_mean = df['pred'].mean()\n",
    "    pred_std = df['pred'].std()\n",
    "    \n",
    "    target_mean = global_target['mean']\n",
    "    target_std = global_target['std']\n",
    "\n",
    "    df['scaled_pred'] = (df['pred'] - pred_mean) / pred_std * target_std + target_mean\n",
    "    return df\n",
    "\n",
    "def model_infer(key, df_feat, use_additional_model=False):\n",
    "    def predictor(boosters):\n",
    "        print(f\"Predictor target models len {len(boosters)}\")\n",
    "        #print(f\"Predictor Feat len {len(df_feat)}\")\n",
    "        if USE_OPTUNA:\n",
    "            predictions_list = [np.mean(booster.predict(df_feat), 0) for booster in boosters]\n",
    "        else:\n",
    "            predictions_list = [booster.predict(df_feat) for booster in boosters]\n",
    "        predictions = np.mean(predictions_list, 0)\n",
    "        std_predictions = np.std(predictions_list, 0)\n",
    "        #print(\"std_predictions\", std_predictions)\n",
    "        return predictions\n",
    "    \n",
    "    if IS_USE_SAVED_MODEL:\n",
    "        model_paths = model_dict_saved[key]\n",
    "        models = [lgb.Booster(model_file=model_path) for model_path in model_paths]\n",
    "        predictions = predictor(models)\n",
    "        del models\n",
    "    else:\n",
    "        if USE_ADDITIONAL_TRAIN and use_additional_model:\n",
    "            print(\"Use additional model\")\n",
    "            boosters = [m.booster for m in additional_model_dict[key]]\n",
    "        else:\n",
    "            boosters = [m.booster for m in model_dict[key]]\n",
    "        predictions = predictor(boosters)\n",
    "    collect()\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a10c193a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:02:26.699389Z",
     "iopub.status.busy": "2023-12-17T05:02:26.698687Z",
     "iopub.status.idle": "2023-12-17T05:08:51.575322Z",
     "shell.execute_reply": "2023-12-17T05:08:51.574365Z"
    },
    "papermill": {
     "duration": 384.917274,
     "end_time": "2023-12-17T05:08:51.577513",
     "exception": false,
     "start_time": "2023-12-17T05:02:26.660239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer Local\n",
      "------- counter 1 start -------\n",
      "copy_revealed_targets len 11000\n",
      "Update revealed_targets\n",
      "df_cache len 200\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.25 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 0.0\n",
      "df_cache_with_features len 200\n",
      "Predictor target models len 5\n",
      "prediction average 0.7796153813883541\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.357372657384559e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 1, execution_time 1.9559648036956787 end -------\n",
      "------- counter 2 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 400\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.25 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 1.0\n",
      "df_cache_with_features len 400\n",
      "Predictor target models len 5\n",
      "prediction average 0.13066907231148606\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 8.750281068614641e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 2, execution_time 1.9314205646514893 end -------\n",
      "------- counter 3 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 600\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 2.0\n",
      "df_cache_with_features len 600\n",
      "Predictor target models len 5\n",
      "prediction average -0.0961817839311734\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.7863004964091544e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 3, execution_time 1.922316312789917 end -------\n",
      "------- counter 4 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 800\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 3.0\n",
      "df_cache_with_features len 800\n",
      "Predictor target models len 5\n",
      "prediction average 0.11531563745191359\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 7.080319441854499e-11\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 4, execution_time 1.9432332515716553 end -------\n",
      "------- counter 5 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 1000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 4.0\n",
      "df_cache_with_features len 1000\n",
      "Predictor target models len 5\n",
      "prediction average 0.12113491623399877\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -4.146002563487628e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 5, execution_time 2.016526460647583 end -------\n",
      "------- counter 6 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 1200\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.25 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 5.0\n",
      "df_cache_with_features len 1200\n",
      "Predictor target models len 5\n",
      "prediction average 0.33368122615252005\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -4.024995643447937e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 6, execution_time 1.9246292114257812 end -------\n",
      "------- counter 7 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 1400\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 6.0\n",
      "df_cache_with_features len 1400\n",
      "Predictor target models len 5\n",
      "prediction average 0.38094640236254745\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.5739235898214475e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 7, execution_time 1.936142921447754 end -------\n",
      "------- counter 8 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 1600\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 7.0\n",
      "df_cache_with_features len 1600\n",
      "Predictor target models len 5\n",
      "prediction average 1.1642766092849133\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.3311954427663863e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 8, execution_time 1.9491045475006104 end -------\n",
      "------- counter 9 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 1800\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 8.0\n",
      "df_cache_with_features len 1800\n",
      "Predictor target models len 5\n",
      "prediction average 1.5637108358023166\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -9.945755570228698e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 9, execution_time 1.934757947921753 end -------\n",
      "------- counter 10 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 9.0\n",
      "df_cache_with_features len 2000\n",
      "Predictor target models len 5\n",
      "prediction average 1.49691360361828\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 7.705257187851089e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 10, execution_time 1.946690559387207 end -------\n",
      "------- counter 11 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 10.0\n",
      "df_cache_with_features len 2200\n",
      "Predictor target models len 5\n",
      "prediction average 0.1455309028428979\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.6834678834575245e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 11, execution_time 1.9226832389831543 end -------\n",
      "------- counter 12 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 11.0\n",
      "df_cache_with_features len 2400\n",
      "Predictor target models len 5\n",
      "prediction average -0.06308130224822144\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.3605389198877446e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 12, execution_time 1.9582173824310303 end -------\n",
      "------- counter 13 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 12.0\n",
      "df_cache_with_features len 2600\n",
      "Predictor target models len 5\n",
      "prediction average -0.10766094323271122\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.4147023349740948e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 13, execution_time 1.9814083576202393 end -------\n",
      "------- counter 14 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 13.0\n",
      "df_cache_with_features len 2800\n",
      "Predictor target models len 5\n",
      "prediction average 0.008365766557289742\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -4.05540808046112e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 14, execution_time 1.9710772037506104 end -------\n",
      "------- counter 15 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 14.0\n",
      "df_cache_with_features len 3000\n",
      "Predictor target models len 5\n",
      "prediction average 0.0727476829177955\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -5.39603663085586e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 15, execution_time 1.9596493244171143 end -------\n",
      "------- counter 16 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 15.0\n",
      "df_cache_with_features len 3200\n",
      "Predictor target models len 5\n",
      "prediction average -0.3303535395006261\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.7515836017167886e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 16, execution_time 1.9464404582977295 end -------\n",
      "------- counter 17 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 16.0\n",
      "df_cache_with_features len 3400\n",
      "Predictor target models len 5\n",
      "prediction average -0.2874489201845061\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -9.400197509279451e-11\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 17, execution_time 2.025547742843628 end -------\n",
      "------- counter 18 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 17.0\n",
      "df_cache_with_features len 3600\n",
      "Predictor target models len 5\n",
      "prediction average -0.2379267657270683\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -5.112703149734443e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 18, execution_time 1.9480478763580322 end -------\n",
      "------- counter 19 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.32 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 18.0\n",
      "df_cache_with_features len 3800\n",
      "Predictor target models len 5\n",
      "prediction average -0.2756707731635619\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 4.273092004680734e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 19, execution_time 2.0818979740142822 end -------\n",
      "------- counter 20 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 19.0\n",
      "df_cache_with_features len 4000\n",
      "Predictor target models len 5\n",
      "prediction average 0.048285946579360035\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.112125718767288e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 20, execution_time 1.9507074356079102 end -------\n",
      "------- counter 21 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 20.0\n",
      "df_cache_with_features len 4200\n",
      "Predictor target models len 5\n",
      "prediction average -0.13129698314211644\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.0126040450586516e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 21, execution_time 1.9648945331573486 end -------\n",
      "------- counter 22 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 21.0\n",
      "df_cache_with_features len 4400\n",
      "Predictor target models len 5\n",
      "prediction average -0.047951986016923805\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -6.648804786735241e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 22, execution_time 1.9437718391418457 end -------\n",
      "------- counter 23 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 22.0\n",
      "df_cache_with_features len 4600\n",
      "Predictor target models len 5\n",
      "prediction average 0.026060155351926736\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.0426135332863851e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 23, execution_time 1.9738502502441406 end -------\n",
      "------- counter 24 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 23.0\n",
      "df_cache_with_features len 4800\n",
      "Predictor target models len 5\n",
      "prediction average -0.2202190565179259\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.910055592053595e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 24, execution_time 1.9429981708526611 end -------\n",
      "------- counter 25 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 24.0\n",
      "df_cache_with_features len 5000\n",
      "Predictor target models len 5\n",
      "prediction average -0.05030341061426226\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -9.93468631804717e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 25, execution_time 1.9335219860076904 end -------\n",
      "------- counter 26 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.32 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 25.0\n",
      "df_cache_with_features len 5200\n",
      "Predictor target models len 5\n",
      "prediction average -0.05077364415897069\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -4.218280640344574e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 26, execution_time 2.0050694942474365 end -------\n",
      "------- counter 27 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 26.0\n",
      "df_cache_with_features len 5400\n",
      "Predictor target models len 5\n",
      "prediction average 0.21505201525182852\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.254303410609282e-11\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 27, execution_time 1.9543912410736084 end -------\n",
      "------- counter 28 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 27.0\n",
      "df_cache_with_features len 5600\n",
      "Predictor target models len 5\n",
      "prediction average 0.1899597815121657\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.2837784996454503e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 28, execution_time 1.9736454486846924 end -------\n",
      "------- counter 29 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 28.0\n",
      "df_cache_with_features len 5800\n",
      "Predictor target models len 5\n",
      "prediction average -0.24572256053833477\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.7056944693626407e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 29, execution_time 1.9750850200653076 end -------\n",
      "------- counter 30 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 29.0\n",
      "df_cache_with_features len 6000\n",
      "Predictor target models len 5\n",
      "prediction average -0.35824357567731624\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 7.286461887190398e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 30, execution_time 1.9854438304901123 end -------\n",
      "------- counter 31 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 30.0\n",
      "df_cache_with_features len 6200\n",
      "Predictor target models len 5\n",
      "prediction average -0.5697223048249168\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.0260151839247555e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 31, execution_time 1.9557456970214844 end -------\n",
      "------- counter 32 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 31.0\n",
      "df_cache_with_features len 6400\n",
      "Predictor target models len 5\n",
      "prediction average -0.26080962538933533\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.00470165165234e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 32, execution_time 2.051459789276123 end -------\n",
      "------- counter 33 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 32.0\n",
      "df_cache_with_features len 6600\n",
      "Predictor target models len 5\n",
      "prediction average -0.48164223867176276\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -6.243250485482576e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 33, execution_time 1.9824891090393066 end -------\n",
      "------- counter 34 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 33.0\n",
      "df_cache_with_features len 6800\n",
      "Predictor target models len 5\n",
      "prediction average -0.5117780450022482\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.9101804582533078e-11\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 34, execution_time 1.972055196762085 end -------\n",
      "------- counter 35 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 34.0\n",
      "df_cache_with_features len 7000\n",
      "Predictor target models len 5\n",
      "prediction average -0.41863438972198863\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.741319897163862e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 35, execution_time 1.9644358158111572 end -------\n",
      "------- counter 36 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 35.0\n",
      "df_cache_with_features len 7200\n",
      "Predictor target models len 5\n",
      "prediction average -0.26663817778045074\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.470831341749857e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 36, execution_time 1.9497556686401367 end -------\n",
      "------- counter 37 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 36.0\n",
      "df_cache_with_features len 7400\n",
      "Predictor target models len 5\n",
      "prediction average -0.26285832794062614\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 3.158546384440797e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 37, execution_time 1.9733586311340332 end -------\n",
      "------- counter 38 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 37.0\n",
      "df_cache_with_features len 7600\n",
      "Predictor target models len 5\n",
      "prediction average -0.2259086794405132\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.4558061423031175e-11\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 38, execution_time 1.9338512420654297 end -------\n",
      "------- counter 39 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 38.0\n",
      "df_cache_with_features len 7800\n",
      "Predictor target models len 5\n",
      "prediction average 0.01556523459245744\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.1625056233199871e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 39, execution_time 1.9702045917510986 end -------\n",
      "------- counter 40 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 39.0\n",
      "df_cache_with_features len 8000\n",
      "Predictor target models len 5\n",
      "prediction average 0.10280931319387623\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.5960289836414176e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 40, execution_time 2.025667667388916 end -------\n",
      "------- counter 41 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.30 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 40.0\n",
      "df_cache_with_features len 8200\n",
      "Predictor target models len 5\n",
      "prediction average 0.08656688268863491\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 7.948473002006295e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 41, execution_time 1.9944016933441162 end -------\n",
      "------- counter 42 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 41.0\n",
      "df_cache_with_features len 8400\n",
      "Predictor target models len 5\n",
      "prediction average -0.008237266832527448\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -7.55045861389192e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 42, execution_time 1.9568407535552979 end -------\n",
      "------- counter 43 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 42.0\n",
      "df_cache_with_features len 8600\n",
      "Predictor target models len 5\n",
      "prediction average 0.04216164346413576\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 3.884313848345755e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 43, execution_time 2.0001139640808105 end -------\n",
      "------- counter 44 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 43.0\n",
      "df_cache_with_features len 8800\n",
      "Predictor target models len 5\n",
      "prediction average 0.1865769187241942\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.752617275447733e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 44, execution_time 1.9638934135437012 end -------\n",
      "------- counter 45 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 44.0\n",
      "df_cache_with_features len 9000\n",
      "Predictor target models len 5\n",
      "prediction average 0.18221183037072436\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -5.187156162378414e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 45, execution_time 1.958075761795044 end -------\n",
      "------- counter 46 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 45.0\n",
      "df_cache_with_features len 9200\n",
      "Predictor target models len 5\n",
      "prediction average 0.24643455574142034\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -4.895563705531458e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 46, execution_time 2.1578805446624756 end -------\n",
      "------- counter 47 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 46.0\n",
      "df_cache_with_features len 9400\n",
      "Predictor target models len 5\n",
      "prediction average 0.27964106451785375\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 3.760649462947185e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 47, execution_time 1.9615955352783203 end -------\n",
      "------- counter 48 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 47.0\n",
      "df_cache_with_features len 9600\n",
      "Predictor target models len 5\n",
      "prediction average 0.16655776601785213\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.742998989584521e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 48, execution_time 1.958780288696289 end -------\n",
      "------- counter 49 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 48.0\n",
      "df_cache_with_features len 9800\n",
      "Predictor target models len 5\n",
      "prediction average 0.10651583218690344\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.7252948342493257e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 49, execution_time 1.9466512203216553 end -------\n",
      "------- counter 50 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.32 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 49.0\n",
      "df_cache_with_features len 10000\n",
      "Predictor target models len 5\n",
      "prediction average 0.22901447117681656\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.657097208924597e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 50, execution_time 2.0014431476593018 end -------\n",
      "------- counter 51 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 50.0\n",
      "df_cache_with_features len 10200\n",
      "Predictor target models len 5\n",
      "prediction average 0.030914286781602764\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.7186559566617633e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 51, execution_time 1.9478487968444824 end -------\n",
      "------- counter 52 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 51.0\n",
      "df_cache_with_features len 10400\n",
      "Predictor target models len 5\n",
      "prediction average 0.10525653792238249\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -9.23229537264092e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 52, execution_time 1.9529268741607666 end -------\n",
      "------- counter 53 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 52.0\n",
      "df_cache_with_features len 10600\n",
      "Predictor target models len 5\n",
      "prediction average 0.09340693045039128\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -8.650535492371603e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 53, execution_time 1.9312200546264648 end -------\n",
      "------- counter 54 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 53.0\n",
      "df_cache_with_features len 10800\n",
      "Predictor target models len 5\n",
      "prediction average 0.042983873579938596\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -6.048619027865243e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 54, execution_time 1.9356968402862549 end -------\n",
      "------- counter 55 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 478,  seconds_in_bucket: 54.0\n",
      "df_cache_with_features len 11000\n",
      "Update global train cache\n",
      "update_global_train_cache\n",
      "Updated global_train_cache, len:  220000\n",
      "220000\n",
      "        date_id  seconds_in_bucket  stock_id  origin  revealed_target  \\\n",
      "0           461                  0         0       0        -2.830029   \n",
      "1           461                  0         1       0        11.060238   \n",
      "2           461                  0         2       0         2.080202   \n",
      "3           461                  0         3       0         2.720356   \n",
      "4           461                  0         4       0        -5.499721   \n",
      "...         ...                ...       ...     ...              ...   \n",
      "219995      480                540       195       0         1.599789   \n",
      "219996      480                540       196       0        -8.440018   \n",
      "219997      480                540       197       0         5.149841   \n",
      "219998      480                540       198       0        -0.249743   \n",
      "219999      480                540       199       0        -7.609725   \n",
      "\n",
      "          target  \n",
      "0       2.900362  \n",
      "1       4.279613  \n",
      "2      -6.049871  \n",
      "3      -4.360080  \n",
      "4       2.360344  \n",
      "...          ...  \n",
      "219995  2.310276  \n",
      "219996 -8.220077  \n",
      "219997  1.169443  \n",
      "219998 -1.540184  \n",
      "219999 -6.530285  \n",
      "\n",
      "[220000 rows x 6 columns]\n",
      "Predictor target models len 5\n",
      "prediction average -0.25444480629528154\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 4.582330390690004e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 55, execution_time 2.515638828277588 end -------\n",
      "------- counter 56 start -------\n",
      "copy_revealed_targets len 11000\n",
      "Update revealed_targets\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.55 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 0.0\n",
      "df_cache_with_features len 11200\n",
      "Predictor target models len 5\n",
      "prediction average -0.15871808404380924\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.3506666505236354e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 56, execution_time 2.411562204360962 end -------\n",
      "------- counter 57 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.52 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 1.0\n",
      "df_cache_with_features len 11400\n",
      "Predictor target models len 5\n",
      "prediction average -0.03179112862142647\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.4379507568283996e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 57, execution_time 2.261605739593506 end -------\n",
      "------- counter 58 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.48 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 2.0\n",
      "df_cache_with_features len 11600\n",
      "Predictor target models len 5\n",
      "prediction average -0.16202086332566087\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.2184627557209067e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 58, execution_time 2.1921682357788086 end -------\n",
      "------- counter 59 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.52 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 3.0\n",
      "df_cache_with_features len 11800\n",
      "Predictor target models len 5\n",
      "prediction average 0.09753773556927006\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.2180109081327828e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 59, execution_time 2.401031970977783 end -------\n",
      "------- counter 60 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.48 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 4.0\n",
      "df_cache_with_features len 12000\n",
      "Predictor target models len 5\n",
      "prediction average 0.2348508249900282\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 8.419462487196938e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 60, execution_time 2.198035478591919 end -------\n",
      "------- counter 61 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.49 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 5.0\n",
      "df_cache_with_features len 12200\n",
      "Predictor target models len 5\n",
      "prediction average 0.17418145291286904\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 3.46084050306672e-11\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 61, execution_time 2.1966869831085205 end -------\n",
      "------- counter 62 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.49 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 6.0\n",
      "df_cache_with_features len 12400\n",
      "Predictor target models len 5\n",
      "prediction average 0.053005408781915675\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.6655180107582056e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 62, execution_time 2.1909432411193848 end -------\n",
      "------- counter 63 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.50 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 7.0\n",
      "df_cache_with_features len 12600\n",
      "Predictor target models len 5\n",
      "prediction average 0.007315829143849335\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.277402488815028e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 63, execution_time 2.1801717281341553 end -------\n",
      "------- counter 64 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.50 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 8.0\n",
      "df_cache_with_features len 12800\n",
      "Predictor target models len 5\n",
      "prediction average -0.4737851401234883\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 8.9859351959376e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 64, execution_time 2.225423812866211 end -------\n",
      "------- counter 65 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 9.0\n",
      "df_cache_with_features len 13000\n",
      "Predictor target models len 5\n",
      "prediction average -0.4744717136829075\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.826643497044756e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 65, execution_time 1.9506869316101074 end -------\n",
      "------- counter 66 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 10.0\n",
      "df_cache_with_features len 13200\n",
      "Predictor target models len 5\n",
      "prediction average -0.3238726547752133\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -5.020422375601186e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 66, execution_time 1.9838910102844238 end -------\n",
      "------- counter 67 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 11.0\n",
      "df_cache_with_features len 13400\n",
      "Predictor target models len 5\n",
      "prediction average -0.1457581049878107\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.3127838844061445e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 67, execution_time 1.963040828704834 end -------\n",
      "------- counter 68 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 12.0\n",
      "df_cache_with_features len 13600\n",
      "Predictor target models len 5\n",
      "prediction average -0.43229772012306866\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.061544988052333e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 68, execution_time 2.004032611846924 end -------\n",
      "------- counter 69 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 13.0\n",
      "df_cache_with_features len 13800\n",
      "Predictor target models len 5\n",
      "prediction average -0.40385371601149905\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.0111456517923898e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 69, execution_time 1.9775099754333496 end -------\n",
      "------- counter 70 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 14.0\n",
      "df_cache_with_features len 14000\n",
      "Predictor target models len 5\n",
      "prediction average -0.3765090571436444\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -3.5876299531167888e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 70, execution_time 1.9650349617004395 end -------\n",
      "------- counter 71 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 15.0\n",
      "df_cache_with_features len 14200\n",
      "Predictor target models len 5\n",
      "prediction average -0.2448440391291021\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.3512601413356861e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 71, execution_time 2.006232500076294 end -------\n",
      "------- counter 72 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 16.0\n",
      "df_cache_with_features len 14400\n",
      "Predictor target models len 5\n",
      "prediction average -0.26483041521022616\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.5446736717782415e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 72, execution_time 2.020059823989868 end -------\n",
      "------- counter 73 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 17.0\n",
      "df_cache_with_features len 14600\n",
      "Predictor target models len 5\n",
      "prediction average -0.06987836926390381\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.1053020330109576e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 73, execution_time 2.006965160369873 end -------\n",
      "------- counter 74 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 18.0\n",
      "df_cache_with_features len 14800\n",
      "Predictor target models len 5\n",
      "prediction average -0.159652365921988\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.8509259153430834e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 74, execution_time 1.9618897438049316 end -------\n",
      "------- counter 75 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.34 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 19.0\n",
      "df_cache_with_features len 15000\n",
      "Predictor target models len 5\n",
      "prediction average 0.018213714210464804\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.446845168118216e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 75, execution_time 2.080644130706787 end -------\n",
      "------- counter 76 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 20.0\n",
      "df_cache_with_features len 15200\n",
      "Predictor target models len 5\n",
      "prediction average -0.12943653645062508\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 4.327524791847281e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 76, execution_time 1.9641294479370117 end -------\n",
      "------- counter 77 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 21.0\n",
      "df_cache_with_features len 15400\n",
      "Predictor target models len 5\n",
      "prediction average -0.10502184090016765\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.4115934671442573e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 77, execution_time 1.9943668842315674 end -------\n",
      "------- counter 78 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 22.0\n",
      "df_cache_with_features len 15600\n",
      "Predictor target models len 5\n",
      "prediction average -0.30967689291211414\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.8437502848911434e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 78, execution_time 1.9739429950714111 end -------\n",
      "------- counter 79 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 23.0\n",
      "df_cache_with_features len 15800\n",
      "Predictor target models len 5\n",
      "prediction average -0.4134342055093373\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 0.01714676210787349\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 79, execution_time 1.953653335571289 end -------\n",
      "------- counter 80 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 24.0\n",
      "df_cache_with_features len 16000\n",
      "Predictor target models len 5\n",
      "prediction average -0.20970728834002422\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 0.1845761263966267\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 80, execution_time 1.97711181640625 end -------\n",
      "------- counter 81 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 25.0\n",
      "df_cache_with_features len 16200\n",
      "Predictor target models len 5\n",
      "prediction average -0.42684119155994193\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 0.1396988109351598\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 81, execution_time 1.9861063957214355 end -------\n",
      "------- counter 82 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 26.0\n",
      "df_cache_with_features len 16400\n",
      "Predictor target models len 5\n",
      "prediction average -0.281500390513698\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 0.13125197110582384\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 82, execution_time 1.9731590747833252 end -------\n",
      "------- counter 83 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 27.0\n",
      "df_cache_with_features len 16600\n",
      "Predictor target models len 5\n",
      "prediction average -0.6540539815266802\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 0.06981127679537215\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 83, execution_time 1.9748625755310059 end -------\n",
      "------- counter 84 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 28.0\n",
      "df_cache_with_features len 16800\n",
      "Predictor target models len 5\n",
      "prediction average -0.14918486637616937\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 0.02002920833721248\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 84, execution_time 1.978947401046753 end -------\n",
      "------- counter 85 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.31 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 29.0\n",
      "df_cache_with_features len 17000\n",
      "Predictor target models len 5\n",
      "prediction average -0.04392141901339279\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 5.274212604433615e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 85, execution_time 2.0752670764923096 end -------\n",
      "------- counter 86 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 30.0\n",
      "df_cache_with_features len 17200\n",
      "Predictor target models len 5\n",
      "prediction average -0.5027585443087719\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 5.060632393139031e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 86, execution_time 2.3925600051879883 end -------\n",
      "------- counter 87 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 31.0\n",
      "df_cache_with_features len 17400\n",
      "Predictor target models len 5\n",
      "prediction average -0.05982172560194883\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -6.840042785505318e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 87, execution_time 1.9550800323486328 end -------\n",
      "------- counter 88 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 32.0\n",
      "df_cache_with_features len 17600\n",
      "Predictor target models len 5\n",
      "prediction average 0.053631046610733535\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.0294444915691657e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 88, execution_time 1.9634711742401123 end -------\n",
      "------- counter 89 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 33.0\n",
      "df_cache_with_features len 17800\n",
      "Predictor target models len 5\n",
      "prediction average 0.09967077185425137\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -8.804206670376403e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 89, execution_time 1.967679500579834 end -------\n",
      "------- counter 90 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 34.0\n",
      "df_cache_with_features len 18000\n",
      "Predictor target models len 5\n",
      "prediction average 0.3721809825529773\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.768703066813316e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 90, execution_time 1.984541893005371 end -------\n",
      "------- counter 91 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 35.0\n",
      "df_cache_with_features len 18200\n",
      "Predictor target models len 5\n",
      "prediction average -0.15369624369023074\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -5.013390271813023e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 91, execution_time 1.968338966369629 end -------\n",
      "------- counter 92 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 36.0\n",
      "df_cache_with_features len 18400\n",
      "Predictor target models len 5\n",
      "prediction average -0.08626079445698334\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.9010924923179574e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 92, execution_time 1.9331965446472168 end -------\n",
      "------- counter 93 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 37.0\n",
      "df_cache_with_features len 18600\n",
      "Predictor target models len 5\n",
      "prediction average -0.20862766840706776\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.0629790061500444e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 93, execution_time 1.951648235321045 end -------\n",
      "------- counter 94 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 38.0\n",
      "df_cache_with_features len 18800\n",
      "Predictor target models len 5\n",
      "prediction average -0.22365343096383763\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.9191609741397998e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 94, execution_time 1.9785425662994385 end -------\n",
      "------- counter 95 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 39.0\n",
      "df_cache_with_features len 19000\n",
      "Predictor target models len 5\n",
      "prediction average -0.24741795792690857\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -4.221471145982036e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 95, execution_time 1.9923095703125 end -------\n",
      "------- counter 96 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 40.0\n",
      "df_cache_with_features len 19200\n",
      "Predictor target models len 5\n",
      "prediction average -0.2887204388478812\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 8.017594463893828e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 96, execution_time 1.9477686882019043 end -------\n",
      "------- counter 97 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 41.0\n",
      "df_cache_with_features len 19400\n",
      "Predictor target models len 5\n",
      "prediction average -0.24954722993714715\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.672544448716451e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 97, execution_time 1.9652414321899414 end -------\n",
      "------- counter 98 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 42.0\n",
      "df_cache_with_features len 19600\n",
      "Predictor target models len 5\n",
      "prediction average -0.3238463197613219\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -5.605592980906238e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 98, execution_time 2.0401506423950195 end -------\n",
      "------- counter 99 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.31 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 43.0\n",
      "df_cache_with_features len 19800\n",
      "Predictor target models len 5\n",
      "prediction average -0.4216685467388775\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.0662459484223064e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 99, execution_time 2.0699594020843506 end -------\n",
      "------- counter 100 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.30 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 44.0\n",
      "df_cache_with_features len 20000\n",
      "Predictor target models len 5\n",
      "prediction average -0.08118468942046589\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.503445113433145e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 100, execution_time 2.0667622089385986 end -------\n",
      "------- counter 101 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 45.0\n",
      "df_cache_with_features len 20200\n",
      "Predictor target models len 5\n",
      "prediction average -0.003019891186651833\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.7618951009268358e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 101, execution_time 1.9878027439117432 end -------\n",
      "------- counter 102 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 46.0\n",
      "df_cache_with_features len 20400\n",
      "Predictor target models len 5\n",
      "prediction average -0.01656685568112602\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 4.513147602303036e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 102, execution_time 1.9617819786071777 end -------\n",
      "------- counter 103 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 47.0\n",
      "df_cache_with_features len 20600\n",
      "Predictor target models len 5\n",
      "prediction average -0.04511829027612091\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 7.200451399569374e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 103, execution_time 1.9690907001495361 end -------\n",
      "------- counter 104 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 48.0\n",
      "df_cache_with_features len 20800\n",
      "Predictor target models len 5\n",
      "prediction average -0.10738701059020184\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.0701087510843763e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 104, execution_time 1.958970308303833 end -------\n",
      "------- counter 105 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 49.0\n",
      "df_cache_with_features len 21000\n",
      "Predictor target models len 5\n",
      "prediction average -0.4126243275995988\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.9446944056488178e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 105, execution_time 1.9906461238861084 end -------\n",
      "------- counter 106 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 50.0\n",
      "df_cache_with_features len 21200\n",
      "Predictor target models len 5\n",
      "prediction average -0.43167786329715985\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -3.5304286871706837e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 106, execution_time 1.9211914539337158 end -------\n",
      "------- counter 107 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 51.0\n",
      "df_cache_with_features len 21400\n",
      "Predictor target models len 5\n",
      "prediction average -0.4486039688618323\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 7.912042168101151e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 107, execution_time 1.9927828311920166 end -------\n",
      "------- counter 108 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 52.0\n",
      "df_cache_with_features len 21600\n",
      "Predictor target models len 5\n",
      "prediction average -0.6137229960724216\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.6843217132134213e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 108, execution_time 1.9716601371765137 end -------\n",
      "------- counter 109 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 53.0\n",
      "df_cache_with_features len 21800\n",
      "Predictor target models len 5\n",
      "prediction average -0.5824407813235861\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.1538839075342366e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 109, execution_time 1.9599614143371582 end -------\n",
      "------- counter 110 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 479,  seconds_in_bucket: 54.0\n",
      "df_cache_with_features len 22000\n",
      "Update global train cache\n",
      "update_global_train_cache\n",
      "Updated global_train_cache, len:  220000\n",
      "220000\n",
      "        date_id  seconds_in_bucket  stock_id  origin  revealed_target  \\\n",
      "0           461                  0         0       0        -2.830029   \n",
      "1           461                  0         1       0        11.060238   \n",
      "2           461                  0         2       0         2.080202   \n",
      "3           461                  0         3       0         2.720356   \n",
      "4           461                  0         4       0        -5.499721   \n",
      "...         ...                ...       ...     ...              ...   \n",
      "219995      480                540       195       0         1.599789   \n",
      "219996      480                540       196       0        -8.440018   \n",
      "219997      480                540       197       0         5.149841   \n",
      "219998      480                540       198       0        -0.249743   \n",
      "219999      480                540       199       0        -7.609725   \n",
      "\n",
      "          target  \n",
      "0       2.900362  \n",
      "1       4.279613  \n",
      "2      -6.049871  \n",
      "3      -4.360080  \n",
      "4       2.360344  \n",
      "...          ...  \n",
      "219995  2.310276  \n",
      "219996 -8.220077  \n",
      "219997  1.169443  \n",
      "219998 -1.540184  \n",
      "219999 -6.530285  \n",
      "\n",
      "[220000 rows x 6 columns]\n",
      "Predictor target models len 5\n",
      "prediction average -0.20296890860965175\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 0.044795662600364994\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 110, execution_time 2.7063498497009277 end -------\n",
      "------- counter 111 start -------\n",
      "copy_revealed_targets len 11000\n",
      "Update revealed_targets\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.52 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 0.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.45073719212512914\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 8.377817017901634e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 111, execution_time 2.369486093521118 end -------\n",
      "------- counter 112 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.49 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 1.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.05121986943634327\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 8.513045735014657e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 112, execution_time 2.2035629749298096 end -------\n",
      "------- counter 113 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.53 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 2.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.17240348126070526\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -3.148169778199872e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 113, execution_time 2.379815101623535 end -------\n",
      "------- counter 114 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.48 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 3.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.0347012097459591\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.1430754060247636e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 114, execution_time 2.2061848640441895 end -------\n",
      "------- counter 115 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.49 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 4.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.02619035970665262\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.344489799492976e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 115, execution_time 2.205030918121338 end -------\n",
      "------- counter 116 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.48 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 5.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.1367005235141956\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.861813541699121e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 116, execution_time 2.1976170539855957 end -------\n",
      "------- counter 117 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.49 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 6.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.08820465662217382\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -4.608384429616308e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 117, execution_time 2.2493176460266113 end -------\n",
      "------- counter 118 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.50 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 7.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.0315714636999579\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.4148272242664462e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 118, execution_time 2.223888635635376 end -------\n",
      "------- counter 119 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.49 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 8.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.07709196643572311\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 5.939489993167513e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 119, execution_time 2.193981170654297 end -------\n",
      "------- counter 120 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 9.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.23631098535196438\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 4.407966055275381e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 120, execution_time 1.9814813137054443 end -------\n",
      "------- counter 121 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 10.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.13354814833544573\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.2371467157379357e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 121, execution_time 1.9534361362457275 end -------\n",
      "------- counter 122 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 11.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.10802606866728674\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.2030816939633267e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 122, execution_time 1.9721519947052002 end -------\n",
      "------- counter 123 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 12.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.006220881826683504\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.158736224444624e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 123, execution_time 1.978588342666626 end -------\n",
      "------- counter 124 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.30 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 13.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.022874925374873723\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.7311504159067683e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 124, execution_time 2.0465469360351562 end -------\n",
      "------- counter 125 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 14.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.25616455239308095\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 6.377857304507018e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 125, execution_time 1.9564025402069092 end -------\n",
      "------- counter 126 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.33 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 15.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.16512054129714307\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -7.757160371824057e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 126, execution_time 2.24493145942688 end -------\n",
      "------- counter 127 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 16.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.6327231516551572\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -3.6764782329612446e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 127, execution_time 1.9506475925445557 end -------\n",
      "------- counter 128 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.30 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 17.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.45975911774208716\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.5149814675273773e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 128, execution_time 2.009927988052368 end -------\n",
      "------- counter 129 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 18.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.20848314742872717\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.0596857552845904e-12\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 129, execution_time 1.9724459648132324 end -------\n",
      "------- counter 130 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 19.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.23702216077930266\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 6.368285223601333e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 130, execution_time 2.0032780170440674 end -------\n",
      "------- counter 131 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 20.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.280893188092866\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -3.4108605717619867e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 131, execution_time 1.9833812713623047 end -------\n",
      "------- counter 132 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 21.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.34586414137381916\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.7661126605617029e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 132, execution_time 1.9451992511749268 end -------\n",
      "------- counter 133 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 22.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.2854778206787754\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -3.0634080472324854e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 133, execution_time 1.9596431255340576 end -------\n",
      "------- counter 134 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 23.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.1821749857200513\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.2022193729421814e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 134, execution_time 1.975754737854004 end -------\n",
      "------- counter 135 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 24.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.29999931789178164\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 7.141544200450767e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 135, execution_time 1.9717381000518799 end -------\n",
      "------- counter 136 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 25.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.08724637401273845\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 7.017596148628513e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 136, execution_time 1.9525251388549805 end -------\n",
      "------- counter 137 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.33 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 26.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.39703794314965435\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 8.377075770837905e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 137, execution_time 2.0860178470611572 end -------\n",
      "------- counter 138 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 27.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.016471552326084618\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.203203190553893e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 138, execution_time 1.9761502742767334 end -------\n",
      "------- counter 139 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 28.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.1935951869709863\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 5.760760382056418e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 139, execution_time 1.9513299465179443 end -------\n",
      "------- counter 140 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 29.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.10602787586791364\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -3.5778576989997645e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 140, execution_time 1.9949207305908203 end -------\n",
      "------- counter 141 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 30.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.36292902672192623\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.530455951304077e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 141, execution_time 1.9792914390563965 end -------\n",
      "------- counter 142 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 31.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.32997475571888457\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.2543418526078653e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 142, execution_time 1.9784953594207764 end -------\n",
      "------- counter 143 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 32.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.21012519039191524\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.93811498849783e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 143, execution_time 2.043437957763672 end -------\n",
      "------- counter 144 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 33.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.22969023092828678\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.1708315295066996e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 144, execution_time 2.043592691421509 end -------\n",
      "------- counter 145 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 34.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.16804708199821314\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -5.707933592447034e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 145, execution_time 2.040461778640747 end -------\n",
      "------- counter 146 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 35.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.16024884269625922\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.007630974214635e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 146, execution_time 1.916133165359497 end -------\n",
      "------- counter 147 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 36.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.2600937184986337\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 9.735142683098275e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 147, execution_time 1.9571278095245361 end -------\n",
      "------- counter 148 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 37.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.2435815276294073\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -7.810462721025146e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 148, execution_time 1.951702356338501 end -------\n",
      "------- counter 149 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 38.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.08529349690466273\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 3.690148409063454e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 149, execution_time 1.989438533782959 end -------\n",
      "------- counter 150 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 39.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.047498891450102845\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 2.2560241541569324e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 150, execution_time 2.01172137260437 end -------\n",
      "------- counter 151 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 40.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.0561769697996227\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.0415875637548311e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 151, execution_time 1.9641075134277344 end -------\n",
      "------- counter 152 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 41.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.13496552055458483\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.5085051323637798e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 152, execution_time 1.9703433513641357 end -------\n",
      "------- counter 153 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 42.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.19055814936305787\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -4.6761800476247115e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 153, execution_time 2.1055378913879395 end -------\n",
      "------- counter 154 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 43.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.11188773650637757\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.7485590486643332e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 154, execution_time 1.9923551082611084 end -------\n",
      "------- counter 155 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.26 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 44.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.06209650492329647\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -7.49504027730552e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 155, execution_time 1.9545879364013672 end -------\n",
      "------- counter 156 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 45.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.0886331434547978\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 4.0329073769385105e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 156, execution_time 1.966787576675415 end -------\n",
      "------- counter 157 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 46.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.1995885173793714\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -2.2696970880531355e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 157, execution_time 1.9887232780456543 end -------\n",
      "------- counter 158 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 47.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.21171030290575799\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -6.427297627453754e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 158, execution_time 1.9915084838867188 end -------\n",
      "------- counter 159 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 48.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.005160190487540053\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -5.167847056952723e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 159, execution_time 1.9911324977874756 end -------\n",
      "------- counter 160 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 49.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.02135152403398235\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -5.892864827217181e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 160, execution_time 1.9829161167144775 end -------\n",
      "------- counter 161 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 50.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.05324650552881112\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -1.1374179937906348e-08\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 161, execution_time 2.006953716278076 end -------\n",
      "------- counter 162 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 51.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average 0.01254282766477851\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average -9.119569543969419e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 162, execution_time 1.9991660118103027 end -------\n",
      "------- counter 163 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.28 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 52.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.036344942287520256\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 6.738767872604967e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 163, execution_time 2.0400846004486084 end -------\n",
      "------- counter 164 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.02 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.31 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 53.0\n",
      "df_cache_with_features len 22000\n",
      "Predictor target models len 5\n",
      "prediction average -0.09815417873376163\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 9.751166452076632e-10\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 164, execution_time 2.050421953201294 end -------\n",
      "------- counter 165 start -------\n",
      "copy_revealed_targets len 0\n",
      "df_cache len 2000\n",
      "generate_enhance_features\n",
      "Use index\n",
      "generate_index_features 0.01 [sec]\n",
      "generate_historical_features\n",
      "generate_historical_features 0.27 [sec]\n",
      "date_id: 480,  seconds_in_bucket: 54.0\n",
      "df_cache_with_features len 22000\n",
      "Update global train cache\n",
      "update_global_train_cache\n",
      "Updated global_train_cache, len:  220000\n",
      "220000\n",
      "        date_id  seconds_in_bucket  stock_id  origin  revealed_target  \\\n",
      "0           461                  0         0       0        -2.830029   \n",
      "1           461                  0         1       0        11.060238   \n",
      "2           461                  0         2       0         2.080202   \n",
      "3           461                  0         3       0         2.720356   \n",
      "4           461                  0         4       0        -5.499721   \n",
      "...         ...                ...       ...     ...              ...   \n",
      "219995      480                540       195       0         1.599789   \n",
      "219996      480                540       196       0        -8.440018   \n",
      "219997      480                540       197       0         5.149841   \n",
      "219998      480                540       198       0        -0.249743   \n",
      "219999      480                540       199       0        -7.609725   \n",
      "\n",
      "          target  \n",
      "0       2.900362  \n",
      "1       4.279613  \n",
      "2      -6.049871  \n",
      "3      -4.360080  \n",
      "4       2.360344  \n",
      "...          ...  \n",
      "219995  2.310276  \n",
      "219996 -8.220077  \n",
      "219997  1.169443  \n",
      "219998 -1.540184  \n",
      "219999 -6.530285  \n",
      "\n",
      "[220000 rows x 6 columns]\n",
      "Predictor target models len 5\n",
      "prediction average 0.2589541738289743\n",
      "Use additional model\n",
      "Predictor target models len 5\n",
      "additional prediction average 1.9841674436804623e-09\n",
      "Submit prediction\n",
      "RAM memory GB usage = 18.81\n",
      "------- counter 165, execution_time 2.7278366088867188 end -------\n",
      "Error [Errno 1] Operation not permitted: 'submission.csv'\n",
      "CPU times: user 8min 25s, sys: 2.19 s, total: 8min 27s\n",
      "Wall time: 6min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "predictions = []\n",
    "df_cache = pd.DataFrame()\n",
    "df_cache_with_features = pd.DataFrame()\n",
    "df_result = pd.DataFrame()\n",
    "\n",
    "df_revealed_targets = pd.DataFrame()\n",
    "\n",
    "if IS_INFER:\n",
    "    if IS_LOCAL or IS_DEBUG:\n",
    "        print(\"Infer Local\")\n",
    "        env = make_env()\n",
    "    else:\n",
    "        print(\"Infer Submission\")\n",
    "        import optiver2023\n",
    "        env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    counter = 1\n",
    "\n",
    "    try:\n",
    "        for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "            now_time = time.time()\n",
    "            print(f\"------- counter {counter} start -------\")\n",
    "\n",
    "            # Add revealed target as target for counituous update\n",
    "            copy_revealed_targets = revealed_targets.copy()\n",
    "            copy_revealed_targets = copy_revealed_targets.dropna()\n",
    "            print(\"copy_revealed_targets len\", len(copy_revealed_targets))\n",
    "\n",
    "            if len(copy_revealed_targets) > 0:\n",
    "                print(\"Update revealed_targets\")\n",
    "                copy_revealed_targets['revealed_date_id'] = copy_revealed_targets['revealed_date_id'].astype(int).astype(str)\n",
    "                copy_revealed_targets['date_id'] = copy_revealed_targets['date_id'].astype(int).astype(str)\n",
    "                copy_revealed_targets['seconds_in_bucket'] = copy_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n",
    "                copy_revealed_targets['stock_id'] = copy_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n",
    "                copy_revealed_targets['revealed_row_id'] = copy_revealed_targets['revealed_date_id'] + '_' + copy_revealed_targets['seconds_in_bucket'] + '_' + copy_revealed_targets['stock_id']\n",
    "                copy_revealed_targets['row_id'] = copy_revealed_targets['date_id'] + '_' + copy_revealed_targets['seconds_in_bucket'] + '_' + copy_revealed_targets['stock_id']\n",
    "                copy_revealed_targets['revealed_target'] = copy_revealed_targets['revealed_target'].astype('float32')\n",
    "\n",
    "                df_revealed_targets = pd.concat([df_revealed_targets, copy_revealed_targets], ignore_index=True, axis=0)\n",
    "                df_revealed_targets = df_revealed_targets.groupby(['stock_id']).tail(DATA_COUNT_IN_SAME_BUCKET * 2)\n",
    "                df_revealed_targets = reduce_mem_usage(df_revealed_targets, 'df_revealed_targets')\n",
    "                \n",
    "            df_cache = pd.concat([df_cache, test], ignore_index=True, axis=0)\n",
    "\n",
    "            if IS_MIN_LEARN:\n",
    "                print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n",
    "                df_cache = df_cache[df_cache[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n",
    "\n",
    "            if counter > 0:\n",
    "                # Clear cache data, tailはhistoricalで作成な分のみ残す\n",
    "                df_cache = df_cache.groupby(['stock_id']).tail(10)\n",
    "                df_cache = default_sort(df_cache)\n",
    "                print(f\"df_cache len {len(df_cache)}\")\n",
    "\n",
    "            # USE_REVEALED_TARGETSが有効の時、cacheのrow_idとdf_revealed_targetsのrow_idをleft joinする\n",
    "            if USE_REVEALED_TARGETS:\n",
    "                df_r = df_revealed_targets[['row_id', 'revealed_target']]\n",
    "                df_cache = pd.merge(df_cache, df_r, how='left', on='row_id')\n",
    "                df_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'revealed_target']]\n",
    "\n",
    "            # Generate features\n",
    "            df_valid = df_cache.copy()\n",
    "            df_valid = generate_basic_features(df_valid)\n",
    "            df_valid = generate_enhance_features(df_valid)\n",
    "            df_valid = reduce_mem_usage(df_valid, 'df_valid')\n",
    "\n",
    "            # testの分のみの長さを抽出\n",
    "            if IS_MIN_LEARN:\n",
    "                df_valid = df_valid[-len(TARGET_STOCK_IDS):].reset_index(drop=True)\n",
    "            else:\n",
    "                df_valid = df_valid[-len(test):].reset_index(drop=True)\n",
    "\n",
    "            df_cache_with_features = pd.concat([df_cache_with_features, df_valid], ignore_index=True, axis=0)\n",
    "\n",
    "            # It faults due to test is iterator\n",
    "            #seconds_in_bucket = test['seconds_in_bucket'][0]\n",
    "            #print(f\"prdict: {test['date_id'][0]}, {seconds_in_bucket}\")\n",
    "\n",
    "            seconds_in_bucket = df_valid['seconds_in_bucket'][0] / 10\n",
    "            date_id = df_valid['date_id'][0]\n",
    "            print(f\"date_id: {date_id},  seconds_in_bucket: {seconds_in_bucket}\")\n",
    "\n",
    "            if counter > 0:\n",
    "                # Clear cache data, tailはhistoricalで作成な分のみ残す\n",
    "                df_cache_with_features = df_cache_with_features.groupby(['stock_id']).tail(DATA_COUNT_IN_SAME_BUCKET * 2)\n",
    "                df_cache_with_features = default_sort(df_cache_with_features)\n",
    "                print(f\"df_cache_with_features len {len(df_cache_with_features)}\")\n",
    "\n",
    "            # Update global train cache\n",
    "            if USE_CONTINUOUS_UPDATE  and ((seconds_in_bucket + 1) % DATA_COUNT_IN_SAME_BUCKET == 0):\n",
    "                print(\"Update global train cache\")\n",
    "                df_r = df_revealed_targets[['revealed_row_id', 'revealed_target']]\n",
    "                df_r.rename(columns={'revealed_target': 'target'}, inplace=True)\n",
    "                df_r.rename(columns={'revealed_row_id': 'row_id'}, inplace=True)\n",
    "                df_update = pd.merge(df_cache_with_features, df_r, how='left', on='row_id')\n",
    "                update_global_train_cache(df_update, 2)\n",
    "\n",
    "            # Update model\n",
    "            if ((seconds_in_bucket + 1)  % (DATA_COUNT_IN_SAME_BUCKET * continuos_train_span) == 0) and USE_CONTINUOUS_UPDATE:\n",
    "                print(\"Update model with revealed_target date start\")\n",
    "                train_now_time = time.time()\n",
    "                model_dict[KEY] = keep_train_models(global_train_cache, model_dict[KEY], features, \"target\", True)\n",
    "                print(f\"ReTrain Time: {time.time() - train_now_time}\")\n",
    "\n",
    "            # Predict\n",
    "            predictions = model_infer(KEY, df_valid[features])\n",
    "            scaled_predictions = zero_clip(df_valid, predictions)\n",
    "\n",
    "            df_valid['pred'] = predictions\n",
    "            df_valid['scaled_pred'] = scaled_predictions\n",
    "            print(\"prediction average\", np.mean(predictions))\n",
    "\n",
    "            if USE_REVEALED_TARGETS:\n",
    "                additional_predictions = model_infer(KEY, df_valid[additional_features], use_additional_model=True)\n",
    "                additional_scaled_predictions = zero_clip(df_valid, additional_predictions)\n",
    "                df_valid['additional_pred'] = additional_predictions\n",
    "                df_valid['additional_scaled_pred'] = additional_scaled_predictions\n",
    "                print(\"additional prediction average\", np.mean(additional_scaled_predictions))\n",
    "            # For save\n",
    "            if IS_DEBUG:\n",
    "                df_result = pd.concat([df_result, df_valid], ignore_index=True, axis=0)\n",
    "\n",
    "            # Submit\n",
    "            if not IS_MIN_LEARN:\n",
    "                print(\"Submit prediction\")\n",
    "                sample_prediction['target'] = scaled_predictions\n",
    "                env.predict(sample_prediction)\n",
    "            else:\n",
    "                print(\"Submit dummy prediction\")\n",
    "                sample_prediction['target'] = 0\n",
    "                env.predict(sample_prediction)\n",
    "\n",
    "            # Clean up\n",
    "            execution_time = time.time() - now_time\n",
    "            df_cache = df_cache.drop('revealed_target', axis=1)\n",
    "            del df_valid\n",
    "            collect()\n",
    "            print(GetMemUsage())\n",
    "            print(f\"------- counter {counter}, execution_time {execution_time} end -------\")\n",
    "            counter += 1\n",
    "    except Exception as e:\n",
    "        print(\"Error\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0fab43b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:08:51.815069Z",
     "iopub.status.busy": "2023-12-17T05:08:51.814720Z",
     "iopub.status.idle": "2023-12-17T05:08:58.624604Z",
     "shell.execute_reply": "2023-12-17T05:08:58.623765Z"
    },
    "papermill": {
     "duration": 6.93153,
     "end_time": "2023-12-17T05:08:58.627147",
     "exception": false,
     "start_time": "2023-12-17T05:08:51.695617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if IS_DEBUG:\n",
    "    df_cache_with_features.to_csv(f\"{BASE_OUTPUT_PATH}/df_cache_with_features.csv\", index=False)\n",
    "    df_result.to_csv(f\"{BASE_OUTPUT_PATH}/result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2f1eec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:08:58.865073Z",
     "iopub.status.busy": "2023-12-17T05:08:58.864195Z",
     "iopub.status.idle": "2023-12-17T05:08:59.096262Z",
     "shell.execute_reply": "2023-12-17T05:08:59.094910Z"
    },
    "papermill": {
     "duration": 0.353599,
     "end_time": "2023-12-17T05:08:59.098293",
     "exception": false,
     "start_time": "2023-12-17T05:08:58.744694",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score\n",
      "        scaled_pred        target         score\n",
      "count  22000.000000  22000.000000  22000.000000\n",
      "mean       0.005682     -0.171779      5.070527\n",
      "std        2.907103      8.208760      5.046819\n",
      "min      -64.000000   -144.379730      0.000889\n",
      "25%       -0.754184     -3.932267      1.713513\n",
      "50%        0.079279     -0.060201      3.673265\n",
      "75%        0.851452      3.730059      6.840511\n",
      "max       55.841760     77.129600     80.379730\n",
      "additional_pred\n",
      "       additional_scaled_pred        target         score\n",
      "count            22000.000000  22000.000000  22000.000000\n",
      "mean                 0.005521     -0.171779      5.164920\n",
      "std                  2.623771      8.208760      5.185243\n",
      "min                -64.000000   -144.379730      0.000479\n",
      "25%                 -0.621228     -3.932267      1.740845\n",
      "50%                  0.069669     -0.060201      3.735483\n",
      "75%                  0.724189      3.730059      6.923081\n",
      "max                 46.050495     77.129600     80.379730\n"
     ]
    }
   ],
   "source": [
    "if IS_DEBUG:\n",
    "    df_revealed_targets = pd.read_csv(REVEALED_TARGETS_FILE)\n",
    "    df_revealed_targets = df_revealed_targets.dropna(subset=['revealed_date_id', 'seconds_in_bucket', 'stock_id'])\n",
    "    df_revealed_targets['revealed_date_id'] = df_revealed_targets['revealed_date_id'].astype(int).astype(str)\n",
    "    df_revealed_targets['seconds_in_bucket'] = df_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n",
    "    df_revealed_targets['stock_id'] = df_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n",
    "\n",
    "    # Concatenate the columns\n",
    "    df_revealed_targets['row_id'] = df_revealed_targets['revealed_date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n",
    "    \n",
    "    df_pred = df_result[['row_id', 'scaled_pred']]\n",
    "    df = pd.merge(df_pred, df_revealed_targets, how='left', on='row_id')\n",
    "\n",
    "    df = df.rename(columns={'revealed_target': 'target'})\n",
    "    df = df.dropna(subset=['target'])\n",
    "    df['score'] = (df['scaled_pred'] - df['target']).abs()\n",
    "\n",
    "    #df['score'] = mean_absolute_error(df['scaled_pred'], df['target'])\n",
    "    df = df[['row_id', 'scaled_pred', 'target', 'score']]\n",
    "    print(\"score\")\n",
    "    print(df.describe())\n",
    "\n",
    "    if USE_ADDITIONAL_TRAIN:\n",
    "        df_pred = df_result[['row_id', 'additional_scaled_pred']]\n",
    "        df = pd.merge(df_pred, df_revealed_targets, how='left', on='row_id')\n",
    "\n",
    "        df = df.rename(columns={'revealed_target': 'target'})\n",
    "        df = df.dropna(subset=['target'])\n",
    "        df['score'] = (df['additional_scaled_pred'] - df['target']).abs()\n",
    "\n",
    "        #df['score'] = mean_absolute_error(df['scaled_pred'], df['target'])\n",
    "        print(\"additional_pred\")\n",
    "        df = df[['row_id', 'additional_scaled_pred', 'target', 'score']]\n",
    "        print(df.describe())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7656.140481,
   "end_time": "2023-12-17T05:09:02.438007",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-17T03:01:26.297526",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
