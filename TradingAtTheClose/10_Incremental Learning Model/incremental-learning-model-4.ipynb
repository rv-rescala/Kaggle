{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Incremental Learning Model\n","This origin comming from https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model/notebook"]},{"cell_type":"markdown","metadata":{},"source":["# Init"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":126,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:04.714475Z","iopub.status.busy":"2023-12-15T11:18:04.713507Z","iopub.status.idle":"2023-12-15T11:18:04.721276Z","shell.execute_reply":"2023-12-15T11:18:04.720367Z","shell.execute_reply.started":"2023-12-15T11:18:04.714439Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","import os\n","import warnings\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import gc  # Garbage collection for memory management\n","import os  # Operating system-related functions\n","import time  # Time-related functions\n","import warnings  # Handling warnings\n","from itertools import combinations  # For creating combinations of elements\n","from warnings import simplefilter  # Simplifying warning handling\n","import joblib  # For saving and loading models\n","import numpy as np  # Numerical operations\n","import pandas as pd  # Data manipulation and analysis\n","from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n","from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n","from concurrent.futures import ThreadPoolExecutor\n","from numba import njit, prange  # Compiling Python code for performance"]},{"cell_type":"markdown","metadata":{},"source":["## Global params"]},{"cell_type":"code","execution_count":127,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:04.723350Z","iopub.status.busy":"2023-12-15T11:18:04.723059Z","iopub.status.idle":"2023-12-15T11:18:04.747414Z","shell.execute_reply":"2023-12-15T11:18:04.746461Z","shell.execute_reply.started":"2023-12-15T11:18:04.723327Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BASE_OUTPUT_PATH: ../output\n","BASE_INPUT_PATH: ../kaggle/input/optiver-trading-at-the-close\n","TRAIN_FILE: ../kaggle/input/optiver-trading-at-the-close/train.csv\n","TEST_FILE: ../kaggle/input/optiver-trading-at-the-close/test.csv\n","IS_LOCAL: True\n","IS_INFER: True\n","IS_USE_SAVED_MODEL: False\n","IS_MIN_LEARN: True\n","USE_OPTUNA: False\n","USE_CONTINUOUS_UPDATE: True\n","USE_ALL_FEATUTES: True\n","USE_REVEALED_TARGETS: True\n","USE_INDEX: True\n"]}],"source":["# Disable warnings to keep the code clean\n","warnings.filterwarnings(\"ignore\")\n","simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","seed = 2023\n","DATA_COUNT_IN_SAME_BUCKET = 55 # 同じbucket内のデータ数\n","\n","# For kaggle environment\n","if os.environ.get(\"KAGGLE_DATA_PROXY_TOKEN\") != None:\n","    BASE_OUTPUT_PATH = Path(f'/kaggle/working')\n","    BASE_INPUT_PATH = Path(f'/kaggle/input/optiver-trading-at-the-close')\n","    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n","    TEST_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/test.csv')\n","    \n","    IS_LOCAL = False # If kaggle environment, set False\n","    IS_INFER = True # If kaggle environment, set True\n","    IS_USE_SAVED_MODEL = False # Use saved model or not\n","    IS_MIN_LEARN = False # Use min learning or not\n","    USE_OPTUNA = False # Use optuna or not\n","    USE_CONTINUOUS_UPDATE = True # Use test date on train or not\n","    USE_ALL_FEATUTES = True # Use all features or not\n","    USE_REVEALED_TARGETS = True # Use revealed targets or not\n","    USE_INDEX = True # Use index or not\n","    IS_DEBUG = False\n","    USE_ADDITIONAL_TRAIN = True\n","    NUM_THREADS = 4\n","    ADDITIONAL_TRAIN_THRESHOLD = 20 # x以上のデータを追加学習する\n","\n","    SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/sample_submission.csv')\n","    REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/revealed_targets.csv')\n","\n","    stopping_rounds = 100 # early_stopping用コールバック関数\n","    num_boost_round = 10000 # 計算回数\n","    update_num_boost_round = 10000 # 再学習の計算回数\n","    num_folds = 5 # クロスバリデーションの分割数\n","    continuos_dataset_span = 20 # DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span が更新の使用する対象のデータ\n","    continuos_train_span = 2 # DATA_COUNT_IN_SAME_BUCKET * continuos_train_span が更新の頻度\n","\n","    DEVICE = 'gpu' # cpu or gpu\n","    OPTUNA_TIME_BUDGET = 60 * 60 * 4 # 1 hours\n","    TARGET_STOCK_IDS = [0, 1]\n","\n","    optuna_params = {\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression',         # 回帰\n","        'metric': 'rmse',                  # 損失（誤差）\n","        'verbosity': -1,\n","        'deterministic':True, #再現性確保用のパラメータ\n","        'force_row_wise':True,  #再現性確保用のパラメータ\n","        'device': DEVICE\n","    }\n","\n","    # サンプルのパラメータ\n","    \"\"\"\n","    lgb_params = {\n","        \"objective\": \"mae\",\n","        \"n_estimators\": 6000,\n","        \"num_leaves\": 256,\n","        \"subsample\": 0.6,\n","        \"colsample_bytree\": 0.8,\n","        \"learning_rate\": 0.00871,\n","        'max_depth': 11,\n","        \"n_jobs\": 4,\n","        \"verbosity\": -1,\n","        \"importance_type\": \"gain\",\n","        \"device\": DEVICE,\n","    }\n","    \"\"\"\n","    \n","    \"\"\" \n","    # こっちのパラメータの方が、計算時間がかかる\n","    lgb_params = {\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression', \n","        'metric': 'rmse', \n","        'verbosity': -1, \n","        'device': DEVICE,\n","        'feature_pre_filter': False, \n","        'lambda_l1': 0.0,\n","        'lambda_l2': 0.0,\n","        'num_leaves': 31, \n","        'feature_fraction': 0.8, \n","        'bagging_fraction': 1.0, \n","        'bagging_freq': 0, \n","        'min_child_samples': 20,\n","        'seed': seed,                       # シード値\n","    }\n","    \"\"\"\n","\n","    \n","   # 計算が早い\n","    lgb_params = {\n","        'task': 'train',                   # 学習\n","        'objective': 'regression',                # 目的関数の種類。ここでは回帰タスクを指定\n","        'metric': 'rmse',                          # 評価指標\n","        'boosting_type': 'gbdt',                  # ブースティングタイプ。勾配ブースティング決定木\n","        \"n_estimators\": 32,                        # ブースティングに使用する木の数。多いほど性能が向上するが計算コストが増加\n","        \"num_leaves\": 64,                         # 木に存在する最大の葉の数。大きい値は精度を向上させるが過学習のリスクが増加\n","        \"subsample\": 0.8,                         # 各木のトレーニングに使用されるデータの割合。過学習を防ぐために一部のデータをサンプリング\n","        \"colsample_bytree\": 0.8,                  # 木を構築する際に使用される特徴の割合。特徴のサブセットを使用し過学習を防ぐ\n","        \"learning_rate\": 0.01,                 # 学習率。小さい値は堅牢なモデルを生成するが収束に時間がかかる\n","        'max_depth': 32,                           # 木の最大の深さ。深い木は複雑なモデルを作成するが過学習のリスクがある\n","        \"device\": DEVICE,                         # トレーニングに使用するデバイス（CPUまたはGPU）\n","        \"verbosity\": -1,                          # LightGBMのログ出力のレベル。-1はログを出力しないことを意味する\n","       # \"importance_type\": \"gain\",                # 特徴重要度を計算する際の指標。\"gain\"は分割による平均情報利得\n","        'lambda_l1': 0.5,                         # L1正則化項の係数。過学習を防ぐためにモデルの複雑さにペナルティを課す\n","        'lambda_l2': 0.5,                         # L2正則化項の係数。同じく過学習を防ぐ\n","        'bagging_freq': 5,                 # バギング実施頻度\n","        'min_child_samples': 10,           # 葉に含まれる最小データ数\n","        'seed': seed,                       # シード値\n","    }\n","\n","# For local environment\n","else:\n","    BASE_OUTPUT_PATH = Path(f'../output')\n","    BASE_INPUT_PATH = Path(f'../kaggle/input/optiver-trading-at-the-close')\n","    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n","    TEST_FILE = Path(f'{BASE_INPUT_PATH}/test.csv')\n","\n","    SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/sample_submission.csv')\n","    REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/revealed_targets.csv')\n","\n","    IS_LOCAL = True\n","    IS_INFER = True\n","    IS_USE_SAVED_MODEL = False # Use saved model or not\n","    IS_MIN_LEARN = True\n","    USE_OPTUNA = False # Use optuna or not\n","    USE_ALL_FEATUTES = True # Use all features or not\n","    USE_CONTINUOUS_UPDATE = True # Use test date on train or not\n","    USE_REVEALED_TARGETS = True # Use revealed targets or not \n","    USE_INDEX = True # Use index or not\n","    IS_DEBUG = True\n","    USE_ADDITIONAL_TRAIN = True\n","    TARGET_STOCK_IDS = [0]\n","    NUM_THREADS = 2\n","    ADDITIONAL_TRAIN_THRESHOLD = 20 # x以上のデータを追加学習する\n","\n","    # For training\n","    stopping_rounds = 1 # early_stopping用コールバック関数\n","    num_boost_round = 1 # 計算回数\n","    update_num_boost_round = 1\n","    num_folds = 2 # クロスバリデーションの分割数\n","    continuos_dataset_span = 3 # DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span が更新の使用する対象のデータ\n","    continuos_train_span = 2 # DATA_COUNT_IN_SAME_BUCKET * continuos_train_span が更新の頻度\n","\n","    DEVICE = 'cpu' # cpu or gpu\n","    OPTUNA_TIME_BUDGET = 60 # 1 min\n","\n","    optuna_params = {\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression',         # 回帰\n","        'metric': 'rmse',                  # 損失（誤差）\n","        'verbosity': -1,\n","        'deterministic':True, #再現性確保用のパラメータ\n","        'force_row_wise':True,  #再現性確保用のパラメータ\n","        'device': DEVICE\n","    }\n","\n","    lgb_params = {\n","        \"objective\": \"mae\",\n","        \"n_estimators\": 6000,\n","        \"num_leaves\": 256,\n","        \"subsample\": 0.6,\n","        \"colsample_bytree\": 0.8,\n","        \"learning_rate\": 0.00871,\n","        'max_depth': 11,\n","        \"n_jobs\": 4,\n","        \"verbosity\": -1,\n","        \"importance_type\": \"gain\",\n","        \"device\": DEVICE,\n","    }\n","\n","\n","print(f\"BASE_OUTPUT_PATH: {BASE_OUTPUT_PATH}\")\n","print(f\"BASE_INPUT_PATH: {BASE_INPUT_PATH}\")\n","print(f\"TRAIN_FILE: {TRAIN_FILE}\")\n","print(f\"TEST_FILE: {TEST_FILE}\")\n","print(f\"IS_LOCAL: {IS_LOCAL}\")\n","print(f\"IS_INFER: {IS_INFER}\")\n","print(f\"IS_USE_SAVED_MODEL: {IS_USE_SAVED_MODEL}\")\n","print(f\"IS_MIN_LEARN: {IS_MIN_LEARN}\")\n","print(f\"USE_OPTUNA: {USE_OPTUNA}\")\n","print(f\"USE_CONTINUOUS_UPDATE: {USE_CONTINUOUS_UPDATE}\")\n","print(f\"USE_ALL_FEATUTES: {USE_ALL_FEATUTES}\")\n","print(f\"USE_REVEALED_TARGETS: {USE_REVEALED_TARGETS}\")\n","print(f\"USE_INDEX: {USE_INDEX}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Functions"]},{"cell_type":"markdown","metadata":{},"source":["## Memory Functions"]},{"cell_type":"code","execution_count":128,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:04.749082Z","iopub.status.busy":"2023-12-15T11:18:04.748680Z","iopub.status.idle":"2023-12-15T11:18:05.034316Z","shell.execute_reply":"2023-12-15T11:18:05.033290Z","shell.execute_reply.started":"2023-12-15T11:18:04.749042Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["RAM memory GB usage = 0.3304\n","CPU times: user 165 ms, sys: 88.8 ms, total: 254 ms\n","Wall time: 317 ms\n"]}],"source":["%%time \n","\n","from gc import collect;\n","from psutil import Process;\n","from os import system, getpid, walk;\n","\n","# Defining global configurations and functions:-\n","\n","    \n","def GetMemUsage():\n","    \"\"\"\n","    This function defines the memory usage across the kernel. \n","    Source-\n","    https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\n","    \"\"\";\n","    \n","    pid = getpid();\n","    py = Process(pid);\n","    memory_use = py.memory_info()[0] / 2. ** 30;\n","    return f\"RAM memory GB usage = {memory_use :.4}\";\n","\n","\n","collect();\n","print(GetMemUsage())"]},{"cell_type":"code","execution_count":129,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.037181Z","iopub.status.busy":"2023-12-15T11:18:05.036784Z","iopub.status.idle":"2023-12-15T11:18:05.050967Z","shell.execute_reply":"2023-12-15T11:18:05.050001Z","shell.execute_reply.started":"2023-12-15T11:18:05.037144Z"},"trusted":true},"outputs":[],"source":["# 🧹 Function to reduce memory usage of a Pandas DataFrame\n","def reduce_mem_usage(df, name: str, show_optimization: bool = False):\n","    \"\"\"\n","    Iterate through all numeric columns of a dataframe and modify the data type\n","    to reduce memory usage.\n","    \"\"\"\n","    \n","    # 📏 Calculate the initial memory usage of the DataFrame\n","    start_mem = df.memory_usage().sum() / 1024**2\n","\n","    # 🔄 Iterate through each column in the DataFrame\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","\n","        # Check if the column's data type is not 'object' (i.e., numeric)\n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            \n","            # Check if the column's data type is an integer\n","            if str(col_type)[:3] == \"int\":\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                # Check if the column's data type is a float\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float32)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float32)\n","\n","    if show_optimization:\n","        print(f\"Memory usage of {name} is {start_mem:.2f} MB\")\n","        end_mem = df.memory_usage().sum() / 1024**2\n","        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n","        decrease = 100 * (start_mem - end_mem) / start_mem\n","        print(f\"Decreased by {decrease:.2f}%\")\n","\n","    # 🔄 Return the DataFrame with optimized memory usage\n","\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["## API Function"]},{"cell_type":"code","execution_count":130,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.052330Z","iopub.status.busy":"2023-12-15T11:18:05.052047Z","iopub.status.idle":"2023-12-15T11:18:05.072366Z","shell.execute_reply":"2023-12-15T11:18:05.071440Z","shell.execute_reply.started":"2023-12-15T11:18:05.052306Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 30 µs, sys: 6 µs, total: 36 µs\n","Wall time: 38.9 µs\n"]}],"source":["%%time \n","\n","from typing import Sequence, Tuple\n","import pandas as pd\n","\n","# for local execution\n","class MockApi:\n","    def __init__(self):\n","        '''\n","        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n","        They've been intentionally left in an invalid state.\n","\n","        Variables to set:\n","            input_paths: a list of two or more paths to the csv files to be served\n","            group_id_column: the column that identifies which groups of rows the API should serve.\n","                A call to iter_test serves all rows of all dataframes with the current group ID value.\n","            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n","        '''\n","        self.input_paths: Sequence[str] = [TEST_FILE, REVEALED_TARGETS_FILE, SAMPLE_SUBMISSION_FILE]\n","        self.group_id_column: str = 'time_id'\n","        self.export_group_id_column: bool = True\n","        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n","        assert len(self.input_paths) >= 2\n","\n","        self._status = 'initialized'\n","        self.predictions = []\n","\n","    def iter_test(self) -> Tuple[pd.DataFrame]:\n","        '''\n","        Loads all of the dataframes specified in self.input_paths,\n","        then yields all rows in those dataframes that equal the current self.group_id_column value.\n","        '''\n","        if self._status != 'initialized':\n","\n","            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n","\n","        dataframes = []\n","        for pth in self.input_paths:\n","            dataframes.append(pd.read_csv(pth, low_memory=False))\n","        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n","        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n","\n","        for group_id in group_order:\n","            self._status = 'prediction_needed'\n","            current_data = []\n","            for df in dataframes:\n","                cur_df = df.loc[group_id].copy()\n","                # returning single line dataframes from df.loc requires special handling\n","                if not isinstance(cur_df, pd.DataFrame):\n","                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n","                    cur_df.index.name = self.group_id_column\n","                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n","                current_data.append(cur_df)\n","            yield tuple(current_data)\n","\n","            while self._status != 'prediction_received':\n","                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n","                yield None\n","\n","        with open('submission.csv', 'w') as f_open:\n","            pd.concat(self.predictions).to_csv(f_open, index=False)\n","        self._status = 'finished'\n","\n","    def predict(self, user_predictions: pd.DataFrame):\n","        '''\n","        Accepts and stores the user's predictions and unlocks iter_test once that is done\n","        '''\n","        if self._status == 'finished':\n","            raise Exception('You have already made predictions for the full test set.')\n","        if self._status != 'prediction_needed':\n","            raise Exception('You must get the next test sample from `iter_test()` first.')\n","        if not isinstance(user_predictions, pd.DataFrame):\n","            raise Exception('You must provide a DataFrame.')\n","\n","        self.predictions.append(user_predictions)\n","        self._status = 'prediction_received'\n","\n","def make_env():\n","    return MockApi()"]},{"cell_type":"markdown","metadata":{},"source":["## Pandas Functions"]},{"cell_type":"code","execution_count":131,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.074216Z","iopub.status.busy":"2023-12-15T11:18:05.073557Z","iopub.status.idle":"2023-12-15T11:18:05.083338Z","shell.execute_reply":"2023-12-15T11:18:05.082562Z","shell.execute_reply.started":"2023-12-15T11:18:05.074184Z"},"trusted":true},"outputs":[],"source":["def pd_display_max():\n","    pd.set_option('display.max_rows', None)  # 行の最大表示数を無制限に設定\n","    pd.set_option('display.max_columns', None)  # 列の最大表示数を無制限に設定\n","    pd.set_option('display.width', None)  # 表示幅を拡張\n","    pd.set_option('display.max_colwidth', None)  # 列の幅を最大に設定\n","\n","def pd_clear_display_max():\n","    pd.set_option('display.max_rows', 10)\n","    pd.set_option('display.max_columns', 10)\n","    pd.set_option('display.width', None)  # 表示幅を拡張\n","    pd.set_option('display.max_colwidth', None)  # 列の幅を最大に設定"]},{"cell_type":"markdown","metadata":{},"source":["## Sorting Functions"]},{"cell_type":"code","execution_count":132,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.084614Z","iopub.status.busy":"2023-12-15T11:18:05.084361Z","iopub.status.idle":"2023-12-15T11:18:05.093911Z","shell.execute_reply":"2023-12-15T11:18:05.093194Z","shell.execute_reply.started":"2023-12-15T11:18:05.084593Z"},"trusted":true},"outputs":[],"source":["def default_sort(df):\n","    return df.sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Generationg train dataset"]},{"cell_type":"code","execution_count":133,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.096804Z","iopub.status.busy":"2023-12-15T11:18:05.096540Z","iopub.status.idle":"2023-12-15T11:18:05.108104Z","shell.execute_reply":"2023-12-15T11:18:05.107172Z","shell.execute_reply.started":"2023-12-15T11:18:05.096782Z"},"trusted":true},"outputs":[],"source":["def load_train_dataset():\n","    df = pd.read_csv(TRAIN_FILE)\n","    # 🧹 Remove rows with missing values in the \"target\" column\n","    df = df.dropna(subset=[\"target\"])\n","    # 🔁 Reset the index of the DataFrame and apply the changes in place\n","    df.reset_index(drop=True, inplace=True)\n","    return df\n","\n","def load_test_dataset():\n","    df_test = pd.read_csv(TEST_FILE)\n","    \n","    df_revealed_targets = pd.read_csv(REVEALED_TARGETS_FILE)\n","    df_revealed_targets = df_revealed_targets.dropna(subset=['date_id', 'seconds_in_bucket', 'stock_id'])\n","    df_revealed_targets['revealed_date_id'] = df_revealed_targets['revealed_date_id'].astype(int).astype(str)\n","    df_revealed_targets['seconds_in_bucket'] = df_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n","    df_revealed_targets['stock_id'] = df_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n","    df_revealed_targets['date_id'] = df_revealed_targets['date_id'].astype(int).astype(str)\n","    df_revealed_targets['row_id'] = df_revealed_targets['date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n","    df_revealed_targets['revealed_row_id'] = df_revealed_targets['revealed_date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n","\n","    # USE_CONTINUOUS_UPDATEが有効の時、cacheのrow_idとdf_revealed_targetsのrevealed_row_idをleft joinする\n","    if USE_CONTINUOUS_UPDATE:\n","        df_r = df_revealed_targets[['revealed_row_id', 'revealed_target']]\n","        df_r.rename(columns={'revealed_target': 'target'}, inplace=True)\n","        df_r.rename(columns={'revealed_row_id': 'row_id'}, inplace=True)\n","        df_test = pd.merge(df_test, df_r, how='left', on='row_id')\n","        \n","    # USE_REVEALED_TARGETSが有効の時、cacheのrow_idとdf_revealed_targetsのrow_idをleft joinする\n","    if USE_REVEALED_TARGETS:\n","        df_r = df_revealed_targets[['row_id', 'revealed_target']]\n","        df_test = pd.merge(df_test, df_r, how='left', on='row_id')\n","        \n","    df_test = df_test.dropna(subset=[\"target\"])\n","    df_test.reset_index(drop=True, inplace=True)\n","    return df_test"]},{"cell_type":"code","execution_count":134,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.110223Z","iopub.status.busy":"2023-12-15T11:18:05.109600Z","iopub.status.idle":"2023-12-15T11:18:18.238113Z","shell.execute_reply":"2023-12-15T11:18:18.237230Z","shell.execute_reply.started":"2023-12-15T11:18:05.110190Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Load train dataset\n","MIN LEARN MODE : [0]\n","['stock_id', 'seconds_in_bucket', 'imbalance_size', 'imbalance_buy_sell_flag', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap']\n","RAM memory GB usage = 1.59\n","CPU times: user 6.35 s, sys: 1.52 s, total: 7.87 s\n","Wall time: 8.83 s\n"]}],"source":["%%time\n","# Check if the code is running in offline or online mode\n","print(\"Load train dataset\")\n","\n","df_train = load_train_dataset()\n","\n","if IS_MIN_LEARN:\n","    print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n","    # In local mode, stock id TARGET_STOCK_ID is used for training\n","    df_train = df_train[df_train[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","    \n","features = [c for c in df_train.columns if c not in [\"row_id\", \"target\", \"time_id\", \"row_id\", \"date_id\", \"currently_scored\"]]\n","print(features)\n","\n","collect();\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Generate Featuers"]},{"cell_type":"markdown","metadata":{},"source":["## Step1. Basic Features"]},{"cell_type":"code","execution_count":135,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:18.239979Z","iopub.status.busy":"2023-12-15T11:18:18.239682Z","iopub.status.idle":"2023-12-15T11:18:18.265404Z","shell.execute_reply":"2023-12-15T11:18:18.264498Z","shell.execute_reply.started":"2023-12-15T11:18:18.239952Z"},"trusted":true},"outputs":[],"source":["# Function to compute triplet imbalance in parallel using Numba\n","@njit(parallel=True)\n","def compute_triplet_imbalance(df_values, comb_indices):\n","    num_rows = df_values.shape[0]\n","    num_combinations = len(comb_indices)\n","    imbalance_features = np.empty((num_rows, num_combinations))\n","\n","    # 🔁 Loop through all combinations of triplets\n","    for i in prange(num_combinations):\n","        a, b, c = comb_indices[i]\n","        \n","        # 🔁 Loop through rows of the DataFrame\n","        for j in range(num_rows):\n","            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n","            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n","            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n","            \n","            # 🚫 Prevent division by zero\n","            if mid_val == min_val:\n","                imbalance_features[j, i] = np.nan\n","            else:\n","                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n","\n","    return imbalance_features\n","\n","# 📈 Function to calculate triplet imbalance for given price data and a DataFrame\n","def calculate_triplet_imbalance_numba(price, df):\n","    # Convert DataFrame to numpy array for Numba compatibility\n","    df_values = df[price].values\n","    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n","\n","    # Calculate the triplet imbalance using the Numba-optimized function\n","    features_array = compute_triplet_imbalance(df_values, comb_indices)\n","\n","    # Create a DataFrame from the results\n","    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n","    features = pd.DataFrame(features_array, columns=columns)\n","\n","    return features\n","\n","# Function to generate imbalance features\n","def imbalance_features(df):\n","    def __imbalance_features(df):\n","        # Define lists of price and size-related column names\n","        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","\n","        # V1 features\n","        # Calculate various features using Pandas eval function\n","        df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n","        df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n","        df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n","        df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n","        df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n","        \n","        # Create features for pairwise price imbalances\n","        for c in combinations(prices, 2):\n","            df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n","            \n","        # V2 features\n","        # Calculate additional features\n","        df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n","        df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n","        df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n","        df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n","        df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n","        df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n","        # Calculate the imbalance ratio\n","        df['match_balance'] = ( df['matched_size']  + (df['imbalance_buy_sell_flag'] * df['imbalance_size'])) / df['matched_size']\n","        return df\n","\n","    if DEVICE == 'gpu':\n","        import cudf\n","        df = cudf.from_pandas(df)\n","        df = __imbalance_features(df)\n","        df = df.to_pandas()\n","    else:\n","        df = __imbalance_features(df)\n","    # Replace infinite values with 0\n","    return df.replace([np.inf, -np.inf], 0)\n","\n","def numba_imb_features(df):\n","    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","    \n","    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n","        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n","        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n","        \n","    # Calculate triplet imbalance features using the Numba-optimized function\n","    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n","        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n","        df[triplet_feature.columns] = triplet_feature.values\n","    return df\n","\n","# 📅 Function to generate time and stock-related features\n","def other_features(df):\n","    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n","    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n","    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n","\n","    # Map global features to the DataFrame\n","    for key, value in global_stock_id_feats.items():\n","        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n","\n","    return df\n","\n","# 🚀 Function to generate all features by combining imbalance and other features\n","def generate_basic_features(df):\n","    prev_cols = list(df.columns)\n","\n","    # Generate imbalance features\n","    df = imbalance_features(df)\n","    df = numba_imb_features(df)\n","    df = other_features(df)\n","\n","    df = default_sort(df)    \n","    \n","    df = reduce_mem_usage(df, \"generate_basic_features\")\n","    collect()  # Perform garbage collection to free up memory\n","    return df"]},{"cell_type":"code","execution_count":136,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:18.267167Z","iopub.status.busy":"2023-12-15T11:18:18.266770Z","iopub.status.idle":"2023-12-15T11:18:58.098749Z","shell.execute_reply":"2023-12-15T11:18:58.097814Z","shell.execute_reply.started":"2023-12-15T11:18:18.267135Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Step1. Generate general Global Stock ID Features and basic features\n","['market_urgency', 'price_spread', 'matched_size_bid_size_ask_size_imb2', 'global_ptp_size', 'near_price_wap_imb', 'imbalance_momentum', 'seconds', 'near_price_bid_price_imb', 'all_prices_mean', 'price_pressure', 'all_sizes_skew', 'all_prices_skew', 'match_balance', 'bid_price_wap_imb', 'matched_imbalance', 'mid_price', 'bid_size_ask_size_imbalance_size_imb2', 'reference_price_bid_price_imb', 'ask_price_wap_imb', 'dow', 'global_ptp_price', 'ask_price_bid_price_reference_price_imb2', 'global_median_price', 'reference_price_far_price_imb', 'far_price_near_price_imb', 'matched_size_ask_size_imbalance_size_imb2', 'spread_intensity', 'all_sizes_kurt', 'ask_price_bid_price_imb', 'all_sizes_std', 'minute', 'far_price_wap_imb', 'reference_price_wap_imb', 'reference_price_near_price_imb', 'all_prices_kurt', 'ask_price_wap_reference_price_imb2', 'all_prices_std', 'ask_price_bid_price_wap_imb2', 'near_price_ask_price_imb', 'size_imbalance', 'global_median_size', 'global_std_size', 'depth_pressure', 'far_price_ask_price_imb', 'all_sizes_mean', 'global_std_price', 'volume', 'bid_price_wap_reference_price_imb2', 'reference_price_ask_price_imb', 'liquidity_imbalance', 'matched_size_bid_size_imbalance_size_imb2', 'far_price_bid_price_imb']\n","RAM memory GB usage = 1.608\n","CPU times: user 751 ms, sys: 65.4 ms, total: 817 ms\n","Wall time: 899 ms\n"]}],"source":["%%time\n","\n","print(\"Step1. Generate general Global Stock ID Features and basic features\")\n","prev_cols = list(df_train.columns)\n","global_stock_id_feats = {\n","    \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n","    \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n","    \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n","    \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n","    \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n","    \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n","}\n","\n","df_train = generate_basic_features(df_train)\n","\n","generated_feature_name = list(set(df_train.columns) - set(prev_cols))\n","features += generated_feature_name\n","print(generated_feature_name)\n","\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["## Step2. Enhance features"]},{"cell_type":"code","execution_count":137,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:58.100454Z","iopub.status.busy":"2023-12-15T11:18:58.100119Z","iopub.status.idle":"2023-12-15T11:18:58.118236Z","shell.execute_reply":"2023-12-15T11:18:58.117215Z","shell.execute_reply.started":"2023-12-15T11:18:58.100429Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Step2. Generate enhanced features\n","CPU times: user 403 µs, sys: 104 µs, total: 507 µs\n","Wall time: 501 µs\n"]}],"source":["%%time\n","\n","print(\"Step2. Generate enhanced features\")\n","prev_cols = list(df_train.columns)\n","\n","@njit()\n","def cal_diff(x, window):\n","    # pands diffより遅い\n","    # xの長さと同じ大きさの配列を作成し、初期値をNaNに設定\n","    log_diff = np.full(x.shape, np.nan)\n","    # 指定されたwindowに基づいて差分を計算\n","    for i in range(window, len(x)):\n","        log_diff[i] = x[i] - x[i - window]\n","\n","    return log_diff\n","\n","#@njit()\n","#@njit(parallel=True)\n","@njit()\n","def cal_vix(x, window, offset=0):\n","    log_x = np.log(x + offset)\n","    log_diff = np.empty(log_x.shape)\n","    roll_std = np.empty(log_diff.shape)\n","\n","    for i in prange(1, len(log_x)):\n","        log_diff[i] = log_x[i] - log_x[i - 1]\n","    \n","    # ローリング標準偏差を計算\n","    # jitを使わない場合、roll_std[i] = np.std(log_diff[i-window+1:i+1], ddof=1)と書ける(不偏推定量を使うためddof=1)\n","    # jitを使う場合、ddof=1は使えないので、標準偏差の計算を自分で実装する\n","    for i in prange(window, len(log_diff)):\n","        window_values = log_diff[i-window+1:i+1]\n","        mean = np.mean(window_values)\n","        sum_sq_diff = np.sum((window_values - mean) ** 2)\n","        roll_std[i] = np.sqrt(sum_sq_diff / (window - 1))\n","\n","    return roll_std\n","\n","USE_DASK = False\n","import dask.dataframe as dd\n","def generate_historical_features(df):\n","    def __generate_historical_features(df):\n","        print(\"generate_historical_features\")\n","        target_cols = ['wap', 'match_balance']\n","        if USE_INDEX:\n","            target_cols.append('index_mean_wap')\n","            target_cols.append('index_mean_match_balance')\n","        if USE_REVEALED_TARGETS:\n","            target_cols.append('revealed_target')\n","\n","        grouped = df.groupby(['stock_id', 'date_id'])\n","\n","        for col in target_cols:\n","            for window in [3, 5, 7]:\n","                col_diff_name = f\"{col}_diff_{window}\"\n","                df[col_diff_name] = grouped[col].diff(window)\n","                #df[col_diff_name] = grouped[col].transform(lambda x: cal_diff(x.values, window))\n","\n","                col_vix_name = f\"{col}_vix_{window}\"\n","\n","                if col == 'revealed_target':\n","                    offset = 10\n","                else:\n","                    offset = 0\n","                \n","                #df[col_vix_name] = grouped[col].transform(lambda x: np.log(x).diff().rolling(window).std())\n","                df[col_vix_name] = grouped[col].transform(lambda x: cal_vix(x.values, window))\n","                #df[col_vix_name] = grouped[col].apply(lambda x: np.log(x + 100).diff().rolling(2).std()).reset_index()[col]\n","\n","        return df\n","\n","    # gpu, dskでも速度が出ないので、cpuで実行\n","    \"\"\"\n","    if DEVICE == 'gpu':\n","        import cudf\n","        df = cudf.from_pandas(df)\n","        df = __generate_historical_features(df)\n","        df = df.to_pandas()\n","    else:\n","        if USE_DASK:\n","            df = dd.from_pandas(df, npartitions=4)  # npartitionsは使用するコアの数に応じて調整\n","            df = df.set_index('stock_id')\n","            df = __generate_historical_features(df)\n","            df = df.compute()\n","        else:\n","            df = __generate_historical_features(df)\n","    \"\"\"\n","    df = __generate_historical_features(df)\n","\n","    df = df.replace([np.inf, -np.inf], 0)\n","    return df\n","\n","# サブセットを処理する関数\n","def subset_generate_historical_features(df_subset):\n","    return generate_historical_features(df_subset)\n","\n","# 並列処理を実行する関数\n","def parallel_generate_historical_features(df, num_threads=NUM_THREADS):\n","    # DataFrameを 'stock_id' でグループ化\n","    grouped = df.groupby('stock_id')\n","\n","    # 並列処理の実行\n","    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","        results = executor.map(subset_generate_historical_features, [group for _, group in grouped])\n","\n","    # 結果の統合\n","    results = pd.concat(results)\n","    return results\n","\n","def generate_index_features(df):\n","     # Calculating mean and std for 'wap' and 'match_balance'\n","    wap_stats = df.groupby(['date_id', 'seconds_in_bucket'])['wap'].agg(['mean', 'std']).reset_index()\n","    match_balance_stats = df.groupby(['date_id', 'seconds_in_bucket'])['match_balance'].agg(['mean', 'std']).reset_index()\n","\n","    # Adding prefix and suffix\n","    wap_stats = wap_stats.add_prefix('index_').add_suffix('_wap')\n","    match_balance_stats = match_balance_stats.add_prefix('index_').add_suffix('_match_balance')\n","\n","    # Adjusting column names for merging\n","    wap_stats = wap_stats.rename(columns={'index_date_id_wap': 'date_id', 'index_seconds_in_bucket_wap': 'seconds_in_bucket'})\n","    match_balance_stats = match_balance_stats.rename(columns={'index_date_id_match_balance': 'date_id', 'index_seconds_in_bucket_match_balance': 'seconds_in_bucket'})\n","\n","    # Merging with the original dataframe\n","    df = df.merge(wap_stats, on=['date_id', 'seconds_in_bucket'], how='left')\n","    df = df.merge(match_balance_stats, on=['date_id', 'seconds_in_bucket'], how='left')\n","\n","    return df\n","\n","def generate_normalized_features(df, is_train):\n","    print(\"generate_normalized_features\")\n","    if is_train:\n","        df['n_target'] = (df['target'] - global_target['mean']) / global_target['std']\n","    df['n_wap'] = (df['wap'] - global_wap['mean']) / global_wap['std']\n","    df['n_match_balance'] = (df['match_balance'] - global_mathch_balance['mean']) / global_mathch_balance['std']\n","    df['n_reference_price'] = (df['reference_price'] - global_reference_price['mean']) / global_reference_price['std']\n","    \n","    df = reduce_mem_usage(df, \"generate_normalized_features\")\n","    return df\n","\n","def generate_enhance_features(df, is_train=False):\n","    print(\"generate_enhance_features\")\n","    if is_train:\n","        if USE_REVEALED_TARGETS:\n","            print(\"Use revealed targets\")\n","            df[f\"revealed_target\"] = df.groupby(['stock_id', 'seconds_in_bucket'])['target'].shift(1)\n","            df = df.dropna(subset=[\"revealed_target\"])\n","            df = default_sort(df)\n","        else:\n","            print(\"Dosent't use revealed targets\")\n","    if USE_INDEX:\n","        print(\"Use index\")\n","        current_time = time.time()\n","        df = generate_index_features(df)\n","        print(f\"generate_index_features {time.time() - current_time:.2f} [sec]\")\n","    current_time = time.time()\n","    \n","    if is_train:\n","        df = parallel_generate_historical_features(df)\n","    else:\n","        df = generate_historical_features(df)\n","    print(f\"generate_historical_features {time.time() - current_time:.2f} [sec]\")\n","\n","    df = df.reset_index(drop=True)\n","    df = default_sort(df)\n","    df = reduce_mem_usage(df, \"generate_enhance_features\")\n","    collect()\n","    return df"]},{"cell_type":"code","execution_count":138,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:58.120341Z","iopub.status.busy":"2023-12-15T11:18:58.119551Z","iopub.status.idle":"2023-12-15T11:19:30.664219Z","shell.execute_reply":"2023-12-15T11:19:30.663280Z","shell.execute_reply.started":"2023-12-15T11:18:58.120305Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["generate_enhance_features\n","Use revealed targets\n","Use index\n","generate_index_features 0.02 [sec]\n","generate_historical_features\n","generate_historical_features 0.42 [sec]\n","['index_mean_wap_vix_7', 'index_std_wap', 'index_mean_match_balance_vix_5', 'wap_diff_3', 'wap_diff_5', 'index_mean_match_balance_diff_5', 'revealed_target', 'index_std_match_balance', 'index_mean_match_balance_vix_7', 'index_mean_match_balance', 'revealed_target_vix_5', 'index_mean_wap_diff_3', 'revealed_target_diff_7', 'match_balance_diff_3', 'revealed_target_vix_7', 'index_mean_wap_diff_5', 'wap_vix_5', 'match_balance_vix_7', 'match_balance_diff_5', 'match_balance_diff_7', 'index_mean_match_balance_diff_3', 'index_mean_match_balance_diff_7', 'revealed_target_diff_3', 'wap_vix_7', 'index_mean_wap', 'index_mean_wap_diff_7', 'index_mean_wap_vix_3', 'revealed_target_vix_3', 'index_mean_match_balance_vix_3', 'index_mean_wap_vix_5', 'revealed_target_diff_5', 'match_balance_vix_3', 'wap_vix_3', 'wap_diff_7', 'match_balance_vix_5']\n","RAM memory GB usage = 1.603\n"]}],"source":["df_train = generate_enhance_features(df_train, is_train=True)\n","generated_feature_name = list(set(df_train.columns) - set(prev_cols))\n","features += generated_feature_name\n","print(generated_feature_name)\n","\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"code","execution_count":139,"metadata":{},"outputs":[],"source":["if IS_DEBUG:\n","    df_train.to_csv(f\"{BASE_OUTPUT_PATH}/df_train.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Feature selection"]},{"cell_type":"code","execution_count":140,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:30.665637Z","iopub.status.busy":"2023-12-15T11:19:30.665355Z","iopub.status.idle":"2023-12-15T11:19:30.675372Z","shell.execute_reply":"2023-12-15T11:19:30.674460Z","shell.execute_reply.started":"2023-12-15T11:19:30.665612Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['stock_id',\n"," 'seconds_in_bucket',\n"," 'imbalance_size',\n"," 'imbalance_buy_sell_flag',\n"," 'reference_price',\n"," 'matched_size',\n"," 'far_price',\n"," 'near_price',\n"," 'bid_price',\n"," 'bid_size',\n"," 'ask_price',\n"," 'ask_size',\n"," 'wap',\n"," 'market_urgency',\n"," 'price_spread',\n"," 'matched_size_bid_size_ask_size_imb2',\n"," 'global_ptp_size',\n"," 'near_price_wap_imb',\n"," 'imbalance_momentum',\n"," 'seconds',\n"," 'near_price_bid_price_imb',\n"," 'all_prices_mean',\n"," 'price_pressure',\n"," 'all_sizes_skew',\n"," 'all_prices_skew',\n"," 'match_balance',\n"," 'bid_price_wap_imb',\n"," 'matched_imbalance',\n"," 'mid_price',\n"," 'bid_size_ask_size_imbalance_size_imb2',\n"," 'reference_price_bid_price_imb',\n"," 'ask_price_wap_imb',\n"," 'dow',\n"," 'global_ptp_price',\n"," 'ask_price_bid_price_reference_price_imb2',\n"," 'global_median_price',\n"," 'reference_price_far_price_imb',\n"," 'far_price_near_price_imb',\n"," 'matched_size_ask_size_imbalance_size_imb2',\n"," 'spread_intensity',\n"," 'all_sizes_kurt',\n"," 'ask_price_bid_price_imb',\n"," 'all_sizes_std',\n"," 'minute',\n"," 'far_price_wap_imb',\n"," 'reference_price_wap_imb',\n"," 'reference_price_near_price_imb',\n"," 'all_prices_kurt',\n"," 'ask_price_wap_reference_price_imb2',\n"," 'all_prices_std',\n"," 'ask_price_bid_price_wap_imb2',\n"," 'near_price_ask_price_imb',\n"," 'size_imbalance',\n"," 'global_median_size',\n"," 'global_std_size',\n"," 'depth_pressure',\n"," 'far_price_ask_price_imb',\n"," 'all_sizes_mean',\n"," 'global_std_price',\n"," 'volume',\n"," 'bid_price_wap_reference_price_imb2',\n"," 'reference_price_ask_price_imb',\n"," 'liquidity_imbalance',\n"," 'matched_size_bid_size_imbalance_size_imb2',\n"," 'far_price_bid_price_imb',\n"," 'index_mean_wap_vix_7',\n"," 'index_std_wap',\n"," 'index_mean_match_balance_vix_5',\n"," 'wap_diff_3',\n"," 'wap_diff_5',\n"," 'index_mean_match_balance_diff_5',\n"," 'revealed_target',\n"," 'index_std_match_balance',\n"," 'index_mean_match_balance_vix_7',\n"," 'index_mean_match_balance',\n"," 'revealed_target_vix_5',\n"," 'index_mean_wap_diff_3',\n"," 'revealed_target_diff_7',\n"," 'match_balance_diff_3',\n"," 'revealed_target_vix_7',\n"," 'index_mean_wap_diff_5',\n"," 'wap_vix_5',\n"," 'match_balance_vix_7',\n"," 'match_balance_diff_5',\n"," 'match_balance_diff_7',\n"," 'index_mean_match_balance_diff_3',\n"," 'index_mean_match_balance_diff_7',\n"," 'revealed_target_diff_3',\n"," 'wap_vix_7',\n"," 'index_mean_wap',\n"," 'index_mean_wap_diff_7',\n"," 'index_mean_wap_vix_3',\n"," 'revealed_target_vix_3',\n"," 'index_mean_match_balance_vix_3',\n"," 'index_mean_wap_vix_5',\n"," 'revealed_target_diff_5',\n"," 'match_balance_vix_3',\n"," 'wap_vix_3',\n"," 'wap_diff_7',\n"," 'match_balance_vix_5']"]},"execution_count":140,"metadata":{},"output_type":"execute_result"}],"source":["# feature selection\n","if not USE_ALL_FEATUTES:\n","    features  = [\n","        \"revealed_target\",\n","        \"wap_diff_1\",\n","        \"index_mean_wap_diff_1\",\n","        \"seconds_in_bucket\",\n","        \"stock_id\",\n","    ]\n","#df_valid = df_train[\"target\"]\n","#df_train = df_train[features]\n","#if USE_REVEALED_TARGETS:\n","#    features.remove(\"revealed_target\")\n","features"]},{"cell_type":"code","execution_count":141,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:30.676954Z","iopub.status.busy":"2023-12-15T11:19:30.676623Z","iopub.status.idle":"2023-12-15T11:19:32.491520Z","shell.execute_reply":"2023-12-15T11:19:32.490523Z","shell.execute_reply.started":"2023-12-15T11:19:30.676923Z"},"trusted":true},"outputs":[{"data":{"text/plain":["index_std_match_balance                     26400\n","index_std_wap                               26400\n","revealed_target_vix_5                       20680\n","revealed_target_vix_7                       20639\n","revealed_target_vix_3                       18592\n","depth_pressure                              14487\n","far_price_bid_price_imb                     14487\n","far_price_ask_price_imb                     14487\n","far_price                                   14487\n","reference_price_far_price_imb               14487\n","far_price_near_price_imb                    14487\n","far_price_wap_imb                           14487\n","near_price                                  14400\n","near_price_wap_imb                          14400\n","near_price_bid_price_imb                    14400\n","reference_price_near_price_imb              14400\n","near_price_ask_price_imb                    14400\n","bid_price_wap_reference_price_imb2           3613\n","ask_price_bid_price_reference_price_imb2     3528\n","wap_diff_7                                   3360\n","revealed_target_diff_7                       3360\n","index_mean_match_balance_diff_7              3360\n","index_mean_wap_diff_7                        3360\n","match_balance_diff_7                         3360\n","revealed_target_diff_5                       2400\n","match_balance_diff_5                         2400\n","index_mean_wap_diff_5                        2400\n","index_mean_match_balance_diff_5              2400\n","wap_diff_5                                   2400\n","index_mean_match_balance_diff_3              1440\n","revealed_target_diff_3                       1440\n","match_balance_diff_3                         1440\n","index_mean_wap_diff_3                        1440\n","wap_diff_3                                   1440\n","index_mean_match_balance_vix_7                640\n","match_balance_vix_7                           610\n","index_mean_match_balance_vix_5                587\n","match_balance_vix_5                           574\n","match_balance_vix_3                           551\n","index_mean_match_balance_vix_3                534\n","wap_vix_5                                     109\n","index_mean_wap_vix_7                           90\n","wap_vix_3                                      78\n","wap_vix_7                                      73\n","index_mean_wap_vix_5                           73\n","index_mean_wap_vix_3                           67\n","ask_price_wap_reference_price_imb2             56\n","ask_price_bid_price_wap_imb2                   37\n","dtype: int64"]},"execution_count":141,"metadata":{},"output_type":"execute_result"}],"source":["pd_display_max()\n","nan_count = df_train[features].isna().sum()\n","#df_train[features].to_csv('train.csv', index=False)\n","nan_count = nan_count[nan_count > 0].sort_values(ascending=False)\n","nan_count"]},{"cell_type":"code","execution_count":142,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:32.493399Z","iopub.status.busy":"2023-12-15T11:19:32.492948Z","iopub.status.idle":"2023-12-15T11:19:32.497973Z","shell.execute_reply":"2023-12-15T11:19:32.496918Z","shell.execute_reply.started":"2023-12-15T11:19:32.493362Z"},"trusted":true},"outputs":[],"source":["pd_clear_display_max()"]},{"cell_type":"markdown","metadata":{},"source":["# Train function (lightgbm)"]},{"cell_type":"code","execution_count":143,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:32.499885Z","iopub.status.busy":"2023-12-15T11:19:32.499413Z","iopub.status.idle":"2023-12-15T11:19:33.389728Z","shell.execute_reply":"2023-12-15T11:19:33.388823Z","shell.execute_reply.started":"2023-12-15T11:19:32.499855Z"},"trusted":true},"outputs":[],"source":["# 📦 Import necessary libraries\n","import numpy as np\n","import lightgbm as lgb\n","from sklearn.metrics import mean_absolute_error\n","import gc\n","import os\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import KFold\n","import numpy as np\n","from dataclasses import dataclass\n","import sys\n","import shutil\n","import lightgbm as lgb\n","\n","from warnings import simplefilter\n","simplefilter(\"ignore\", category=RuntimeWarning)\n","\n","@dataclass\n","class Model:\n","    booster: lgb.Booster\n","    fold: int\n","    feature_importance: pd.DataFrame\n","    score: float\n","    best_iteration: int\n","    train_time: float = None\n","    weight: float = None\n","    mem_usage: float = None\n","    train_func: str = None\n","    is_latest: bool = False\n","\n","def train_model(train_x, train_y, val_x, val_y, best_params=None):\n","    trains = lgb.Dataset(train_x, train_y)\n","    valids = lgb.Dataset(val_x, val_y, reference=trains)\n","\n","    verbose_eval = -1\n","    if best_params is None:\n","        params = lgb_params\n","    else:\n","        params = best_params\n","\n","    print(\"Use params:\")\n","    print(params)\n","    print(f\"stopping_rounds: {stopping_rounds}, num_boost_round: {num_boost_round}\")\n","    \n","    booster = lgb.train(\n","        params,\n","        trains,\n","        valid_sets=valids, # 検証データ\n","        num_boost_round=num_boost_round,\n","        keep_training_booster=True,\n","        callbacks=[\n","                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n","                lgb.log_evaluation(verbose_eval)\n","        ]\n","    )\n","\n","    del trains, valids\n","    return booster\n","\n","def cross_train(df, key, n_splits, features, valid_name, best_params=None):\n","    \"\"\" For Cross Train\n","\n","    Args:\n","        df (_type_): _description_\n","        n_splits (_type_): _description_\n","\n","    Returns:\n","        _type_: _description_\n","    \"\"\"\n","    print(\"----------------------------------------\")\n","    print(f\"Cross Train key id {key}: start, shape: {df_train.shape}, n_splits: {n_splits}\")\n","    print(f\"num_boost_round: {num_boost_round}, stopping_rounds: {stopping_rounds}, folds: {num_folds}\")\n","\n","    models = []\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","    df.reset_index(drop=True, inplace=True)\n","    \n","    for fold, (train_indices, valid_indices) in enumerate(kf.split(df)):\n","        print(f\"----- Train {key}: {fold} start -----\")\n","        now_time = time.time()\n","\n","        print(f\"train_indices: {train_indices}, valid_indices: {valid_indices}\")\n","        X_train, X_valid = df[features].iloc[train_indices], df[features].iloc[valid_indices]\n","        y_train, y_valid = df[valid_name].loc[train_indices], df[valid_name].loc[valid_indices]\n","        print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}, y_train: {y_train.shape}, y_valid: {y_valid.shape}\")\n","\n","        booster = train_model(X_train, y_train, X_valid, y_valid, best_params)\n","        print(booster.best_score)\n","        y_valid_pred = booster.predict(X_valid)\n","        \n","        score = mean_absolute_error(y_valid, y_valid_pred)\n","        train_time = time.time() - now_time\n","        mem_usage = sys.getsizeof(booster) / (1024 * 1024) # MB\n","        model = Model(booster, fold, booster.feature_importance(), score, booster.best_iteration, train_time, weight= 1 / n_splits, mem_usage=mem_usage, train_func=\"lightgbm\", is_latest=True)\n","        print(f\"{key}: {fold} end, score: {score}, time: {model.train_time}, best_iteration: {model.best_iteration}, memory usage: {model.mem_usage}\")\n","        \n","        models.append(model)\n","        \n","        del X_train, X_valid, y_train, y_valid\n","        gc.collect()\n","        print(GetMemUsage())\n","        print(f\"----- Train {key}: {fold} end -----\")\n","\n","    print(f\"Cross train {key} model len {len(models)}\")\n","    print(\"----------------------------------------\")\n","    return key, models"]},{"cell_type":"markdown","metadata":{},"source":["# Train function (optuna)"]},{"cell_type":"code","execution_count":144,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:33.391267Z","iopub.status.busy":"2023-12-15T11:19:33.390965Z","iopub.status.idle":"2023-12-15T11:19:33.412793Z","shell.execute_reply":"2023-12-15T11:19:33.411830Z","shell.execute_reply.started":"2023-12-15T11:19:33.391226Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 91 µs, sys: 10 µs, total: 101 µs\n","Wall time: 102 µs\n"]}],"source":["%%time\n","\n","import optuna.integration.lightgbm as optuna_lgb\n","import optuna\n","import lightgbm\n","optuna.logging.set_verbosity(optuna.logging.ERROR)\n","\n","class TunerCVCheckpointCallback(object):\n","    \"\"\"Optuna の LightGBMTunerCV から学習済みモデルを取り出すためのコールバック\"\"\"\n","\n","    def __init__(self):\n","        # Models\n","        self.models = []\n","        self.counter = 0\n","\n","    def get_models(self):\n","        # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster\n","        return self.models\n","\n","    def __call__(self, env: lightgbm.callback.CallbackEnv):\n","        \"\"\"_summary_\n","\n","        Args:\n","            env (lightgbm.callback.CallbackEnv): _description_\n","            \"model\",\n","            \"params\",\n","            \"iteration\",\n","            \"begin_iteration\",\n","            \"end_iteration\",\n","            \"evaluation_result_list\"\n","        \"\"\"\n","        print(\"\")\n","\n","        self.counter += 1\n","        print(\"-------------------\")\n","        print(f\"Counter: {self.counter}\")\n","        print(f\"Iteration: {env.iteration}\")\n","        print(f\"Begin_iteration: {env.begin_iteration}\")\n","        print(f\"End_iteration: {env.end_iteration}\")\n","        print(f\"Evaluation_result_list: {env.evaluation_result_list}\")\n","        print(f\"Model best_iteration: {env.model.best_iteration}\")\n","        print(\"Params: \", env.params)\n","        #self.models.append(env.model)\n","        del env\n","\n","        collect();\n","        print(GetMemUsage())\n","\n","def optuna_tuning(df, n_splits, features, valid_name, model_save_path):\n","    df_train = df[features]\n","    df_valid = df[valid_name]\n","    \n","    trains = optuna_lgb.Dataset(df_train, df_valid)\n","    \n","    print(\"------- Optuna Tuning Start -------\")\n","    now_time = time.time()\n","    print(f\"num_boost_round: {num_boost_round}, stopping_rounds: {stopping_rounds}, folds: {num_folds}\")\n","\n","    folds = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","    checkpoint_cb = TunerCVCheckpointCallback()\n","    \n","    verbose_eval = 0\n","    # https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.lightgbm.LightGBMTunerCV.html\n","    tuner = optuna_lgb.LightGBMTunerCV(\n","        optuna_params,\n","        trains,\n","        num_boost_round=num_boost_round,\n","        folds=folds,\n","        show_progress_bar=False,\n","        return_cvbooster=True,\n","        verbosity=-1,\n","        model_dir=model_save_path,\n","        optuna_seed=seed,\n","        time_budget=OPTUNA_TIME_BUDGET,\n","        callbacks=[\n","                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n","                lgb.log_evaluation(verbose_eval),\n","                checkpoint_cb\n","        ]\n","    )\n","    \n","    tuner.run()\n","    best_params = tuner.best_params\n","    \n","    print(\"Params: \")\n","    for key, value in best_params.items():\n","        print(\" {}: {}\".format(key, value))\n","\n","    print(\"\")\n","    print(\"len(tuner.study.trials): \", len(tuner.study.trials))\n","    #print(\"len(checkpoint_cb.cv_boosters): \", len(checkpoint_cb.models))\n","    print(\"Tuner best_params\", tuner.best_params)\n","    print(\"Tuner best score: \", tuner.best_score)\n","   \n","    # 最も良かったパラメータをキーにして学習済みモデルを取り出す\n","    best_booster = tuner.get_best_booster()\n","    score = -1\n","    train_time = time.time() - now_time\n","    mem_usage = sys.getsizeof(best_booster) / (1024 * 1024) # MB\n","    feature_importance = np.mean(best_booster.feature_importance(), axis=0)\n","\n","    best_model = Model(best_booster, 1, feature_importance, score, best_booster.best_iteration, train_time, weight= 1, mem_usage=mem_usage, train_func=\"optuna_lgb\")\n","    print(\"------- Optuna Tuning End -------\")\n","    return best_params, best_model\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":145,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:33.414776Z","iopub.status.busy":"2023-12-15T11:19:33.414181Z","iopub.status.idle":"2023-12-15T11:26:09.828083Z","shell.execute_reply":"2023-12-15T11:26:09.827090Z","shell.execute_reply.started":"2023-12-15T11:19:33.414742Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------\n","Cross Train key id -1: start, shape: (26400, 104), n_splits: 2\n","num_boost_round: 1, stopping_rounds: 1, folds: 2\n","----- Train -1: 0 start -----\n","train_indices: [    1     2     7 ... 26396 26398 26399], valid_indices: [    0     3     4 ... 26391 26392 26397]\n","X_train: (13200, 100), X_valid: (13200, 100), y_train: (13200,), y_valid: (13200,)\n","Use params:\n","{'objective': 'mae', 'n_estimators': 6000, 'num_leaves': 256, 'subsample': 0.6, 'colsample_bytree': 0.8, 'learning_rate': 0.00871, 'max_depth': 11, 'n_jobs': 4, 'verbosity': -1, 'importance_type': 'gain', 'device': 'cpu'}\n","stopping_rounds: 1, num_boost_round: 1\n","Training until validation scores don't improve for 1 rounds\n","Early stopping, best iteration is:\n","[280]\tvalid_0's l1: 4.25478\n","defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 4.254775875930648)])})\n","-1: 0 end, score: 4.254775875930645, time: 4.49294900894165, best_iteration: 280, memory usage: 5.340576171875e-05\n","RAM memory GB usage = 1.636\n","----- Train -1: 0 end -----\n","----- Train -1: 1 start -----\n","train_indices: [    0     3     4 ... 26391 26392 26397], valid_indices: [    1     2     7 ... 26396 26398 26399]\n","X_train: (13200, 100), X_valid: (13200, 100), y_train: (13200,), y_valid: (13200,)\n","Use params:\n","{'objective': 'mae', 'n_estimators': 6000, 'num_leaves': 256, 'subsample': 0.6, 'colsample_bytree': 0.8, 'learning_rate': 0.00871, 'max_depth': 11, 'n_jobs': 4, 'verbosity': -1, 'importance_type': 'gain', 'device': 'cpu'}\n","stopping_rounds: 1, num_boost_round: 1\n","Training until validation scores don't improve for 1 rounds\n","Early stopping, best iteration is:\n","[346]\tvalid_0's l1: 4.19801\n","defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 4.198009751184259)])})\n","-1: 1 end, score: 4.198009751184262, time: 5.164078950881958, best_iteration: 346, memory usage: 5.340576171875e-05\n","RAM memory GB usage = 1.706\n","----- Train -1: 1 end -----\n","Cross train -1 model len 2\n","----------------------------------------\n","RAM memory GB usage = 1.707\n","CPU times: user 19.1 s, sys: 4.12 s, total: 23.2 s\n","Wall time: 10 s\n"]}],"source":["%%time\n","\n","KEY = \"-1\"\n","\n","# Train\n","best_params = None\n","key_models = None\n","if USE_OPTUNA:\n","    model_save_base_path = f\"{BASE_OUTPUT_PATH}/model\"\n","    if os.path.exists(model_save_base_path):\n","        print(f\"{model_save_base_path} already exists, clean up it.\")\n","        shutil.rmtree(model_save_base_path)\n","    os.makedirs(model_save_base_path)\n","    print(f\"model_save_base_path: {model_save_base_path}\")\n","\n","    best_params, best_model = optuna_tuning(df=df_train, n_splits=num_folds, features=features, valid_name=\"target\", model_save_path=model_save_base_path)\n","    key_models = [(KEY, [best_model])]\n","else:\n","    #key_models = df_train.groupby(\"seconds_in_bucket\").apply(lambda x: cross_train(df=x, key=x.name, n_splits=num_folds, feature_name=feature_name, valid_name=\"target\", best_params=best_params))\n","    key_models = [cross_train(df_train, key=KEY, n_splits=num_folds, features=features, valid_name=\"target\", best_params=best_params)]\n","    if IS_USE_SAVED_MODEL:\n","        model_save_base_path = f\"{BASE_OUTPUT_PATH}/model\"\n","        if os.path.exists(model_save_base_path):\n","            print(f\"{model_save_base_path} already exists, clean up it.\")\n","            shutil.rmtree(model_save_base_path)\n","        os.makedirs(model_save_base_path)\n","\n","        key_model_paths = []\n","        for key, models in key_models:\n","            model_save_path = f\"{model_save_base_path}/{key}\"\n","            os.makedirs(model_save_path)\n","            model_paths = []\n","            for model in models:\n","                model_save_fullpath = f\"{model_save_path}/model_{key}_{model.fold}.txt\"\n","                model.model.save_model(model_save_fullpath)\n","                model_paths.append(model_save_fullpath)\n","            key_model_paths.append((key, model_paths))\n","\n","        model_dict_saved = {key: model_paths for key, model_paths in key_model_paths}\n","        print(model_dict_saved)\n","\n","\n","model_dict = {key: model for key, model in key_models}\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Additional training by important features"]},{"cell_type":"code","execution_count":146,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Additional Train\n","df_train_over: (178, 106)\n","----------------------------------------\n","Cross Train key id -1: start, shape: (26400, 106), n_splits: 2\n","num_boost_round: 1, stopping_rounds: 1, folds: 2\n","----- Train -1: 0 start -----\n","train_indices: [  1   3   5   7   8  13  14  17  20  21  33  34  35  37  39  43  46  47\n","  48  49  50  52  53  54  57  58  59  61  62  63  70  71  72  73  74  77\n","  79  80  83  84  86  87  88  89  91  92  94  99 101 102 103 105 106 107\n"," 110 112 115 116 120 121 124 125 129 130 131 133 134 135 136 139 148 149\n"," 151 152 155 156 157 160 161 162 163 165 166 168 172 173 175 176 177], valid_indices: [  0   2   4   6   9  10  11  12  15  16  18  19  22  23  24  25  26  27\n","  28  29  30  31  32  36  38  40  41  42  44  45  51  55  56  60  64  65\n","  66  67  68  69  75  76  78  81  82  85  90  93  95  96  97  98 100 104\n"," 108 109 111 113 114 117 118 119 122 123 126 127 128 132 137 138 140 141\n"," 142 143 144 145 146 147 150 153 154 158 159 164 167 169 170 171 174]\n","X_train: (89, 24), X_valid: (89, 24), y_train: (89,), y_valid: (89,)\n","Use params:\n","{'objective': 'mae', 'n_estimators': 6000, 'num_leaves': 256, 'subsample': 0.6, 'colsample_bytree': 0.8, 'learning_rate': 0.00871, 'max_depth': 11, 'n_jobs': 4, 'verbosity': -1, 'importance_type': 'gain', 'device': 'cpu'}\n","stopping_rounds: 1, num_boost_round: 1\n","Training until validation scores don't improve for 1 rounds\n","Early stopping, best iteration is:\n","[3]\tvalid_0's l1: 26.4064\n","defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 26.40641803184824)])})\n","-1: 0 end, score: 26.406418031848236, time: 0.0065882205963134766, best_iteration: 3, memory usage: 5.340576171875e-05\n","RAM memory GB usage = 1.717\n","----- Train -1: 0 end -----\n","----- Train -1: 1 start -----\n","train_indices: [  0   2   4   6   9  10  11  12  15  16  18  19  22  23  24  25  26  27\n","  28  29  30  31  32  36  38  40  41  42  44  45  51  55  56  60  64  65\n","  66  67  68  69  75  76  78  81  82  85  90  93  95  96  97  98 100 104\n"," 108 109 111 113 114 117 118 119 122 123 126 127 128 132 137 138 140 141\n"," 142 143 144 145 146 147 150 153 154 158 159 164 167 169 170 171 174], valid_indices: [  1   3   5   7   8  13  14  17  20  21  33  34  35  37  39  43  46  47\n","  48  49  50  52  53  54  57  58  59  61  62  63  70  71  72  73  74  77\n","  79  80  83  84  86  87  88  89  91  92  94  99 101 102 103 105 106 107\n"," 110 112 115 116 120 121 124 125 129 130 131 133 134 135 136 139 148 149\n"," 151 152 155 156 157 160 161 162 163 165 166 168 172 173 175 176 177]\n","X_train: (89, 24), X_valid: (89, 24), y_train: (89,), y_valid: (89,)\n","Use params:\n","{'objective': 'mae', 'n_estimators': 6000, 'num_leaves': 256, 'subsample': 0.6, 'colsample_bytree': 0.8, 'learning_rate': 0.00871, 'max_depth': 11, 'n_jobs': 4, 'verbosity': -1, 'importance_type': 'gain', 'device': 'cpu'}\n","stopping_rounds: 1, num_boost_round: 1\n","Training until validation scores don't improve for 1 rounds\n","Early stopping, best iteration is:\n","[2]\tvalid_0's l1: 25.3229\n","defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 25.322925210136887)])})\n","-1: 1 end, score: 25.322925210136887, time: 0.006853818893432617, best_iteration: 2, memory usage: 5.340576171875e-05\n","RAM memory GB usage = 1.72\n","----- Train -1: 1 end -----\n","Cross train -1 model len 2\n","----------------------------------------\n","RAM memory GB usage = 1.72\n","CPU times: user 3.81 s, sys: 28.4 ms, total: 3.84 s\n","Wall time: 957 ms\n"]}],"source":["%%time\n","\n","additional_features = ['reference_price', 'match_balance_diff_5', 'wap', 'global_std_price',\n","       'all_sizes_skew', 'matched_size_bid_size_ask_size_imb2', 'ask_price',\n","       'index_mean_wap_diff_7', 'seconds_in_bucket', 'mid_price',\n","       'ask_price_bid_price_reference_price_imb2', 'wap_vix_7', 'wap_vix_3',\n","       'all_sizes_std', 'global_median_price', 'volume', 'all_sizes_mean',\n","       'revealed_target', 'wap_diff_7', 'global_ptp_size',\n","       'reference_price_wap_imb', 'bid_price_wap_reference_price_imb2',\n","       'stock_id', 'global_median_size']\n","\n","if not USE_REVEALED_TARGETS:\n","      additional_features.remove('revealed_target')\n","\n","if USE_ADDITIONAL_TRAIN:\n","   print(\"Additional Train\")\n","   boosters = [m.booster for m in model_dict[KEY]]\n","   predictions_list = [booster.predict(df_train[features]) for booster in boosters]\n","   predictions = np.mean(predictions_list, 0)\n","   df_train['pred'] = predictions\n","   df_train['score'] = np.abs(df_train['target'] - df_train['pred'])\n","   df_train_over = df_train[df_train['score'] > ADDITIONAL_TRAIN_THRESHOLD]\n","   print(f\"df_train_over: {df_train_over.shape}\")\n","\n","   key_additional_models = [cross_train(df_train_over, key=KEY, n_splits=num_folds, features=additional_features, valid_name=\"target\", best_params=best_params)]\n","   additional_model_dict = {key: model for key, model in key_additional_models}\n","\n","   df_train = df_train.drop(columns=['pred', 'score'])\n","\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Update model using test"]},{"cell_type":"code","execution_count":147,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:26:09.833812Z","iopub.status.busy":"2023-12-15T11:26:09.833233Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["date_duration 165\n","Update model with test date\n","MIN LEARN MODE : [0]\n","generate_enhance_features\n","Use index\n","generate_index_features 0.01 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","update_global_train_cache\n","Updated global_train_cache, len:  165\n","165\n","     date_id  seconds_in_bucket  stock_id  origin  revealed_target    target\n","0        478                  0         0       1        -2.310276 -5.429983\n","1        478                 10         0       1        -7.200241 -3.489852\n","2        478                 20         0       1        -7.500052 -3.330112\n","3        478                 30         0       1       -12.480021 -1.900196\n","4        478                 40         0       1        -9.570122  2.189875\n","..       ...                ...       ...     ...              ...       ...\n","160      480                500         0       0         1.579523  3.999472\n","161      480                510         0       0        -0.180006  3.190041\n","162      480                520         0       0         2.199411 -0.169873\n","163      480                530         0       0         1.929998  3.110170\n","164      480                540         0       0         3.889799  0.760555\n","\n","[165 rows x 6 columns]\n","Update model with test date end\n","RAM memory GB usage = 1.733\n","CPU times: user 573 ms, sys: 17.8 ms, total: 591 ms\n","Wall time: 591 ms\n"]}],"source":["%%time\n","\n","# global train cache for continuous update\n","global_train_cache = df_train.copy()\n","# origin 0 is train, 1 is test, 2 is revaled\n","global_train_cache['origin'] = 0\n","date_duration = DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span\n","print(\"date_duration\", date_duration)\n","\n","def update_global_train_cache(df, origin, valid_key: str = 'target'):\n","    global global_train_cache\n","    df['origin'] = origin\n","    print(\"update_global_train_cache\")\n","    global_train_cache = pd.concat([global_train_cache, df], axis=0)\n","    global_train_cache = global_train_cache.dropna(subset=['target'])\n","    global_train_cache = global_train_cache.sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id', 'origin'])\n","    global_train_cache = global_train_cache.drop_duplicates(['date_id', 'seconds_in_bucket', 'stock_id'], keep='last')\n","    global_train_cache = global_train_cache.reset_index(drop=True)\n","    global_train_cache = global_train_cache.groupby(['stock_id']).tail(date_duration).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n","\n","    global_train_cache = reduce_mem_usage(global_train_cache, 'global_train_cache')\n","    print(f\"Updated global_train_cache, len: \", len(global_train_cache))\n","    if IS_DEBUG:\n","        print(len(global_train_cache))\n","        if USE_REVEALED_TARGETS:\n","            cdf = global_train_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'origin', 'revealed_target', 'target']]\n","        else:\n","            cdf = global_train_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'origin', 'target']]\n","        print(cdf)\n","\n","def update_models(df, models, features, valid_name):\n","    \"\"\" For Update Model\n","\n","    Args:\n","        df (_type_): _description_\n","        n_splits (_type_): _description_\n","\n","    Returns:\n","        _type_: _description_\n","    \"\"\"\n","\n","    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","    df.reset_index(drop=True, inplace=True)\n","    \n","    for fold, (train_indices, valid_indices) in enumerate(kf.split(df)):\n","        print(f\"{key}: {fold} update\")\n","        now_time = time.time()\n","        X_train, X_valid = df[features].iloc[train_indices], df[features].iloc[valid_indices]\n","        y_train, y_valid = df[valid_name].loc[train_indices], df[valid_name].loc[valid_indices]\n","        trains = lgb.Dataset(X_train, y_train, free_raw_data=False)\n","        valids = lgb.Dataset(X_valid, y_valid, reference=trains)\n","\n","        print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}, y_train: {y_train.shape}, y_valid: {y_valid.shape}\")\n","        models[fold].booster.update(trains)\n","\n","def keep_train_models(df, models, features, valid_name, is_append=False):\n","    print(f\"----------------- keep_train_models, is_append: {is_append}, model len: {len(models)}, df len: {len(df)} ---------------------\")\n","    train_x = df[features]\n","    train_y = df[valid_name]\n","    trains = lgb.Dataset(train_x, train_y, free_raw_data=False)\n","    verbose_eval = -1\n","\n","    counter = 0\n","    r_models = []\n","    if IS_DEBUG:\n","        print(f\"Re-train dataset:\")\n","        if USE_REVEALED_TARGETS:\n","            print(df[['date_id', 'seconds_in_bucket', 'stock_id', 'target', 'revealed_target']])\n","        else:\n","            print(df[['date_id', 'seconds_in_bucket', 'stock_id', 'target']])\n","    for model in models:\n","        print(f\"---- train start, counter: {counter} ----\")\n","        if model.is_latest:\n","            print(\"Update latest model\")\n","            now_time = time.time()\n","            booster = lgb.train(\n","                lgb_params,\n","                trains,\n","                num_boost_round=update_num_boost_round,\n","                keep_training_booster=True,\n","                init_model=model.booster,\n","            )\n","            train_time = time.time() - now_time\n","            updated_model = Model(\n","                booster=booster,\n","                fold=1,\n","                best_iteration=booster.best_iteration, \n","                feature_importance=booster.feature_importance(),\n","                score=-1, \n","                train_time=train_time, \n","                weight=-1, \n","                mem_usage=-1,\n","                is_latest=True,\n","                train_func=\"lightgbm update by test\")\n","            r_models.append(updated_model)\n","            if is_append:\n","                print(\"Adding previous model\")\n","                model.is_latest = False\n","                r_models.append(model)\n","        else:\n","            print(\"Dose not latest, just append\")\n","            r_models.append(model)\n","        counter = counter + 1\n","    print(f\"---- train end, train time: {train_time}, updated model len {len(r_models)} ----\")\n","    return r_models\n","\n","if USE_CONTINUOUS_UPDATE:\n","    try:\n","        print(\"Update model with test date\")\n","        df_test = load_test_dataset()\n","        if IS_MIN_LEARN:\n","            print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n","            df_test = df_test[df_test[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","        df_test = generate_basic_features(df_test)\n","        df_test = generate_enhance_features(df_test)\n","        update_global_train_cache(df_test, 1)\n","        #model_dict[KEY] = keep_train_models(global_train_cache, model_dict[KEY], features, \"target\", is_append=True)\n","        print(\"Update model with test date end\")\n","    except Exception as e:\n","        print(\"Cannot get test date\", e)\n","\n","collect()\n","print(GetMemUsage())\n"]},{"cell_type":"code","execution_count":148,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Key: -1, model len: 2\n","           fold     score  best_iteration  train_time\n","count  2.000000  2.000000        2.000000    2.000000\n","mean   0.500000  4.226393      313.000000    4.828514\n","std    0.707107  0.040140       46.669048    0.474561\n","min    0.000000  4.198010      280.000000    4.492949\n","25%    0.250000  4.212201      296.500000    4.660731\n","50%    0.500000  4.226393      313.000000    4.828514\n","75%    0.750000  4.240584      329.500000    4.996296\n","max    1.000000  4.254776      346.000000    5.164079\n"]}],"source":["# Show results\n","\n","for key, models in model_dict.items():\n","    print(f\"Key: {key}, model len: {len(models)}\")\n","    data = []\n","    for model in models:\n","        score = model.score\n","        best_iteration = model.best_iteration\n","        fold = model.fold\n","        train_time = model.train_time\n","        data.append({\"key\": key, \"fold\": fold, \"score\": score, \"best_iteration\": best_iteration, \"train_time\": train_time})\n","\n","    df_model = pd.DataFrame(data)\n","    print(df_model.describe())"]},{"cell_type":"code","execution_count":149,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fold</th>\n","      <th>score</th>\n","      <th>best_iteration</th>\n","      <th>train_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.500000</td>\n","      <td>4.226393</td>\n","      <td>313.000000</td>\n","      <td>4.828514</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.707107</td>\n","      <td>0.040140</td>\n","      <td>46.669048</td>\n","      <td>0.474561</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>4.198010</td>\n","      <td>280.000000</td>\n","      <td>4.492949</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.250000</td>\n","      <td>4.212201</td>\n","      <td>296.500000</td>\n","      <td>4.660731</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.500000</td>\n","      <td>4.226393</td>\n","      <td>313.000000</td>\n","      <td>4.828514</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.750000</td>\n","      <td>4.240584</td>\n","      <td>329.500000</td>\n","      <td>4.996296</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>4.254776</td>\n","      <td>346.000000</td>\n","      <td>5.164079</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           fold     score  best_iteration  train_time\n","count  2.000000  2.000000        2.000000    2.000000\n","mean   0.500000  4.226393      313.000000    4.828514\n","std    0.707107  0.040140       46.669048    0.474561\n","min    0.000000  4.198010      280.000000    4.492949\n","25%    0.250000  4.212201      296.500000    4.660731\n","50%    0.500000  4.226393      313.000000    4.828514\n","75%    0.750000  4.240584      329.500000    4.996296\n","max    1.000000  4.254776      346.000000    5.164079"]},"execution_count":149,"metadata":{},"output_type":"execute_result"}],"source":["# Check model quality\n","data = []\n","\n","for key, i_models in model_dict.items():\n","    for model in i_models:\n","        score = model.score\n","        best_iteration = model.best_iteration\n","        fold = model.fold\n","        train_time = model.train_time\n","        data.append({\"key\": key, \"fold\": fold, \"score\": score, \"best_iteration\": best_iteration, \"train_time\": train_time})\n","\n","df_model = pd.DataFrame(data)\n","df_model.describe()"]},{"cell_type":"code","execution_count":150,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>revealed_target</th>\n","      <td>1121.5</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price_wap_reference_price_imb2</th>\n","      <td>1029.5</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_mean</th>\n","      <td>994.5</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_bid_price_reference_price_imb2</th>\n","      <td>980.5</td>\n","    </tr>\n","    <tr>\n","      <th>revealed_target_diff_7</th>\n","      <td>919.5</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_3</th>\n","      <td>894.5</td>\n","    </tr>\n","    <tr>\n","      <th>revealed_target_diff_3</th>\n","      <td>889.5</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_diff_7</th>\n","      <td>883.5</td>\n","    </tr>\n","    <tr>\n","      <th>revealed_target_diff_5</th>\n","      <td>830.5</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_diff_7</th>\n","      <td>827.5</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size</th>\n","      <td>816.5</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_5</th>\n","      <td>792.5</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_vix_7</th>\n","      <td>774.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size</th>\n","      <td>769.5</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_std</th>\n","      <td>740.5</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price</th>\n","      <td>722.5</td>\n","    </tr>\n","    <tr>\n","      <th>ask_size</th>\n","      <td>719.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_size</th>\n","      <td>717.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_bid_size_ask_size_imb2</th>\n","      <td>709.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_diff_3</th>\n","      <td>694.5</td>\n","    </tr>\n","    <tr>\n","      <th>wap_vix_5</th>\n","      <td>688.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_diff_5</th>\n","      <td>678.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_vix_7</th>\n","      <td>661.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance</th>\n","      <td>648.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_vix_3</th>\n","      <td>647.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_skew</th>\n","      <td>637.0</td>\n","    </tr>\n","    <tr>\n","      <th>seconds_in_bucket</th>\n","      <td>632.5</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_std</th>\n","      <td>601.0</td>\n","    </tr>\n","    <tr>\n","      <th>volume</th>\n","      <td>595.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_vix_7</th>\n","      <td>591.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_momentum</th>\n","      <td>580.5</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_vix_5</th>\n","      <td>574.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_size_ask_size_imbalance_size_imb2</th>\n","      <td>573.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_wap_imb</th>\n","      <td>571.5</td>\n","    </tr>\n","    <tr>\n","      <th>price_spread</th>\n","      <td>571.0</td>\n","    </tr>\n","    <tr>\n","      <th>spread_intensity</th>\n","      <td>555.5</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_vix_3</th>\n","      <td>552.5</td>\n","    </tr>\n","    <tr>\n","      <th>market_urgency</th>\n","      <td>549.5</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price_wap_imb</th>\n","      <td>548.5</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_wap_imb</th>\n","      <td>541.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_vix_7</th>\n","      <td>536.0</td>\n","    </tr>\n","    <tr>\n","      <th>price_pressure</th>\n","      <td>534.5</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_mean</th>\n","      <td>531.5</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price</th>\n","      <td>520.5</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_wap_reference_price_imb2</th>\n","      <td>509.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_skew</th>\n","      <td>493.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_vix_5</th>\n","      <td>486.5</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_ask_price_imb</th>\n","      <td>456.5</td>\n","    </tr>\n","    <tr>\n","      <th>dow</th>\n","      <td>432.5</td>\n","    </tr>\n","    <tr>\n","      <th>revealed_target_vix_3</th>\n","      <td>423.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_kurt</th>\n","      <td>414.5</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_bid_price_imb</th>\n","      <td>410.5</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price</th>\n","      <td>409.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_vix_5</th>\n","      <td>390.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_bid_price_wap_imb2</th>\n","      <td>363.5</td>\n","    </tr>\n","    <tr>\n","      <th>far_price</th>\n","      <td>354.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_bid_price_imb</th>\n","      <td>327.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_vix_3</th>\n","      <td>311.5</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_vix_3</th>\n","      <td>310.5</td>\n","    </tr>\n","    <tr>\n","      <th>wap</th>\n","      <td>294.0</td>\n","    </tr>\n","    <tr>\n","      <th>near_price</th>\n","      <td>292.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_near_price_imb</th>\n","      <td>276.5</td>\n","    </tr>\n","    <tr>\n","      <th>depth_pressure</th>\n","      <td>252.0</td>\n","    </tr>\n","    <tr>\n","      <th>revealed_target_vix_7</th>\n","      <td>251.5</td>\n","    </tr>\n","    <tr>\n","      <th>revealed_target_vix_5</th>\n","      <td>237.5</td>\n","    </tr>\n","    <tr>\n","      <th>matched_imbalance</th>\n","      <td>222.0</td>\n","    </tr>\n","    <tr>\n","      <th>mid_price</th>\n","      <td>219.5</td>\n","    </tr>\n","    <tr>\n","      <th>seconds</th>\n","      <td>219.5</td>\n","    </tr>\n","    <tr>\n","      <th>near_price_wap_imb</th>\n","      <td>210.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_diff_5</th>\n","      <td>202.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_ask_size_imbalance_size_imb2</th>\n","      <td>195.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_7</th>\n","      <td>189.5</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_diff_7</th>\n","      <td>187.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_diff_3</th>\n","      <td>187.0</td>\n","    </tr>\n","    <tr>\n","      <th>size_imbalance</th>\n","      <td>177.5</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_diff_5</th>\n","      <td>153.0</td>\n","    </tr>\n","    <tr>\n","      <th>near_price_bid_price_imb</th>\n","      <td>140.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_diff_3</th>\n","      <td>138.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_bid_size_imbalance_size_imb2</th>\n","      <td>133.5</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_far_price_imb</th>\n","      <td>121.5</td>\n","    </tr>\n","    <tr>\n","      <th>near_price_ask_price_imb</th>\n","      <td>114.5</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_wap_imb</th>\n","      <td>103.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance</th>\n","      <td>96.0</td>\n","    </tr>\n","    <tr>\n","      <th>liquidity_imbalance</th>\n","      <td>92.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_kurt</th>\n","      <td>86.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_near_price_imb</th>\n","      <td>75.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_ask_price_imb</th>\n","      <td>72.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap</th>\n","      <td>61.5</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_bid_price_imb</th>\n","      <td>59.0</td>\n","    </tr>\n","    <tr>\n","      <th>minute</th>\n","      <td>57.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag</th>\n","      <td>49.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_std_size</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_std_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_median_size</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_median_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_ptp_size</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_ptp_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_wap</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_match_balance</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>stock_id</th>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          importance\n","revealed_target                               1121.5\n","bid_price_wap_reference_price_imb2            1029.5\n","all_sizes_mean                                 994.5\n","ask_price_bid_price_reference_price_imb2       980.5\n","revealed_target_diff_7                         919.5\n","wap_diff_3                                     894.5\n","revealed_target_diff_3                         889.5\n","index_mean_wap_diff_7                          883.5\n","revealed_target_diff_5                         830.5\n","match_balance_diff_7                           827.5\n","matched_size                                   816.5\n","wap_diff_5                                     792.5\n","index_mean_wap_vix_7                           774.0\n","imbalance_size                                 769.5\n","all_sizes_std                                  740.5\n","reference_price                                722.5\n","ask_size                                       719.0\n","bid_size                                       717.0\n","matched_size_bid_size_ask_size_imb2            709.0\n","match_balance_diff_3                           694.5\n","wap_vix_5                                      688.0\n","index_mean_match_balance_diff_5                678.0\n","index_mean_match_balance_vix_7                 661.0\n","match_balance                                  648.0\n","index_mean_wap_vix_3                           647.0\n","all_prices_skew                                637.0\n","seconds_in_bucket                              632.5\n","all_prices_std                                 601.0\n","volume                                         595.0\n","wap_vix_7                                      591.0\n","imbalance_momentum                             580.5\n","index_mean_match_balance_vix_5                 574.0\n","bid_size_ask_size_imbalance_size_imb2          573.0\n","reference_price_wap_imb                        571.5\n","price_spread                                   571.0\n","spread_intensity                               555.5\n","index_mean_match_balance_vix_3                 552.5\n","market_urgency                                 549.5\n","bid_price_wap_imb                              548.5\n","ask_price_wap_imb                              541.0\n","match_balance_vix_7                            536.0\n","price_pressure                                 534.5\n","all_prices_mean                                531.5\n","bid_price                                      520.5\n","ask_price_wap_reference_price_imb2             509.0\n","all_sizes_skew                                 493.0\n","index_mean_wap_vix_5                           486.5\n","reference_price_ask_price_imb                  456.5\n","dow                                            432.5\n","revealed_target_vix_3                          423.0\n","all_prices_kurt                                414.5\n","reference_price_bid_price_imb                  410.5\n","ask_price                                      409.0\n","match_balance_vix_5                            390.0\n","ask_price_bid_price_wap_imb2                   363.5\n","far_price                                      354.0\n","ask_price_bid_price_imb                        327.0\n","wap_vix_3                                      311.5\n","match_balance_vix_3                            310.5\n","wap                                            294.0\n","near_price                                     292.0\n","far_price_near_price_imb                       276.5\n","depth_pressure                                 252.0\n","revealed_target_vix_7                          251.5\n","revealed_target_vix_5                          237.5\n","matched_imbalance                              222.0\n","mid_price                                      219.5\n","seconds                                        219.5\n","near_price_wap_imb                             210.0\n","index_mean_wap_diff_5                          202.0\n","matched_size_ask_size_imbalance_size_imb2      195.0\n","wap_diff_7                                     189.5\n","index_mean_match_balance_diff_7                187.0\n","index_mean_wap_diff_3                          187.0\n","size_imbalance                                 177.5\n","match_balance_diff_5                           153.0\n","near_price_bid_price_imb                       140.0\n","index_mean_match_balance_diff_3                138.0\n","matched_size_bid_size_imbalance_size_imb2      133.5\n","reference_price_far_price_imb                  121.5\n","near_price_ask_price_imb                       114.5\n","far_price_wap_imb                              103.0\n","index_mean_match_balance                        96.0\n","liquidity_imbalance                             92.0\n","all_sizes_kurt                                  86.0\n","reference_price_near_price_imb                  75.0\n","far_price_ask_price_imb                         72.0\n","index_mean_wap                                  61.5\n","far_price_bid_price_imb                         59.0\n","minute                                          57.0\n","imbalance_buy_sell_flag                         49.0\n","global_std_size                                  0.0\n","global_std_price                                 0.0\n","global_median_size                               0.0\n","global_median_price                              0.0\n","global_ptp_size                                  0.0\n","global_ptp_price                                 0.0\n","index_std_wap                                    0.0\n","index_std_match_balance                          0.0\n","stock_id                                         0.0"]},"execution_count":150,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize an empty DataFrame for aggregated importances\n","aggregated_importance = pd.DataFrame(index=features, columns=['importance'])\n","\n","# Aggregate the importances from each model\n","for key, i_models in model_dict.items():\n","    for model in i_models:\n","        importance = pd.DataFrame({'feature': features, 'importance': model.feature_importance})\n","        aggregated_importance = aggregated_importance.add(importance.set_index('feature'), fill_value=0)\n","\n","aggregated_importance['importance'] /= len(df_model)\n","\n","pd_display_max()\n","# Sort the features by importance\n","aggregated_importance = aggregated_importance.sort_values(by='importance', ascending=False)\n","aggregated_importance"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset types"]},{"cell_type":"code","execution_count":151,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["stock_id                                        int8\n","seconds_in_bucket                              int16\n","imbalance_size                               float32\n","imbalance_buy_sell_flag                         int8\n","reference_price                              float32\n","matched_size                                 float32\n","far_price                                    float32\n","near_price                                   float32\n","bid_price                                    float32\n","bid_size                                     float32\n","ask_price                                    float32\n","ask_size                                     float32\n","wap                                          float32\n","market_urgency                               float32\n","price_spread                                 float32\n","matched_size_bid_size_ask_size_imb2          float32\n","global_ptp_size                              float32\n","near_price_wap_imb                           float32\n","imbalance_momentum                           float32\n","seconds                                         int8\n","near_price_bid_price_imb                     float32\n","all_prices_mean                              float32\n","price_pressure                               float32\n","all_sizes_skew                               float32\n","all_prices_skew                              float32\n","match_balance                                float32\n","bid_price_wap_imb                            float32\n","matched_imbalance                            float32\n","mid_price                                    float32\n","bid_size_ask_size_imbalance_size_imb2        float32\n","reference_price_bid_price_imb                float32\n","ask_price_wap_imb                            float32\n","dow                                             int8\n","global_ptp_price                             float32\n","ask_price_bid_price_reference_price_imb2     float32\n","global_median_price                          float32\n","reference_price_far_price_imb                float32\n","far_price_near_price_imb                     float32\n","matched_size_ask_size_imbalance_size_imb2    float32\n","spread_intensity                             float32\n","all_sizes_kurt                               float32\n","ask_price_bid_price_imb                      float32\n","all_sizes_std                                float32\n","minute                                          int8\n","far_price_wap_imb                            float32\n","reference_price_wap_imb                      float32\n","reference_price_near_price_imb               float32\n","all_prices_kurt                              float32\n","ask_price_wap_reference_price_imb2           float32\n","all_prices_std                               float32\n","ask_price_bid_price_wap_imb2                 float32\n","near_price_ask_price_imb                     float32\n","size_imbalance                               float32\n","global_median_size                           float32\n","global_std_size                              float32\n","depth_pressure                               float32\n","far_price_ask_price_imb                      float32\n","all_sizes_mean                               float32\n","global_std_price                             float32\n","volume                                       float32\n","bid_price_wap_reference_price_imb2           float32\n","reference_price_ask_price_imb                float32\n","liquidity_imbalance                          float32\n","matched_size_bid_size_imbalance_size_imb2    float32\n","far_price_bid_price_imb                      float32\n","index_mean_wap_vix_7                         float32\n","index_std_wap                                float32\n","index_mean_match_balance_vix_5               float32\n","wap_diff_3                                   float32\n","wap_diff_5                                   float32\n","index_mean_match_balance_diff_5              float32\n","revealed_target                              float32\n","index_std_match_balance                      float32\n","index_mean_match_balance_vix_7               float32\n","index_mean_match_balance                     float32\n","revealed_target_vix_5                        float32\n","index_mean_wap_diff_3                        float32\n","revealed_target_diff_7                       float32\n","match_balance_diff_3                         float32\n","revealed_target_vix_7                        float32\n","index_mean_wap_diff_5                        float32\n","wap_vix_5                                    float32\n","match_balance_vix_7                          float32\n","match_balance_diff_5                         float32\n","match_balance_diff_7                         float32\n","index_mean_match_balance_diff_3              float32\n","index_mean_match_balance_diff_7              float32\n","revealed_target_diff_3                       float32\n","wap_vix_7                                    float32\n","index_mean_wap                               float32\n","index_mean_wap_diff_7                        float32\n","index_mean_wap_vix_3                         float32\n","revealed_target_vix_3                        float32\n","index_mean_match_balance_vix_3               float32\n","index_mean_wap_vix_5                         float32\n","revealed_target_diff_5                       float32\n","match_balance_vix_3                          float32\n","wap_vix_3                                    float32\n","wap_diff_7                                   float32\n","match_balance_vix_5                          float32\n","dtype: object"]},"execution_count":151,"metadata":{},"output_type":"execute_result"}],"source":["features_types = df_train[features].dtypes\n","features_types"]},{"cell_type":"code","execution_count":152,"metadata":{"trusted":true},"outputs":[],"source":["def convert_dtypes(df):\n","    df_types = df[features].dtypes\n","    different_types = [col for col in df_types.index if col in features_types and df_types[col] != features_types[col]]\n","    print(f\"Different Types: {different_types}\")\n","    return different_types\n","\n","def update_dtypes_by_origin(df):\n","    diff_types = convert_dtypes(df)\n","    for col in diff_types:\n","        df[col] = df[col].astype(features_types[col])\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["# Clear trains"]},{"cell_type":"code","execution_count":153,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["RAM memory GB usage = 1.743\n"]}],"source":["# Clean up\n","pd_clear_display_max()\n","del key_models\n","if IS_USE_SAVED_MODEL:\n","    print(\"Delete model_dict\")\n","    del model_dict\n","del df_train\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Infer"]},{"cell_type":"code","execution_count":154,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 9 µs, sys: 1e+03 ns, total: 10 µs\n","Wall time: 11 µs\n"]}],"source":["%%time\n","\n","y_min, y_max = -64, 64\n","\n","# 📉 Define a function to adjust prices based on volumes\n","def zero_sum(prices, volumes):\n","    std_error = np.sqrt(volumes)  # 🧮 Calculate standard error based on volumes\n","    step = np.sum(prices) / np.sum(std_error)  # 🧮 Calculate the step size based on prices and standard error\n","    out = prices - std_error * step  # 💰 Adjust prices by subtracting the standardized step size\n","    return out\n","\n","def zero_clip(df, predictions):\n","    # Adjust the predictions based on the order book imbalance\n","    zerosum_predictions = zero_sum(predictions, df['bid_size'] + df['ask_size'])\n","    clipped_predictions = np.clip(zerosum_predictions, y_min, y_max)\n","    clipped_predictions.replace([np.nan, np.inf, -np.inf], 0, inplace=True)\n","    clipped_predictions = clipped_predictions.astype('float64').values  \n","    return clipped_predictions\n","\n","def model_infer(key, df_feat, additional_infer=False):\n","    def predictor(boosters):\n","        #print(f\"Predictor Feat len {len(df_feat)}\")\n","        if USE_OPTUNA:\n","            predictions_list = [np.mean(booster.predict(df_feat), 0) for booster in boosters]\n","        else:\n","            predictions_list = [booster.predict(df_feat) for booster in boosters]\n","        predictions = np.mean(predictions_list, 0)\n","        std_predictions = np.std(predictions_list, 0)\n","        #print(\"std_predictions\", std_predictions)\n","        return predictions\n","    \n","    if IS_USE_SAVED_MODEL:\n","        model_paths = model_dict_saved[key]\n","        models = [lgb.Booster(model_file=model_path) for model_path in model_paths]\n","        predictions = predictor(models)\n","        del models\n","    else:\n","        if additional_infer:\n","            print(\"Use additional model\")\n","            boosters = [m.booster for m in additional_model_dict[key]]\n","            print(f\"Additional predictor target models len {len(boosters)}\")\n","        else:\n","            boosters = [m.booster for m in model_dict[key]]\n","            print(f\"Predictor target models len {len(boosters)}\")\n","        predictions = predictor(boosters)\n","    collect()\n","    return predictions"]},{"cell_type":"code","execution_count":155,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Infer Local\n","------- counter 1 start -------\n","copy_revealed_targets len 11000\n","Update revealed_targets\n","MIN LEARN MODE : [0]\n","df_cache len 1\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 0.0\n","df_cache_with_features len 1\n","Predictor target models len 2\n","prediction average -0.3216656944684691\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -2.556653484475646e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 1, execution_time 0.9878432750701904 end -------\n","------- counter 2 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 2\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 1.0\n","df_cache_with_features len 2\n","Predictor target models len 2\n","prediction average -0.580331506924344\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -2.556653484475646e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 2, execution_time 0.5546820163726807 end -------\n","------- counter 3 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 3\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 2.0\n","df_cache_with_features len 3\n","Predictor target models len 2\n","prediction average -0.7058621151263421\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -2.556653484475646e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 3, execution_time 0.5467538833618164 end -------\n","------- counter 4 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 4\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 3.0\n","df_cache_with_features len 4\n","Predictor target models len 2\n","prediction average -0.6060066043356924\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.1212019472850443e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 4, execution_time 0.552170991897583 end -------\n","------- counter 5 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 5\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 4.0\n","df_cache_with_features len 5\n","Predictor target models len 2\n","prediction average -0.37179318377849196\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -7.861466855274557e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 5, execution_time 0.5587339401245117 end -------\n","------- counter 6 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 6\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 5.0\n","df_cache_with_features len 6\n","Predictor target models len 2\n","prediction average -0.11882353847350918\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -7.861466855274557e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 6, execution_time 0.5512678623199463 end -------\n","------- counter 7 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 7\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 6.0\n","df_cache_with_features len 7\n","Predictor target models len 2\n","prediction average -0.3656419532855128\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -2.556653484475646e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 7, execution_time 0.5496630668640137 end -------\n","------- counter 8 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 8\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 7.0\n","df_cache_with_features len 8\n","Predictor target models len 2\n","prediction average -0.1271565987905076\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.5207049983700927e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 8, execution_time 0.5608551502227783 end -------\n","------- counter 9 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 9\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 8.0\n","df_cache_with_features len 9\n","Predictor target models len 2\n","prediction average -0.14481054085980322\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -3.8664363444240735e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 9, execution_time 0.5513749122619629 end -------\n","------- counter 10 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 9.0\n","df_cache_with_features len 10\n","Predictor target models len 2\n","prediction average -0.24995226951163996\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.5207049983700927e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 10, execution_time 0.5624198913574219 end -------\n","------- counter 11 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 10.0\n","df_cache_with_features len 11\n","Predictor target models len 2\n","prediction average 0.2600991646532702\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4383770263748374e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 11, execution_time 0.5680818557739258 end -------\n","------- counter 12 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 11.0\n","df_cache_with_features len 12\n","Predictor target models len 2\n","prediction average 0.13457754480618767\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -3.8664363444240735e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 12, execution_time 0.5492630004882812 end -------\n","------- counter 13 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 12.0\n","df_cache_with_features len 13\n","Predictor target models len 2\n","prediction average 0.20315656100177076\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.6516832843649354e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 13, execution_time 0.5658090114593506 end -------\n","------- counter 14 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 13.0\n","df_cache_with_features len 14\n","Predictor target models len 2\n","prediction average 0.07110723303788798\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -3.8664363444240735e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 14, execution_time 0.5610589981079102 end -------\n","------- counter 15 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 14.0\n","df_cache_with_features len 15\n","Predictor target models len 2\n","prediction average 0.06733895586206377\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.5207049983700927e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 15, execution_time 0.554189920425415 end -------\n","------- counter 16 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 15.0\n","df_cache_with_features len 16\n","Predictor target models len 2\n","prediction average -0.03225291794581299\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4383770263748374e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 16, execution_time 0.5597438812255859 end -------\n","------- counter 17 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 16.0\n","df_cache_with_features len 17\n","Predictor target models len 2\n","prediction average 0.0698736946433336\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.1212019472850443e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 17, execution_time 0.5633358955383301 end -------\n","------- counter 18 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 17.0\n","df_cache_with_features len 18\n","Predictor target models len 2\n","prediction average -0.28350939914452034\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -7.861466855274557e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 18, execution_time 0.5607707500457764 end -------\n","------- counter 19 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 18.0\n","df_cache_with_features len 19\n","Predictor target models len 2\n","prediction average -0.2869636859587864\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.478213924130614e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 19, execution_time 0.553955078125 end -------\n","------- counter 20 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 19.0\n","df_cache_with_features len 20\n","Predictor target models len 2\n","prediction average -0.027697736992555955\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4383770263748374e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 20, execution_time 0.555743932723999 end -------\n","------- counter 21 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 20.0\n","df_cache_with_features len 21\n","Predictor target models len 2\n","prediction average -0.21074907374060223\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4383770263748374e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 21, execution_time 0.5519907474517822 end -------\n","------- counter 22 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 21.0\n","df_cache_with_features len 22\n","Predictor target models len 2\n","prediction average -0.7622328057689893\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4383770263748374e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 22, execution_time 0.5599029064178467 end -------\n","------- counter 23 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 22.0\n","df_cache_with_features len 23\n","Predictor target models len 2\n","prediction average -1.180064696276862\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -3.8664363444240735e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 23, execution_time 0.5511398315429688 end -------\n","------- counter 24 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 23.0\n","df_cache_with_features len 24\n","Predictor target models len 2\n","prediction average -1.4009256807315806\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.5207049983700927e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 24, execution_time 0.5613269805908203 end -------\n","------- counter 25 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 24.0\n","df_cache_with_features len 25\n","Predictor target models len 2\n","prediction average -0.3208811493278173\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.478213924130614e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 25, execution_time 0.5555977821350098 end -------\n","------- counter 26 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 25.0\n","df_cache_with_features len 26\n","Predictor target models len 2\n","prediction average -0.09453252383638655\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4383770263748374e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 26, execution_time 0.5753939151763916 end -------\n","------- counter 27 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 26.0\n","df_cache_with_features len 27\n","Predictor target models len 2\n","prediction average -0.698905626921862\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4383770263748374e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 27, execution_time 0.5688719749450684 end -------\n","------- counter 28 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 27.0\n","df_cache_with_features len 28\n","Predictor target models len 2\n","prediction average -0.49388593687674776\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.478213924130614e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 28, execution_time 0.5607340335845947 end -------\n","------- counter 29 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 28.0\n","df_cache_with_features len 29\n","Predictor target models len 2\n","prediction average -0.2596020695645365\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -3.8664363444240735e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 29, execution_time 0.6198878288269043 end -------\n","------- counter 30 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.01 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 29.0\n","df_cache_with_features len 30\n","Predictor target models len 2\n","prediction average -0.8640072440276717\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -1.4237644521131188e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 30, execution_time 0.5772559642791748 end -------\n","------- counter 31 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.01 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 30.0\n","df_cache_with_features len 31\n","Predictor target models len 2\n","prediction average -0.2709783315160856\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -1.4237644521131188e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 31, execution_time 0.5874378681182861 end -------\n","------- counter 32 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 31.0\n","df_cache_with_features len 32\n","Predictor target models len 2\n","prediction average -0.20084004551832005\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.861440494607905e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 32, execution_time 0.5601089000701904 end -------\n","------- counter 33 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 32.0\n","df_cache_with_features len 33\n","Predictor target models len 2\n","prediction average 0.7606687082818983\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 33, execution_time 0.6033918857574463 end -------\n","------- counter 34 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 33.0\n","df_cache_with_features len 34\n","Predictor target models len 2\n","prediction average 0.4107919008384208\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 34, execution_time 0.6114797592163086 end -------\n","------- counter 35 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 34.0\n","df_cache_with_features len 35\n","Predictor target models len 2\n","prediction average 0.20082817242777853\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.861440494607905e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 35, execution_time 0.566370964050293 end -------\n","------- counter 36 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 35.0\n","df_cache_with_features len 36\n","Predictor target models len 2\n","prediction average -0.1549308645459164\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.861440494607905e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 36, execution_time 0.571990966796875 end -------\n","------- counter 37 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 36.0\n","df_cache_with_features len 37\n","Predictor target models len 2\n","prediction average -0.45998186506589833\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 37, execution_time 0.567054271697998 end -------\n","------- counter 38 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 37.0\n","df_cache_with_features len 38\n","Predictor target models len 2\n","prediction average -0.3361178174833082\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 38, execution_time 0.5738377571105957 end -------\n","------- counter 39 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 38.0\n","df_cache_with_features len 39\n","Predictor target models len 2\n","prediction average -0.5981858006021759\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 39, execution_time 0.5595588684082031 end -------\n","------- counter 40 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 39.0\n","df_cache_with_features len 40\n","Predictor target models len 2\n","prediction average -0.7460009729870287\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 40, execution_time 0.5828192234039307 end -------\n","------- counter 41 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 40.0\n","df_cache_with_features len 41\n","Predictor target models len 2\n","prediction average -0.41033495872822723\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 41, execution_time 0.5800940990447998 end -------\n","------- counter 42 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 41.0\n","df_cache_with_features len 42\n","Predictor target models len 2\n","prediction average -0.3488877011326862\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -1.4212045833517095e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 42, execution_time 0.6258189678192139 end -------\n","------- counter 43 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.01 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 42.0\n","df_cache_with_features len 43\n","Predictor target models len 2\n","prediction average -0.4516598294780587\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.861440494607905e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 43, execution_time 0.6248619556427002 end -------\n","------- counter 44 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 43.0\n","df_cache_with_features len 44\n","Predictor target models len 2\n","prediction average 0.04889375477201461\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.861440494607905e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 44, execution_time 0.5797080993652344 end -------\n","------- counter 45 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 44.0\n","df_cache_with_features len 45\n","Predictor target models len 2\n","prediction average 0.20781131169935046\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 45, execution_time 0.5653619766235352 end -------\n","------- counter 46 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 45.0\n","df_cache_with_features len 46\n","Predictor target models len 2\n","prediction average 0.3841725021464101\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 46, execution_time 0.5819408893585205 end -------\n","------- counter 47 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 46.0\n","df_cache_with_features len 47\n","Predictor target models len 2\n","prediction average 0.4425335710278968\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 47, execution_time 0.5492291450500488 end -------\n","------- counter 48 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 47.0\n","df_cache_with_features len 48\n","Predictor target models len 2\n","prediction average 0.43764288009566843\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 48, execution_time 0.5664889812469482 end -------\n","------- counter 49 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 48.0\n","df_cache_with_features len 49\n","Predictor target models len 2\n","prediction average -0.1859780565719355\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 1.4639757495160666e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 49, execution_time 0.5567469596862793 end -------\n","------- counter 50 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 49.0\n","df_cache_with_features len 50\n","Predictor target models len 2\n","prediction average -1.1868690568228026\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.861440494607905e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 50, execution_time 0.5496048927307129 end -------\n","------- counter 51 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 50.0\n","df_cache_with_features len 51\n","Predictor target models len 2\n","prediction average -0.3099632244756287\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -1.4212045833517095e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 51, execution_time 0.5540368556976318 end -------\n","------- counter 52 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 51.0\n","df_cache_with_features len 52\n","Predictor target models len 2\n","prediction average -0.5601295558586398\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.861440494607905e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 52, execution_time 0.5716948509216309 end -------\n","------- counter 53 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 52.0\n","df_cache_with_features len 53\n","Predictor target models len 2\n","prediction average -0.3382928781855268\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.861440494607905e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 53, execution_time 0.5601811408996582 end -------\n","------- counter 54 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 53.0\n","df_cache_with_features len 54\n","Predictor target models len 2\n","prediction average 0.015112325847813815\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -1.4212045833517095e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 54, execution_time 0.5731630325317383 end -------\n","------- counter 55 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 54.0\n","df_cache_with_features len 55\n","Update global train cache\n","update_global_train_cache\n","Updated global_train_cache, len:  165\n","165\n","     date_id  seconds_in_bucket  stock_id  origin  revealed_target    target\n","0        478                  0         0       1        -2.310276 -5.429983\n","1        478                 10         0       1        -7.200241 -3.489852\n","2        478                 20         0       1        -7.500052 -3.330112\n","3        478                 30         0       1       -12.480021 -1.900196\n","4        478                 40         0       1        -9.570122  2.189875\n","..       ...                ...       ...     ...              ...       ...\n","160      480                500         0       0         1.579523  3.999472\n","161      480                510         0       0        -0.180006  3.190041\n","162      480                520         0       0         2.199411 -0.169873\n","163      480                530         0       0         1.929998  3.110170\n","164      480                540         0       0         3.889799  0.760555\n","\n","[165 rows x 6 columns]\n","Predictor target models len 2\n","prediction average -0.2742992841707298\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -1.7609510578608933e-06\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 55, execution_time 0.7048208713531494 end -------\n","------- counter 56 start -------\n","copy_revealed_targets len 11000\n","Update revealed_targets\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 479,  seconds_in_bucket: 0.0\n","df_cache_with_features len 56\n","Predictor target models len 2\n","prediction average 0.7147348191456369\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -2.556653484475646e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 56, execution_time 0.5853569507598877 end -------\n","------- counter 57 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 479,  seconds_in_bucket: 1.0\n","df_cache_with_features len 57\n","Predictor target models len 2\n","prediction average -0.90846372772666\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -2.556653484475646e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 57, execution_time 0.5620279312133789 end -------\n","------- counter 58 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 479,  seconds_in_bucket: 2.0\n","df_cache_with_features len 58\n","Predictor target models len 2\n","prediction average -1.2036298576155193\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average -2.556653484475646e-07\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 58, execution_time 0.5482680797576904 end -------\n","------- counter 59 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 479,  seconds_in_bucket: 3.0\n","df_cache_with_features len 59\n","Predictor target models len 2\n","prediction average -0.34752368504520836\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.831834132801305e-08\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 59, execution_time 0.568925142288208 end -------\n","------- counter 60 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 479,  seconds_in_bucket: 4.0\n","df_cache_with_features len 60\n","Predictor target models len 2\n","prediction average 0.023162181750371186\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 2\n","additional prediction average 4.831834132801305e-08\n","Submit dummy prediction\n","RAM memory GB usage = 1.762\n","------- counter 60, execution_time 0.5749688148498535 end -------\n","------- counter 61 start -------\n","copy_revealed_targets len 0\n","MIN LEARN MODE : [0]\n","df_cache len 10\n","generate_enhance_features\n","Use index\n","generate_index_features 0.00 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n"]}],"source":["%%time\n","\n","predictions = []\n","df_cache = pd.DataFrame()\n","df_cache_with_features = pd.DataFrame()\n","df_result = pd.DataFrame()\n","\n","df_revealed_targets = pd.DataFrame()\n","\n","if IS_INFER:\n","    if IS_LOCAL or IS_DEBUG:\n","        print(\"Infer Local\")\n","        env = make_env()\n","    else:\n","        print(\"Infer Submission\")\n","        import optiver2023\n","        env = optiver2023.make_env()\n","    iter_test = env.iter_test()\n","    counter = 1\n","\n","    try:\n","        for (test, revealed_targets, sample_prediction) in iter_test:\n","            now_time = time.time()\n","            print(f\"------- counter {counter} start -------\")\n","\n","            # Add revealed target as target for counituous update\n","            copy_revealed_targets = revealed_targets.copy()\n","            copy_revealed_targets = copy_revealed_targets.dropna()\n","            print(\"copy_revealed_targets len\", len(copy_revealed_targets))\n","\n","            if len(copy_revealed_targets) > 0:\n","                print(\"Update revealed_targets\")\n","                copy_revealed_targets['revealed_date_id'] = copy_revealed_targets['revealed_date_id'].astype(int).astype(str)\n","                copy_revealed_targets['date_id'] = copy_revealed_targets['date_id'].astype(int).astype(str)\n","                copy_revealed_targets['seconds_in_bucket'] = copy_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n","                copy_revealed_targets['stock_id'] = copy_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n","                copy_revealed_targets['revealed_row_id'] = copy_revealed_targets['revealed_date_id'] + '_' + copy_revealed_targets['seconds_in_bucket'] + '_' + copy_revealed_targets['stock_id']\n","                copy_revealed_targets['row_id'] = copy_revealed_targets['date_id'] + '_' + copy_revealed_targets['seconds_in_bucket'] + '_' + copy_revealed_targets['stock_id']\n","                copy_revealed_targets['revealed_target'] = copy_revealed_targets['revealed_target'].astype('float32')\n","\n","                df_revealed_targets = pd.concat([df_revealed_targets, copy_revealed_targets], ignore_index=True, axis=0)\n","                df_revealed_targets = df_revealed_targets.groupby(['stock_id']).tail(DATA_COUNT_IN_SAME_BUCKET * 2)\n","                df_revealed_targets = reduce_mem_usage(df_revealed_targets, 'df_revealed_targets')\n","                \n","            df_cache = pd.concat([df_cache, test], ignore_index=True, axis=0)\n","\n","            if IS_MIN_LEARN:\n","                print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n","                df_cache = df_cache[df_cache[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","\n","            if counter > 0:\n","                # Clear cache data, tailはhistoricalで作成な分のみ残す\n","                df_cache = df_cache.groupby(['stock_id']).tail(10)\n","                df_cache = default_sort(df_cache)\n","                print(f\"df_cache len {len(df_cache)}\")\n","\n","            # USE_REVEALED_TARGETSが有効の時、cacheのrow_idとdf_revealed_targetsのrow_idをleft joinする\n","            if USE_REVEALED_TARGETS:\n","                df_r = df_revealed_targets[['row_id', 'revealed_target']]\n","                df_cache = pd.merge(df_cache, df_r, how='left', on='row_id')\n","                df_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'revealed_target']]\n","\n","            # Generate features\n","            df_valid = df_cache.copy()\n","            df_valid = generate_basic_features(df_valid)\n","            df_valid = generate_enhance_features(df_valid)\n","            df_valid = reduce_mem_usage(df_valid, 'df_valid')\n","\n","            # testの分のみの長さを抽出\n","            if IS_MIN_LEARN:\n","                df_valid = df_valid[-len(TARGET_STOCK_IDS):].reset_index(drop=True)\n","            else:\n","                df_valid = df_valid[-len(test):].reset_index(drop=True)\n","\n","            df_cache_with_features = pd.concat([df_cache_with_features, df_valid], ignore_index=True, axis=0)\n","\n","            # It faults due to test is iterator\n","            #seconds_in_bucket = test['seconds_in_bucket'][0]\n","            #print(f\"prdict: {test['date_id'][0]}, {seconds_in_bucket}\")\n","\n","            seconds_in_bucket = df_valid['seconds_in_bucket'][0] / 10\n","            date_id = df_valid['date_id'][0]\n","            print(f\"date_id: {date_id},  seconds_in_bucket: {seconds_in_bucket}\")\n","\n","            if counter > 0:\n","                # Clear cache data, tailはhistoricalで作成な分のみ残す\n","                df_cache_with_features = df_cache_with_features.groupby(['stock_id']).tail(DATA_COUNT_IN_SAME_BUCKET * 2)\n","                df_cache_with_features = default_sort(df_cache_with_features)\n","                print(f\"df_cache_with_features len {len(df_cache_with_features)}\")\n","\n","            # Update global train cache\n","            if USE_CONTINUOUS_UPDATE  and (counter % DATA_COUNT_IN_SAME_BUCKET == 0):\n","                print(\"Update global train cache\")\n","                df_r = df_revealed_targets[['revealed_row_id', 'revealed_target']]\n","                df_r.rename(columns={'revealed_target': 'target'}, inplace=True)\n","                df_r.rename(columns={'revealed_row_id': 'row_id'}, inplace=True)\n","                df_update = pd.merge(df_cache_with_features, df_r, how='left', on='row_id')\n","                update_global_train_cache(df_update, 2)\n","\n","            # Update model\n","            if (counter % (DATA_COUNT_IN_SAME_BUCKET * continuos_train_span) == 0) and USE_CONTINUOUS_UPDATE:\n","                print(\"Update model with revealed_target date start\")\n","                train_now_time = time.time()\n","                model_dict[KEY] = keep_train_models(global_train_cache, model_dict[KEY], features, \"target\", True)\n","                print(f\"ReTrain Time: {time.time() - train_now_time}\")\n","\n","            # Predict\n","            predictions = model_infer(KEY, df_valid[features])\n","            scaled_predictions = zero_clip(df_valid, predictions)\n","\n","            df_valid['base_pred'] = predictions\n","            df_valid['base_scaled_pred'] = scaled_predictions\n","            print(\"prediction average\", np.mean(predictions))\n","\n","            if USE_ADDITIONAL_TRAIN:\n","                print(\"Additional prediction\")\n","                additional_predictions = model_infer(KEY, df_valid[additional_features], additional_infer=True)\n","                additional_scaled_predictions = zero_clip(df_valid, additional_predictions)\n","                df_valid['additional_pred'] = additional_predictions\n","                df_valid['additional_scaled_pred'] = additional_scaled_predictions\n","                print(\"additional prediction average\", np.mean(additional_scaled_predictions))\n","\n","                predictions = (predictions + additional_predictions) / 2\n","                df_valid['average_pred'] = predictions\n","                scaled_predictions = zero_clip(df_valid, predictions)\n","                df_valid['average_scaled_pred'] = scaled_predictions\n","\n","            # For save\n","            if IS_DEBUG:\n","                df_result = pd.concat([df_result, df_valid], ignore_index=True, axis=0)\n","\n","            # Submit\n","            if not IS_MIN_LEARN:\n","                print(\"Submit prediction\")\n","                sample_prediction['target'] = scaled_predictions\n","                env.predict(sample_prediction)\n","            else:\n","                print(\"Submit dummy prediction\")\n","                sample_prediction['target'] = 0\n","                env.predict(sample_prediction)\n","\n","            # Clean up\n","            execution_time = time.time() - now_time\n","            df_cache = df_cache.drop('revealed_target', axis=1)\n","            del df_valid\n","            collect()\n","            print(GetMemUsage())\n","            print(f\"------- counter {counter}, execution_time {execution_time} end -------\")\n","            counter += 1\n","    except Exception as e:\n","        print(\"Error\", e)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if IS_DEBUG:\n","    df_cache_with_features.to_csv(f\"{BASE_OUTPUT_PATH}/df_cache_with_features.csv\", index=False)\n","    df_result.to_csv(f\"{BASE_OUTPUT_PATH}/result.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["score\n","            score\n","count  110.000000\n","mean     3.416614\n","std      2.687239\n","min      0.009537\n","25%      1.224875\n","50%      2.739727\n","75%      5.224943\n","max     11.439919\n","additional_pred\n","       additional_score  average_score\n","count        110.000000     110.000000\n","mean           3.416614       3.416614\n","std            2.687239       2.687239\n","min            0.009537       0.009537\n","25%            1.224875       1.224876\n","50%            2.739727       2.739728\n","75%            5.224943       5.224943\n","max           11.439919      11.439919\n"]}],"source":["if IS_DEBUG:\n","    df_revealed_targets = pd.read_csv(REVEALED_TARGETS_FILE)\n","    df_revealed_targets = df_revealed_targets.dropna(subset=['revealed_date_id', 'seconds_in_bucket', 'stock_id'])\n","    df_revealed_targets['revealed_date_id'] = df_revealed_targets['revealed_date_id'].astype(int).astype(str)\n","    df_revealed_targets['seconds_in_bucket'] = df_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n","    df_revealed_targets['stock_id'] = df_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n","\n","    # Concatenate the columns\n","    df_revealed_targets['row_id'] = df_revealed_targets['revealed_date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n","    \n","    df_pred = df_result[['row_id', 'base_scaled_pred']]\n","    df = pd.merge(df_pred, df_revealed_targets, how='left', on='row_id')\n","\n","    df = df.rename(columns={'revealed_target': 'target'})\n","    df = df.dropna(subset=['target'])\n","    df['score'] = (df['base_scaled_pred'] - df['target']).abs()\n","\n","    df = df[['score']]\n","    print(\"score\")\n","    print(df.describe())\n","\n","    if USE_ADDITIONAL_TRAIN:\n","        df_pred = df_result[['row_id', 'additional_scaled_pred', 'average_scaled_pred']]\n","        df = pd.merge(df_pred, df_revealed_targets, how='left', on='row_id')\n","\n","        df = df.rename(columns={'revealed_target': 'target'})\n","        df = df.dropna(subset=['target'])\n","        df['additional_score'] = (df['additional_scaled_pred'] - df['target']).abs()\n","        df['average_score'] = (df['average_scaled_pred'] - df['target']).abs()\n","\n","        #df['score'] = mean_absolute_error(df['scaled_pred'], df['target'])\n","        print(\"additional_pred\")\n","        df = df[['additional_score', 'average_score']]\n","        print(df.describe())\n","\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7056235,"sourceId":57891,"sourceType":"competition"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
