{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Incremental Learning Model\n","This origin comming from https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model/notebook"]},{"cell_type":"markdown","metadata":{},"source":["# Global params"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:28:43.280641Z","iopub.status.busy":"2023-11-23T00:28:43.280357Z","iopub.status.idle":"2023-11-23T00:28:59.446106Z","shell.execute_reply":"2023-11-23T00:28:59.445172Z","shell.execute_reply.started":"2023-11-23T00:28:43.280616Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BASE_OUTPUT_PATH: ../output\n","BASE_INPUT_PATH: ../kaggle/input/optiver-trading-at-the-close\n","TRAIN_FILE: ../kaggle/input/optiver-trading-at-the-close/train.csv\n","TEST_FILE: ../kaggle/input/optiver-trading-at-the-close/test.csv\n","IS_LOCAL: True\n","IS_INFER: True\n","IS_USE_SAVED_MODEL: False\n","IS_MIN_LEARN: True\n","USE_OPTUNA: False\n","USE_CONTINUOUS_UPDATE: True\n","USE_ALL_FEATUTES: True\n","USE_REVEALED_TARGETS: False\n","USE_INDEX: True\n"]}],"source":["from pathlib import Path\n","import os\n","import warnings\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import gc  # Garbage collection for memory management\n","import os  # Operating system-related functions\n","import time  # Time-related functions\n","import warnings  # Handling warnings\n","from itertools import combinations  # For creating combinations of elements\n","from warnings import simplefilter  # Simplifying warning handling\n","import joblib  # For saving and loading models\n","import numpy as np  # Numerical operations\n","import pandas as pd  # Data manipulation and analysis\n","from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n","from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n","\n","# Disable warnings to keep the code clean\n","warnings.filterwarnings(\"ignore\")\n","simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","seed = 2023\n","DATA_COUNT_IN_SAME_BUCKET = 55 # 同じbucket内のデータ数\n","\n","# valid_split_day = 435  # Split day for time series data\n","\n","# For kaggle environment\n","if os.environ.get(\"KAGGLE_DATA_PROXY_TOKEN\") != None:\n","    BASE_OUTPUT_PATH = Path(f'/kaggle/working')\n","    BASE_INPUT_PATH = Path(f'/kaggle/input/optiver-trading-at-the-close')\n","    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n","    TEST_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/test.csv')\n","    \n","    IS_LOCAL = False # If kaggle environment, set False\n","    IS_INFER = True # If kaggle environment, set True\n","    IS_USE_SAVED_MODEL = False # Use saved model or not\n","    IS_MIN_LEARN = False # Use min learning or not\n","    USE_OPTUNA = False # Use optuna or not\n","    USE_CONTINUOUS_UPDATE = True # Use test date on train or not\n","    USE_ALL_FEATUTES = True # Use all features or not\n","    USE_REVEALED_TARGETS = True # Use revealed targets or not\n","    USE_INDEX = True # Use index or not\n","    IS_DEBUG = False\n","\n","    SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/sample_submission.csv')\n","    REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/revealed_targets.csv')\n","\n","    stopping_rounds = 100 # early_stopping用コールバック関数\n","    num_boost_round = 1000 # 計算回数\n","    update_num_boost_round = 1000 # 再学習の計算回数\n","    num_folds = 3 # クロスバリデーションの分割数\n","    continuos_dataset_span = 3 # DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span が更新の使用する対象のデータ\n","    continuos_train_span = 1 # DATA_COUNT_IN_SAME_BUCKET * continuos_train_span が更新の頻度\n","\n","    DEVICE = 'gpu' # cpu or gpu\n","    OPTUNA_TIME_BUDGET = 60 * 60 * 4 # 1 hours\n","    TARGET_STOCK_IDS = [0, 1]\n","\n","    optuna_params = {\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression',         # 回帰\n","        'metric': 'rmse',                  # 損失（誤差）\n","        'verbosity': -1,\n","        'deterministic':True, #再現性確保用のパラメータ\n","        'force_row_wise':True,  #再現性確保用のパラメータ\n","        'device': DEVICE\n","    }\n","\n","    \"\"\"\n","    lgb_params = {\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression', \n","        'metric': 'rmse', \n","        'verbosity': -1, \n","        'device': DEVICE,\n","        'feature_pre_filter': False, \n","        'lambda_l1': 0.0,\n","        'lambda_l2': 0.0,\n","        'num_leaves': 31, \n","        'feature_fraction': 0.8, \n","        'bagging_fraction': 1.0, \n","        'bagging_freq': 0, \n","        'min_child_samples': 20\n","    }\n","    \"\"\"\n","   \n","    lgb_params = {\n","        'task': 'train',                   # 学習\n","        'objective': 'regression',                # 目的関数の種類。ここでは回帰タスクを指定\n","        'metric': 'rmse',                          # 評価指標\n","        'boosting_type': 'gbdt',                  # ブースティングタイプ。勾配ブースティング決定木\n","        \"n_estimators\": 32,                        # ブースティングに使用する木の数。多いほど性能が向上するが計算コストが増加\n","        \"num_leaves\": 64,                         # 木に存在する最大の葉の数。大きい値は精度を向上させるが過学習のリスクが増加\n","        \"subsample\": 0.8,                         # 各木のトレーニングに使用されるデータの割合。過学習を防ぐために一部のデータをサンプリング\n","        \"colsample_bytree\": 0.8,                  # 木を構築する際に使用される特徴の割合。特徴のサブセットを使用し過学習を防ぐ\n","        \"learning_rate\": 0.01,                 # 学習率。小さい値は堅牢なモデルを生成するが収束に時間がかかる\n","        'max_depth': 32,                           # 木の最大の深さ。深い木は複雑なモデルを作成するが過学習のリスクがある\n","        \"device\": DEVICE,                         # トレーニングに使用するデバイス（CPUまたはGPU）\n","        \"verbosity\": -1,                          # LightGBMのログ出力のレベル。-1はログを出力しないことを意味する\n","       # \"importance_type\": \"gain\",                # 特徴重要度を計算する際の指標。\"gain\"は分割による平均情報利得\n","        'lambda_l1': 0.5,                         # L1正則化項の係数。過学習を防ぐためにモデルの複雑さにペナルティを課す\n","        'lambda_l2': 0.5,                         # L2正則化項の係数。同じく過学習を防ぐ\n","        'bagging_freq': 5,                 # バギング実施頻度\n","        'min_child_samples': 10,           # 葉に含まれる最小データ数\n","        'seed': seed,                       # シード値\n","    }\n","\n","# For local environment\n","else:\n","    BASE_OUTPUT_PATH = Path(f'../output')\n","    BASE_INPUT_PATH = Path(f'../kaggle/input/optiver-trading-at-the-close')\n","    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n","    TEST_FILE = Path(f'{BASE_INPUT_PATH}/test.csv')\n","\n","    SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/sample_submission.csv')\n","    REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/revealed_targets.csv')\n","\n","    IS_LOCAL = True\n","    IS_INFER = True\n","    IS_USE_SAVED_MODEL = False # Use saved model or not\n","    IS_MIN_LEARN = True\n","    USE_OPTUNA = False # Use optuna or not\n","    USE_ALL_FEATUTES = True # Use all features or not\n","    USE_CONTINUOUS_UPDATE = True # Use test date on train or not\n","    USE_REVEALED_TARGETS = False # Use revealed targets or not \n","    USE_INDEX = True # Use index or not\n","    IS_DEBUG = True\n","    TARGET_STOCK_IDS = [0]\n","\n","    # For training\n","    stopping_rounds = 1 # early_stopping用コールバック関数\n","    num_boost_round = 1 # 計算回数\n","    update_num_boost_round = 1\n","    num_folds = 2 # クロスバリデーションの分割数\n","    continuos_dataset_span = 3 # DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span が更新の使用する対象のデータ\n","    continuos_train_span = 2 # DATA_COUNT_IN_SAME_BUCKET * continuos_train_span が更新の頻度\n","\n","    DEVICE = 'cpu' # cpu or gpu\n","    OPTUNA_TIME_BUDGET = 60 # 1 min\n","\n","    optuna_params = {\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression',         # 回帰\n","        'metric': 'rmse',                  # 損失（誤差）\n","        'verbosity': -1,\n","        'deterministic':True, #再現性確保用のパラメータ\n","        'force_row_wise':True,  #再現性確保用のパラメータ\n","        'device': DEVICE\n","    }\n","\n","    lgb_params = {\n","        'task': 'train',                   # 学習\n","        'boosting_type': 'gbdt',           # GBDT\n","        'objective': 'regression',         # 回帰\n","        'metric': 'rmse',                  # 損失（誤差）\n","        'learning_rate': 0.01,             # 学習率\n","        'lambda_l1': 0.5,                  # L1正則化項の係数\n","        'lambda_l2': 0.5,                  # L2正則化項の係数\n","        'num_leaves': 10,                  # 最大葉枚数\n","        'feature_fraction': 0.5,           # ランダムに抽出される列の割合\n","        'bagging_fraction': 0.5,           # ランダムに抽出される標本の割合\n","        'bagging_freq': 5,                 # バギング実施頻度\n","        'min_child_samples': 10,           # 葉に含まれる最小データ数\n","        'seed': seed,                       # シード値\n","        \"device\": DEVICE,\n","        'verbosity': -1\n","    }\n","\n","\n","print(f\"BASE_OUTPUT_PATH: {BASE_OUTPUT_PATH}\")\n","print(f\"BASE_INPUT_PATH: {BASE_INPUT_PATH}\")\n","print(f\"TRAIN_FILE: {TRAIN_FILE}\")\n","print(f\"TEST_FILE: {TEST_FILE}\")\n","print(f\"IS_LOCAL: {IS_LOCAL}\")\n","print(f\"IS_INFER: {IS_INFER}\")\n","print(f\"IS_USE_SAVED_MODEL: {IS_USE_SAVED_MODEL}\")\n","print(f\"IS_MIN_LEARN: {IS_MIN_LEARN}\")\n","print(f\"USE_OPTUNA: {USE_OPTUNA}\")\n","print(f\"USE_CONTINUOUS_UPDATE: {USE_CONTINUOUS_UPDATE}\")\n","print(f\"USE_ALL_FEATUTES: {USE_ALL_FEATUTES}\")\n","print(f\"USE_REVEALED_TARGETS: {USE_REVEALED_TARGETS}\")\n","print(f\"USE_INDEX: {USE_INDEX}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Functions"]},{"cell_type":"markdown","metadata":{},"source":["## Memory Functions"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RAM memory GB usage = 0.4249\n","CPU times: user 75.7 ms, sys: 1.86 ms, total: 77.5 ms\n","Wall time: 77.1 ms\n"]}],"source":["%%time \n","\n","from gc import collect;\n","from psutil import Process;\n","from os import system, getpid, walk;\n","\n","# Defining global configurations and functions:-\n","\n","    \n","def GetMemUsage():\n","    \"\"\"\n","    This function defines the memory usage across the kernel. \n","    Source-\n","    https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\n","    \"\"\";\n","    \n","    pid = getpid();\n","    py = Process(pid);\n","    memory_use = py.memory_info()[0] / 2. ** 30;\n","    return f\"RAM memory GB usage = {memory_use :.4}\";\n","\n","\n","collect();\n","print(GetMemUsage())"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# 🧹 Function to reduce memory usage of a Pandas DataFrame\n","def reduce_mem_usage(df, name: str):\n","    \"\"\"\n","    Iterate through all numeric columns of a dataframe and modify the data type\n","    to reduce memory usage.\n","    \"\"\"\n","    \n","    # 📏 Calculate the initial memory usage of the DataFrame\n","    start_mem = df.memory_usage().sum() / 1024**2\n","\n","    # 🔄 Iterate through each column in the DataFrame\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","\n","        # Check if the column's data type is not 'object' (i.e., numeric)\n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            \n","            # Check if the column's data type is an integer\n","            if str(col_type)[:3] == \"int\":\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                # Check if the column's data type is a float\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float32)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float32)\n","\n","\n","    print(f\"Memory usage of {name} is {start_mem:.2f} MB\")\n","    end_mem = df.memory_usage().sum() / 1024**2\n","    print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n","    decrease = 100 * (start_mem - end_mem) / start_mem\n","    print(f\"Decreased by {decrease:.2f}%\")\n","\n","    # 🔄 Return the DataFrame with optimized memory usage\n","\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["## API Function"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:28:59.692816Z","iopub.status.busy":"2023-11-23T00:28:59.692460Z","iopub.status.idle":"2023-11-23T00:28:59.713818Z","shell.execute_reply":"2023-11-23T00:28:59.712908Z","shell.execute_reply.started":"2023-11-23T00:28:59.692784Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 44 µs, sys: 1 µs, total: 45 µs\n","Wall time: 47 µs\n"]}],"source":["%%time \n","\n","from typing import Sequence, Tuple\n","import pandas as pd\n","\n","# for local execution\n","class MockApi:\n","    def __init__(self):\n","        '''\n","        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n","        They've been intentionally left in an invalid state.\n","\n","        Variables to set:\n","            input_paths: a list of two or more paths to the csv files to be served\n","            group_id_column: the column that identifies which groups of rows the API should serve.\n","                A call to iter_test serves all rows of all dataframes with the current group ID value.\n","            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n","        '''\n","        self.input_paths: Sequence[str] = [TEST_FILE, REVEALED_TARGETS_FILE, SAMPLE_SUBMISSION_FILE]\n","        self.group_id_column: str = 'time_id'\n","        self.export_group_id_column: bool = True\n","        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n","        assert len(self.input_paths) >= 2\n","\n","        self._status = 'initialized'\n","        self.predictions = []\n","\n","    def iter_test(self) -> Tuple[pd.DataFrame]:\n","        '''\n","        Loads all of the dataframes specified in self.input_paths,\n","        then yields all rows in those dataframes that equal the current self.group_id_column value.\n","        '''\n","        if self._status != 'initialized':\n","\n","            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n","\n","        dataframes = []\n","        for pth in self.input_paths:\n","            dataframes.append(pd.read_csv(pth, low_memory=False))\n","        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n","        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n","\n","        for group_id in group_order:\n","            self._status = 'prediction_needed'\n","            current_data = []\n","            for df in dataframes:\n","                cur_df = df.loc[group_id].copy()\n","                # returning single line dataframes from df.loc requires special handling\n","                if not isinstance(cur_df, pd.DataFrame):\n","                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n","                    cur_df.index.name = self.group_id_column\n","                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n","                current_data.append(cur_df)\n","            yield tuple(current_data)\n","\n","            while self._status != 'prediction_received':\n","                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n","                yield None\n","\n","        with open('submission.csv', 'w') as f_open:\n","            pd.concat(self.predictions).to_csv(f_open, index=False)\n","        self._status = 'finished'\n","\n","    def predict(self, user_predictions: pd.DataFrame):\n","        '''\n","        Accepts and stores the user's predictions and unlocks iter_test once that is done\n","        '''\n","        if self._status == 'finished':\n","            raise Exception('You have already made predictions for the full test set.')\n","        if self._status != 'prediction_needed':\n","            raise Exception('You must get the next test sample from `iter_test()` first.')\n","        if not isinstance(user_predictions, pd.DataFrame):\n","            raise Exception('You must provide a DataFrame.')\n","\n","        self.predictions.append(user_predictions)\n","        self._status = 'prediction_received'\n","\n","def make_env():\n","    return MockApi()"]},{"cell_type":"markdown","metadata":{},"source":["## Pandas Functions"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:28:59.716182Z","iopub.status.busy":"2023-11-23T00:28:59.715904Z","iopub.status.idle":"2023-11-23T00:28:59.728659Z","shell.execute_reply":"2023-11-23T00:28:59.727815Z","shell.execute_reply.started":"2023-11-23T00:28:59.716159Z"},"trusted":true},"outputs":[],"source":["def pd_display_max():\n","    pd.set_option('display.max_rows', None)  # 行の最大表示数を無制限に設定\n","    pd.set_option('display.max_columns', None)  # 列の最大表示数を無制限に設定\n","    pd.set_option('display.width', None)  # 表示幅を拡張\n","    pd.set_option('display.max_colwidth', None)  # 列の幅を最大に設定\n","\n","def pd_clear_display_max():\n","    pd.set_option('display.max_rows', 10)\n","    pd.set_option('display.max_columns', 10)\n","    pd.set_option('display.width', None)  # 表示幅を拡張\n","    pd.set_option('display.max_colwidth', None)  # 列の幅を最大に設定"]},{"cell_type":"markdown","metadata":{},"source":["## Feature Functions"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:28:59.746762Z","iopub.status.busy":"2023-11-23T00:28:59.746419Z","iopub.status.idle":"2023-11-23T00:29:00.392193Z","shell.execute_reply":"2023-11-23T00:29:00.391454Z","shell.execute_reply.started":"2023-11-23T00:28:59.746732Z"},"trusted":true},"outputs":[],"source":["# 🏎️ Import Numba for just-in-time (JIT) compilation and parallel processing\n","from numba import njit, prange\n","\n","# 📊 Function to compute triplet imbalance in parallel using Numba\n","@njit(parallel=True)\n","def compute_triplet_imbalance(df_values, comb_indices):\n","    num_rows = df_values.shape[0]\n","    num_combinations = len(comb_indices)\n","    imbalance_features = np.empty((num_rows, num_combinations))\n","\n","    # 🔁 Loop through all combinations of triplets\n","    for i in prange(num_combinations):\n","        a, b, c = comb_indices[i]\n","        \n","        # 🔁 Loop through rows of the DataFrame\n","        for j in range(num_rows):\n","            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n","            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n","            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n","            \n","            # 🚫 Prevent division by zero\n","            if mid_val == min_val:\n","                imbalance_features[j, i] = np.nan\n","            else:\n","                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n","\n","    return imbalance_features\n","\n","# 📈 Function to calculate triplet imbalance for given price data and a DataFrame\n","def calculate_triplet_imbalance_numba(price, df):\n","    # Convert DataFrame to numpy array for Numba compatibility\n","    df_values = df[price].values\n","    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n","\n","    # Calculate the triplet imbalance using the Numba-optimized function\n","    features_array = compute_triplet_imbalance(df_values, comb_indices)\n","\n","    # Create a DataFrame from the results\n","    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n","    features = pd.DataFrame(features_array, columns=columns)\n","\n","    return features"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:00.393697Z","iopub.status.busy":"2023-11-23T00:29:00.393368Z","iopub.status.idle":"2023-11-23T00:29:00.417347Z","shell.execute_reply":"2023-11-23T00:29:00.416464Z","shell.execute_reply.started":"2023-11-23T00:29:00.393671Z"},"trusted":true},"outputs":[],"source":["# 📊 Function to generate imbalance features\n","def imbalance_features(df):\n","    if DEVICE == 'gpu':\n","        import cudf\n","        df = cudf.from_pandas(df)\n","    \n","    # Define lists of price and size-related column names\n","    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","\n","    # V1 features\n","    # Calculate various features using Pandas eval function\n","    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n","    df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n","    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n","    df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n","    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n","    \n","    # Create features for pairwise price imbalances\n","    for c in combinations(prices, 2):\n","        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n","        \n","    # V2 features\n","    # Calculate additional features\n","    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n","    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n","    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n","    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n","    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n","    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n","    # Calculate the imbalance ratio\n","    df['match_balance'] = ( df['matched_size']  + (df['imbalance_buy_sell_flag'] * df['imbalance_size'])) / df['matched_size']\n","    \n","    # Calculate various statistical aggregation features\n","    \n","        \n","    # V3 features\n","    # Calculate shifted and return features for specific columns\n","    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n","        for window in [1, 2, 3, 10]:\n","            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n","            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n","    \n","    # Calculate diff features for specific columns\n","    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size']:\n","        for window in [1, 2, 3, 10]:\n","            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n","    if DEVICE == 'gpu':\n","        df = df.to_pandas()\n","    # Replace infinite values with 0\n","    return df.replace([np.inf, -np.inf], 0)\n","\n","def numba_imb_features(df):\n","    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","    \n","    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n","        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n","        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n","        \n","    # Calculate triplet imbalance features using the Numba-optimized function\n","    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n","        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n","        df[triplet_feature.columns] = triplet_feature.values\n","    return df\n","\n","# 📅 Function to generate time and stock-related features\n","def other_features(df):\n","    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n","    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n","    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n","\n","    # Map global features to the DataFrame\n","    for key, value in global_stock_id_feats.items():\n","        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n","\n","    return df\n","\n","# 🚀 Function to generate all features by combining imbalance and other features\n","def generate_basic_features(df):\n","    prev_cols = list(df.columns)\n","\n","    # Generate imbalance features\n","    df = imbalance_features(df)\n","    df = numba_imb_features(df)\n","\n","    df = reduce_mem_usage(df, \"generate_basic_features\")\n","\n","    collect()  # Perform garbage collection to free up memory\n","\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["# Generationg train dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:00.419125Z","iopub.status.busy":"2023-11-23T00:29:00.418758Z","iopub.status.idle":"2023-11-23T00:29:00.434666Z","shell.execute_reply":"2023-11-23T00:29:00.433818Z","shell.execute_reply.started":"2023-11-23T00:29:00.419092Z"},"trusted":true},"outputs":[],"source":["def load_train_dataset():\n","    df = pd.read_csv(TRAIN_FILE)\n","    # 🧹 Remove rows with missing values in the \"target\" column\n","    df = df.dropna(subset=[\"target\"])\n","    # 🔁 Reset the index of the DataFrame and apply the changes in place\n","    df.reset_index(drop=True, inplace=True)\n","    return df\n","\n","def load_test_dataset():\n","    df_test = pd.read_csv(TEST_FILE)\n","    \n","    df_revealed_targets = pd.read_csv(REVEALED_TARGETS_FILE)\n","    df_revealed_targets = df_revealed_targets.dropna(subset=['date_id', 'seconds_in_bucket', 'stock_id'])\n","    df_revealed_targets['revealed_date_id'] = df_revealed_targets['revealed_date_id'].astype(int).astype(str)\n","    df_revealed_targets['seconds_in_bucket'] = df_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n","    df_revealed_targets['stock_id'] = df_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n","    df_revealed_targets['date_id'] = df_revealed_targets['date_id'].astype(int).astype(str)\n","    df_revealed_targets['row_id'] = df_revealed_targets['date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n","    df_revealed_targets['revealed_row_id'] = df_revealed_targets['revealed_date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n","\n","    # USE_CONTINUOUS_UPDATEが有効の時、cacheのrow_idとdf_revealed_targetsのrevealed_row_idをleft joinする\n","    if USE_CONTINUOUS_UPDATE:\n","        df_r = df_revealed_targets[['revealed_row_id', 'revealed_target']]\n","        df_r.rename(columns={'revealed_target': 'target'}, inplace=True)\n","        df_r.rename(columns={'revealed_row_id': 'row_id'}, inplace=True)\n","        df_test = pd.merge(df_test, df_r, how='left', on='row_id')\n","        \n","    # USE_REVEALED_TARGETSが有効の時、cacheのrow_idとdf_revealed_targetsのrow_idをleft joinする\n","    if USE_REVEALED_TARGETS:\n","        df_r = df_revealed_targets[['row_id', 'revealed_target']]\n","        df_test = pd.merge(df_test, df_r, how='left', on='row_id')\n","        \n","    df_test = df_test.dropna(subset=[\"target\"])\n","    df_test.reset_index(drop=True, inplace=True)\n","    return df_test"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Load train dataset\n","MIN LEARN MODE : [0]\n","['stock_id', 'seconds_in_bucket', 'imbalance_size', 'imbalance_buy_sell_flag', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap']\n","RAM memory GB usage = 1.398\n","CPU times: user 6.26 s, sys: 1.98 s, total: 8.23 s\n","Wall time: 9.09 s\n"]}],"source":["%%time\n","# Check if the code is running in offline or online mode\n","print(\"Load train dataset\")\n","\n","df_train = load_train_dataset()\n","\n","if IS_MIN_LEARN:\n","    print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n","    # In local mode, stock id TARGET_STOCK_ID is used for training\n","    df_train = df_train[df_train[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","    \n","features = [c for c in df_train.columns if c not in [\"row_id\", \"target\", \"time_id\", \"row_id\", \"date_id\", \"currently_scored\"]]\n","print(features)\n","\n","collect();\n","print(GetMemUsage())"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:00.436164Z","iopub.status.busy":"2023-11-23T00:29:00.435884Z","iopub.status.idle":"2023-11-23T00:29:22.634330Z","shell.execute_reply":"2023-11-23T00:29:22.633415Z","shell.execute_reply.started":"2023-11-23T00:29:00.436140Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Step1. Generate general Global Stock ID Features and basic features\n","Memory usage of generate_basic_features is 22.00 MB\n","Memory usage after optimization is: 10.90 MB\n","Decreased by 50.46%\n","['spread_intensity', 'price_spread', 'all_sizes_skew', 'market_urgency', 'bid_size_diff_2', 'far_price_ask_price_imb', 'imbalance_size_shift_1', 'imbalance_size_shift_3', 'ask_size_diff_2', 'far_price_near_price_imb', 'bid_price_wap_imb', 'reference_price_shift_1', 'matched_imbalance', 'mid_price', 'matched_size_bid_size_ask_size_imb2', 'imbalance_buy_sell_flag_ret_2', 'reference_price_shift_2', 'ask_price_diff_10', 'ask_size_diff_3', 'reference_price_ret_10', 'matched_size_ask_size_imbalance_size_imb2', 'imbalance_size_ret_3', 'ask_price_bid_price_imb', 'size_imbalance', 'matched_size_ret_1', 'imbalance_size_ret_10', 'reference_price_wap_imb', 'matched_size_ret_10', 'imbalance_momentum', 'ask_price_diff_1', 'match_balance', 'ask_price_bid_price_reference_price_imb2', 'all_prices_skew', 'bid_price_diff_2', 'reference_price_ret_1', 'ask_price_diff_3', 'bid_price_diff_1', 'imbalance_size_shift_10', 'bid_price_diff_3', 'all_sizes_std', 'imbalance_buy_sell_flag_shift_10', 'all_sizes_mean', 'all_prices_std', 'bid_size_ask_size_imbalance_size_imb2', 'ask_price_wap_imb', 'all_prices_kurt', 'imbalance_buy_sell_flag_shift_1', 'reference_price_ret_3', 'volume', 'depth_pressure', 'liquidity_imbalance', 'imbalance_buy_sell_flag_ret_10', 'matched_size_shift_10', 'reference_price_ask_price_imb', 'bid_size_diff_1', 'matched_size_ret_2', 'ask_size_diff_10', 'imbalance_buy_sell_flag_shift_2', 'reference_price_far_price_imb', 'matched_size_bid_size_imbalance_size_imb2', 'far_price_bid_price_imb', 'imbalance_buy_sell_flag_shift_3', 'near_price_wap_imb', 'bid_price_diff_10', 'far_price_wap_imb', 'ask_price_wap_reference_price_imb2', 'reference_price_shift_10', 'matched_size_shift_3', 'bid_size_diff_3', 'ask_price_diff_2', 'ask_size_diff_1', 'reference_price_near_price_imb', 'all_prices_mean', 'bid_size_diff_10', 'matched_size_ret_3', 'imbalance_buy_sell_flag_ret_3', 'reference_price_shift_3', 'imbalance_buy_sell_flag_ret_1', 'near_price_bid_price_imb', 'near_price_ask_price_imb', 'imbalance_size_shift_2', 'ask_price_bid_price_wap_imb2', 'imbalance_size_ret_1', 'matched_size_shift_2', 'bid_price_wap_reference_price_imb2', 'all_sizes_kurt', 'imbalance_size_ret_2', 'price_pressure', 'reference_price_bid_price_imb', 'reference_price_ret_2', 'matched_size_shift_1']\n","RAM memory GB usage = 1.427\n","CPU times: user 898 ms, sys: 48.2 ms, total: 946 ms\n","Wall time: 1.12 s\n"]}],"source":["%%time\n","\n","print(\"Step1. Generate general Global Stock ID Features and basic features\")\n","prev_cols = list(df_train.columns)\n","global_stock_id_feats = {\n","    \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n","    \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n","    \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n","    \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n","    \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n","    \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n","}\n","\n","df_train = generate_basic_features(df_train)\n","\n","generated_feature_name = list(set(df_train.columns) - set(prev_cols))\n","features += generated_feature_name\n","print(generated_feature_name)\n","\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["## Generation train features"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Step2. Generate enhanced features\n","Dosent't use revealed targets\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 16.55 MB\n","Memory usage after optimization is: 14.73 MB\n","Decreased by 10.98%\n","['index_mean_match_balance_diff_1', 'index_std_match_balance_diff_7', 'index_mean_match_balance_diff_3', 'wap_vix_7', 'index_mean_match_balance_vix_5', 'index_mean_match_balance_vix_3', 'index_std_wap_diff_3', 'match_balance_diff_1', 'wap_diff_1', 'match_balance_vix_7', 'index_std_wap_vix_5', 'index_mean_match_balance_vix_7', 'index_mean_wap_vix_3', 'index_mean_wap_diff_3', 'index_mean_wap_vix_7', 'index_mean_wap', 'match_balance_vix_5', 'match_balance_diff_3', 'index_mean_wap_diff_7', 'index_std_wap_diff_1', 'index_std_match_balance', 'index_mean_match_balance', 'match_balance_vix_3', 'index_std_wap_vix_3', 'wap_vix_5', 'index_std_wap', 'index_std_wap_vix_7', 'wap_vix_3', 'wap_diff_7', 'index_mean_wap_vix_5', 'index_std_match_balance_diff_3', 'index_mean_match_balance_diff_7', 'wap_diff_3', 'index_std_wap_diff_7', 'index_mean_wap_diff_1', 'index_std_match_balance_vix_3', 'match_balance_diff_7', 'index_std_match_balance_diff_1', 'index_std_match_balance_vix_7', 'index_std_match_balance_vix_5']\n","RAM memory GB usage = 1.384\n","CPU times: user 1.79 s, sys: 25.3 ms, total: 1.82 s\n","Wall time: 1.82 s\n"]}],"source":["%%time\n","\n","print(\"Step2. Generate enhanced features\")\n","prev_cols = list(df_train.columns)\n","\n","def generate_historical_features(df):\n","    def cal_vix(df, group_key, target_col, period):\n","        return df.groupby(group_key)[target_col].transform(lambda x: np.log(x).diff().rolling(period).std())\n","\n","    print(\"generate_historical_features\")\n","    taget_cols = ['wap', 'match_balance']\n","    if USE_REVEALED_TARGETS:\n","        taget_cols.append('revealed_target')\n","    if USE_INDEX:\n","        taget_cols.append('index_mean_wap')\n","        taget_cols.append('index_std_wap')\n","        taget_cols.append('index_mean_match_balance')\n","        taget_cols.append('index_std_match_balance')\n","    for col in taget_cols:\n","        for window in [1, 3, 7]:\n","            col_name = f\"{col}_diff_{window}\"\n","            df[col_name] = df.groupby(\"stock_id\")[col].diff(window)\n","            #df[col_name] = df[col_name].fillna(0)  # NaNを0で置き換える\n","        for period in [3, 5, 7]:\n","            col_name = f\"{col}_vix_{period}\"\n","            df[col_name] = cal_vix(df, ['stock_id', 'date_id'], col, period)\n","    df = df.replace([np.inf, -np.inf], 0)\n","    return df\n","\n","def generate_index_features(df):\n","    print(\"generate_index_features\")\n","     # Calculating mean and std for 'wap' and 'match_balance'\n","    wap_stats = df.groupby(['date_id', 'seconds_in_bucket'])['wap'].agg(['mean', 'std']).reset_index()\n","    match_balance_stats = df.groupby(['date_id', 'seconds_in_bucket'])['match_balance'].agg(['mean', 'std']).reset_index()\n","\n","    # Adding prefix and suffix\n","    wap_stats = wap_stats.add_prefix('index_').add_suffix('_wap')\n","    match_balance_stats = match_balance_stats.add_prefix('index_').add_suffix('_match_balance')\n","\n","    # Adjusting column names for merging\n","    wap_stats = wap_stats.rename(columns={'index_date_id_wap': 'date_id', 'index_seconds_in_bucket_wap': 'seconds_in_bucket'})\n","    match_balance_stats = match_balance_stats.rename(columns={'index_date_id_match_balance': 'date_id', 'index_seconds_in_bucket_match_balance': 'seconds_in_bucket'})\n","\n","    # Merging with the original dataframe\n","    df = df.merge(wap_stats, on=['date_id', 'seconds_in_bucket'], how='left')\n","    df = df.merge(match_balance_stats, on=['date_id', 'seconds_in_bucket'], how='left')\n","\n","    del wap_stats, match_balance_stats\n","    return df\n","\n","def generate_enhance_features(df, is_train=False):\n","    if is_train:\n","        if USE_REVEALED_TARGETS:\n","            print(\"Use revealed targets\")\n","            df[f\"revealed_target\"] = df.groupby(['date_id', 'stock_id'])['target'].shift(1)\n","        else:\n","            print(\"Dosent't use revealed targets\")\n","    if USE_INDEX:\n","        print(\"Use index\")\n","        df = generate_index_features(df)\n","    df = generate_historical_features(df)\n","    df = reduce_mem_usage(df, \"generate_enhance_features\")\n","    collect()\n","    return df\n","\n","df_train = generate_enhance_features(df_train, is_train=True)\n","generated_feature_name = list(set(df_train.columns) - set(prev_cols))\n","features += generated_feature_name\n","print(generated_feature_name)\n","\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["cdf=None\n","if USE_REVEALED_TARGETS:\n","    cdf = df_train[['date_id', 'seconds_in_bucket', 'stock_id', 'target', 'revealed_target']].head(100)\n","cdf"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n","Wall time: 4.77 µs\n"]},{"data":{"text/plain":["'\\nprint(\"Step3. Normalized features\")\\nprev_cols = list(df_train.columns)\\n\\n\\n# Global features for normarization\\nglobal_wap = df_train[\\'wap\\'].describe()\\nglobal_mathch_balance = df_train[\\'match_balance\\'].describe()\\nglobal_target = df_train[\\'target\\'].describe()\\nglobal_reference_price = df_train[\\'reference_price\\'].describe()\\n\\n# 📈 Function to generate normalized features\\ndef generate_normalized_features(df, is_train):\\n    print(\"generate_normalized_features\")\\n    if is_train:\\n        df[\\'n_target\\'] = (df[\\'target\\'] - global_target[\\'mean\\']) / global_target[\\'std\\']\\n    df[\\'n_wap\\'] = (df[\\'wap\\'] - global_wap[\\'mean\\']) / global_wap[\\'std\\']\\n    df[\\'n_match_balance\\'] = (df[\\'match_balance\\'] - global_mathch_balance[\\'mean\\']) / global_mathch_balance[\\'std\\']\\n    df[\\'n_reference_price\\'] = (df[\\'reference_price\\'] - global_reference_price[\\'mean\\']) / global_reference_price[\\'std\\']\\n    \\n    df = reduce_mem_usage(df, \"generate_normalized_features\")\\n    return df\\n\\ndf_train = generate_normalized_features(df_train, True)\\ngenerated_feature_name = list(set(df_train.columns) - set(prev_cols))\\ngenerated_feature_name = [c for c in generated_feature_name if c not in [\"n_target\"]]\\n\\nfeatures += generated_feature_name\\nprint(generated_feature_name)\\n\\ncollect();\\nprint(GetMemUsage())\\n'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","\n","\"\"\"\n","print(\"Step3. Normalized features\")\n","prev_cols = list(df_train.columns)\n","\n","\n","# Global features for normarization\n","global_wap = df_train['wap'].describe()\n","global_mathch_balance = df_train['match_balance'].describe()\n","global_target = df_train['target'].describe()\n","global_reference_price = df_train['reference_price'].describe()\n","\n","# 📈 Function to generate normalized features\n","def generate_normalized_features(df, is_train):\n","    print(\"generate_normalized_features\")\n","    if is_train:\n","        df['n_target'] = (df['target'] - global_target['mean']) / global_target['std']\n","    df['n_wap'] = (df['wap'] - global_wap['mean']) / global_wap['std']\n","    df['n_match_balance'] = (df['match_balance'] - global_mathch_balance['mean']) / global_mathch_balance['std']\n","    df['n_reference_price'] = (df['reference_price'] - global_reference_price['mean']) / global_reference_price['std']\n","    \n","    df = reduce_mem_usage(df, \"generate_normalized_features\")\n","    return df\n","\n","df_train = generate_normalized_features(df_train, True)\n","generated_feature_name = list(set(df_train.columns) - set(prev_cols))\n","generated_feature_name = [c for c in generated_feature_name if c not in [\"n_target\"]]\n","\n","features += generated_feature_name\n","print(generated_feature_name)\n","\n","collect();\n","print(GetMemUsage())\n","\"\"\""]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Memory usage of train is 14.73 MB\n","Memory usage after optimization is: 14.73 MB\n","Decreased by 0.00%\n","RAM memory GB usage = 1.385\n"]}],"source":["df_train = reduce_mem_usage(df_train, 'train')\n","\n","collect();\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Feature selection"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:22.637651Z","iopub.status.busy":"2023-11-23T00:29:22.637344Z","iopub.status.idle":"2023-11-23T00:29:22.646354Z","shell.execute_reply":"2023-11-23T00:29:22.645449Z","shell.execute_reply.started":"2023-11-23T00:29:22.637627Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['stock_id',\n"," 'seconds_in_bucket',\n"," 'imbalance_size',\n"," 'imbalance_buy_sell_flag',\n"," 'reference_price',\n"," 'matched_size',\n"," 'far_price',\n"," 'near_price',\n"," 'bid_price',\n"," 'bid_size',\n"," 'ask_price',\n"," 'ask_size',\n"," 'wap',\n"," 'spread_intensity',\n"," 'price_spread',\n"," 'all_sizes_skew',\n"," 'market_urgency',\n"," 'bid_size_diff_2',\n"," 'far_price_ask_price_imb',\n"," 'imbalance_size_shift_1',\n"," 'imbalance_size_shift_3',\n"," 'ask_size_diff_2',\n"," 'far_price_near_price_imb',\n"," 'bid_price_wap_imb',\n"," 'reference_price_shift_1',\n"," 'matched_imbalance',\n"," 'mid_price',\n"," 'matched_size_bid_size_ask_size_imb2',\n"," 'imbalance_buy_sell_flag_ret_2',\n"," 'reference_price_shift_2',\n"," 'ask_price_diff_10',\n"," 'ask_size_diff_3',\n"," 'reference_price_ret_10',\n"," 'matched_size_ask_size_imbalance_size_imb2',\n"," 'imbalance_size_ret_3',\n"," 'ask_price_bid_price_imb',\n"," 'size_imbalance',\n"," 'matched_size_ret_1',\n"," 'imbalance_size_ret_10',\n"," 'reference_price_wap_imb',\n"," 'matched_size_ret_10',\n"," 'imbalance_momentum',\n"," 'ask_price_diff_1',\n"," 'match_balance',\n"," 'ask_price_bid_price_reference_price_imb2',\n"," 'all_prices_skew',\n"," 'bid_price_diff_2',\n"," 'reference_price_ret_1',\n"," 'ask_price_diff_3',\n"," 'bid_price_diff_1',\n"," 'imbalance_size_shift_10',\n"," 'bid_price_diff_3',\n"," 'all_sizes_std',\n"," 'imbalance_buy_sell_flag_shift_10',\n"," 'all_sizes_mean',\n"," 'all_prices_std',\n"," 'bid_size_ask_size_imbalance_size_imb2',\n"," 'ask_price_wap_imb',\n"," 'all_prices_kurt',\n"," 'imbalance_buy_sell_flag_shift_1',\n"," 'reference_price_ret_3',\n"," 'volume',\n"," 'depth_pressure',\n"," 'liquidity_imbalance',\n"," 'imbalance_buy_sell_flag_ret_10',\n"," 'matched_size_shift_10',\n"," 'reference_price_ask_price_imb',\n"," 'bid_size_diff_1',\n"," 'matched_size_ret_2',\n"," 'ask_size_diff_10',\n"," 'imbalance_buy_sell_flag_shift_2',\n"," 'reference_price_far_price_imb',\n"," 'matched_size_bid_size_imbalance_size_imb2',\n"," 'far_price_bid_price_imb',\n"," 'imbalance_buy_sell_flag_shift_3',\n"," 'near_price_wap_imb',\n"," 'bid_price_diff_10',\n"," 'far_price_wap_imb',\n"," 'ask_price_wap_reference_price_imb2',\n"," 'reference_price_shift_10',\n"," 'matched_size_shift_3',\n"," 'bid_size_diff_3',\n"," 'ask_price_diff_2',\n"," 'ask_size_diff_1',\n"," 'reference_price_near_price_imb',\n"," 'all_prices_mean',\n"," 'bid_size_diff_10',\n"," 'matched_size_ret_3',\n"," 'imbalance_buy_sell_flag_ret_3',\n"," 'reference_price_shift_3',\n"," 'imbalance_buy_sell_flag_ret_1',\n"," 'near_price_bid_price_imb',\n"," 'near_price_ask_price_imb',\n"," 'imbalance_size_shift_2',\n"," 'ask_price_bid_price_wap_imb2',\n"," 'imbalance_size_ret_1',\n"," 'matched_size_shift_2',\n"," 'bid_price_wap_reference_price_imb2',\n"," 'all_sizes_kurt',\n"," 'imbalance_size_ret_2',\n"," 'price_pressure',\n"," 'reference_price_bid_price_imb',\n"," 'reference_price_ret_2',\n"," 'matched_size_shift_1',\n"," 'index_mean_match_balance_diff_1',\n"," 'index_std_match_balance_diff_7',\n"," 'index_mean_match_balance_diff_3',\n"," 'wap_vix_7',\n"," 'index_mean_match_balance_vix_5',\n"," 'index_mean_match_balance_vix_3',\n"," 'index_std_wap_diff_3',\n"," 'match_balance_diff_1',\n"," 'wap_diff_1',\n"," 'match_balance_vix_7',\n"," 'index_std_wap_vix_5',\n"," 'index_mean_match_balance_vix_7',\n"," 'index_mean_wap_vix_3',\n"," 'index_mean_wap_diff_3',\n"," 'index_mean_wap_vix_7',\n"," 'index_mean_wap',\n"," 'match_balance_vix_5',\n"," 'match_balance_diff_3',\n"," 'index_mean_wap_diff_7',\n"," 'index_std_wap_diff_1',\n"," 'index_std_match_balance',\n"," 'index_mean_match_balance',\n"," 'match_balance_vix_3',\n"," 'index_std_wap_vix_3',\n"," 'wap_vix_5',\n"," 'index_std_wap',\n"," 'index_std_wap_vix_7',\n"," 'wap_vix_3',\n"," 'wap_diff_7',\n"," 'index_mean_wap_vix_5',\n"," 'index_std_match_balance_diff_3',\n"," 'index_mean_match_balance_diff_7',\n"," 'wap_diff_3',\n"," 'index_std_wap_diff_7',\n"," 'index_mean_wap_diff_1',\n"," 'index_std_match_balance_vix_3',\n"," 'match_balance_diff_7',\n"," 'index_std_match_balance_diff_1',\n"," 'index_std_match_balance_vix_7',\n"," 'index_std_match_balance_vix_5']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["# feature selection\n","if not USE_ALL_FEATUTES:\n","    features  = [\n","        \"revealed_target\",\n","        \"wap_diff_1\",\n","        \"index_mean_wap_diff_1\",\n","        \"seconds_in_bucket\",\n","        \"stock_id\",\n","    ]\n","#df_valid = df_train[\"target\"]\n","#df_train = df_train[features]\n","if USE_REVEALED_TARGETS:\n","    features.remove(\"revealed_target\")\n","features"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"data":{"text/plain":["index_std_match_balance_vix_5               26455\n","index_std_wap                               26455\n","index_std_match_balance_diff_7              26455\n","index_std_wap_diff_3                        26455\n","index_std_wap_vix_5                         26455\n","index_std_wap_diff_1                        26455\n","index_std_match_balance                     26455\n","index_std_wap_vix_3                         26455\n","index_std_wap_vix_7                         26455\n","index_std_match_balance_diff_3              26455\n","index_std_wap_diff_7                        26455\n","index_std_match_balance_vix_3               26455\n","index_std_match_balance_diff_1              26455\n","index_std_match_balance_vix_7               26455\n","depth_pressure                              14517\n","far_price_wap_imb                           14517\n","far_price_bid_price_imb                     14517\n","reference_price_far_price_imb               14517\n","far_price                                   14517\n","far_price_ask_price_imb                     14517\n","far_price_near_price_imb                    14517\n","reference_price_near_price_imb              14430\n","near_price_bid_price_imb                    14430\n","near_price                                  14430\n","near_price_wap_imb                          14430\n","near_price_ask_price_imb                    14430\n","imbalance_buy_sell_flag_ret_1                4207\n","imbalance_size_ret_1                         4207\n","index_mean_match_balance_vix_7               3893\n","match_balance_vix_7                          3893\n","imbalance_buy_sell_flag_ret_2                3855\n","imbalance_size_ret_2                         3855\n","bid_price_wap_reference_price_imb2           3614\n","imbalance_buy_sell_flag_ret_3                3568\n","imbalance_size_ret_3                         3568\n","ask_price_bid_price_reference_price_imb2     3528\n","wap_vix_7                                    3367\n","index_mean_wap_vix_7                         3367\n","match_balance_vix_5                          2920\n","index_mean_match_balance_vix_5               2920\n","index_mean_wap_vix_5                         2405\n","wap_vix_5                                    2405\n","imbalance_size_ret_10                        2239\n","imbalance_buy_sell_flag_ret_10               2239\n","index_mean_match_balance_vix_3               1946\n","match_balance_vix_3                          1946\n","index_mean_wap_vix_3                         1443\n","wap_vix_3                                    1443\n","ask_price_wap_reference_price_imb2             56\n","ask_price_bid_price_wap_imb2                   38\n","imbalance_size_shift_10                        10\n","matched_size_ret_10                            10\n","reference_price_ret_10                         10\n","bid_price_diff_10                              10\n","bid_size_diff_10                               10\n","reference_price_shift_10                       10\n","imbalance_buy_sell_flag_shift_10               10\n","ask_size_diff_10                               10\n","ask_price_diff_10                              10\n","matched_size_shift_10                          10\n","match_balance_diff_7                            7\n","index_mean_wap_diff_7                           7\n","index_mean_match_balance_diff_7                 7\n","wap_diff_7                                      7\n","ask_price_diff_3                                3\n","match_balance_diff_3                            3\n","ask_size_diff_3                                 3\n","imbalance_buy_sell_flag_shift_3                 3\n","index_mean_wap_diff_3                           3\n","matched_size_ret_3                              3\n","index_mean_match_balance_diff_3                 3\n","bid_price_diff_3                                3\n","matched_size_shift_3                            3\n","wap_diff_3                                      3\n","reference_price_ret_3                           3\n","bid_size_diff_3                                 3\n","reference_price_shift_3                         3\n","imbalance_size_shift_3                          3\n","reference_price_shift_2                         2\n","bid_size_diff_2                                 2\n","ask_size_diff_2                                 2\n","matched_size_shift_2                            2\n","imbalance_buy_sell_flag_shift_2                 2\n","bid_price_diff_2                                2\n","reference_price_ret_2                           2\n","imbalance_size_shift_2                          2\n","ask_price_diff_2                                2\n","matched_size_ret_2                              2\n","bid_size_diff_1                                 1\n","spread_intensity                                1\n","reference_price_ret_1                           1\n","bid_price_diff_1                                1\n","index_mean_wap_diff_1                           1\n","imbalance_size_shift_1                          1\n","imbalance_buy_sell_flag_shift_1                 1\n","wap_diff_1                                      1\n","match_balance_diff_1                            1\n","matched_size_ret_1                              1\n","ask_size_diff_1                                 1\n","ask_price_diff_1                                1\n","reference_price_shift_1                         1\n","matched_size_shift_1                            1\n","index_mean_match_balance_diff_1                 1\n","imbalance_momentum                              1\n","dtype: int64"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["pd_display_max()\n","nan_count = df_train[features].isna().sum()\n","#df_train[features].to_csv('train.csv', index=False)\n","nan_count = nan_count[nan_count > 0].sort_values(ascending=False)\n","nan_count"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["pd_clear_display_max()"]},{"cell_type":"markdown","metadata":{},"source":["# Train function (lightgbm)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:29:24.167604Z","iopub.status.busy":"2023-11-23T00:29:24.167007Z","iopub.status.idle":"2023-11-23T00:31:37.372490Z","shell.execute_reply":"2023-11-23T00:31:37.371543Z","shell.execute_reply.started":"2023-11-23T00:29:24.167568Z"},"trusted":true},"outputs":[],"source":["# 📦 Import necessary libraries\n","import numpy as np\n","import lightgbm as lgb\n","from sklearn.metrics import mean_absolute_error\n","import gc\n","import os\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.model_selection import KFold\n","import numpy as np\n","from dataclasses import dataclass\n","import sys\n","import shutil\n","import lightgbm as lgb\n","\n","from warnings import simplefilter\n","simplefilter(\"ignore\", category=RuntimeWarning)\n","\n","@dataclass\n","class Model:\n","    booster: lgb.Booster\n","    fold: int\n","    feature_importance: pd.DataFrame\n","    score: float\n","    best_iteration: int\n","    train_time: float = None\n","    weight: float = None\n","    mem_usage: float = None\n","    train_func: str = None\n","    is_latest: bool = False\n","\n","def train_model(train_x, train_y, val_x, val_y, best_params=None):\n","    trains = lgb.Dataset(train_x, train_y)\n","    valids = lgb.Dataset(val_x, val_y, reference=trains)\n","\n","    verbose_eval = -1\n","    if best_params is None:\n","        params = lgb_params\n","    else:\n","        params = best_params\n","\n","    print(\"Use params:\")\n","    print(params)\n","\n","    booster = lgb.train(\n","        params,\n","        trains,\n","        valid_sets=valids, # 検証データ\n","        num_boost_round=num_boost_round,\n","        keep_training_booster=True,\n","        callbacks=[\n","                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n","                lgb.log_evaluation(verbose_eval)\n","        ]\n","    )\n","\n","    del trains, valids\n","    return booster\n","\n","def cross_train(df, key, n_splits, features, valid_name, best_params=None):\n","    \"\"\" For Cross Train\n","\n","    Args:\n","        df (_type_): _description_\n","        n_splits (_type_): _description_\n","\n","    Returns:\n","        _type_: _description_\n","    \"\"\"\n","    print(\"----------------------------------------\")\n","    print(f\"Cross Train key id {key}: start, shape: {df_train.shape}, n_splits: {n_splits}\")\n","    print(f\"num_boost_round: {num_boost_round}, stopping_rounds: {stopping_rounds}, folds: {num_folds}\")\n","\n","    models = []\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","    df.reset_index(drop=True, inplace=True)\n","    \n","    for fold, (train_indices, valid_indices) in enumerate(kf.split(df)):\n","        print(f\"----- Train {key}: {fold} start -----\")\n","        now_time = time.time()\n","\n","        print(f\"train_indices: {train_indices}, valid_indices: {valid_indices}\")\n","        X_train, X_valid = df[features].iloc[train_indices], df[features].iloc[valid_indices]\n","        y_train, y_valid = df[valid_name].loc[train_indices], df[valid_name].loc[valid_indices]\n","        print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}, y_train: {y_train.shape}, y_valid: {y_valid.shape}\")\n","\n","        booster = train_model(X_train, y_train, X_valid, y_valid, best_params)\n","        print(booster.best_score)\n","        y_valid_pred = booster.predict(X_valid)\n","        \n","        score = mean_absolute_error(y_valid, y_valid_pred)\n","        train_time = time.time() - now_time\n","        mem_usage = sys.getsizeof(booster) / (1024 * 1024) # MB\n","        model = Model(booster, fold, booster.feature_importance(), score, booster.best_iteration, train_time, weight= 1 / n_splits, mem_usage=mem_usage, train_func=\"lightgbm\", is_latest=True)\n","        print(f\"{key}: {fold} end, score: {score}, time: {model.train_time}, best_iteration: {model.best_iteration}, memory usage: {model.mem_usage}\")\n","        \n","        models.append(model)\n","        \n","        del X_train, X_valid, y_train, y_valid\n","        gc.collect()\n","        print(GetMemUsage())\n","        print(f\"----- Train {key}: {fold} end -----\")\n","\n","    print(f\"Cross train {key} model len {len(models)}\")\n","    print(\"----------------------------------------\")\n","    return key, models"]},{"cell_type":"markdown","metadata":{},"source":["# Train function (optuna)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 163 ms, sys: 27.4 ms, total: 190 ms\n","Wall time: 741 ms\n"]}],"source":["%%time\n","\n","import optuna.integration.lightgbm as optuna_lgb\n","import optuna\n","import lightgbm\n","optuna.logging.set_verbosity(optuna.logging.ERROR)\n","\n","class TunerCVCheckpointCallback(object):\n","    \"\"\"Optuna の LightGBMTunerCV から学習済みモデルを取り出すためのコールバック\"\"\"\n","\n","    def __init__(self):\n","        # Models\n","        self.models = []\n","        self.counter = 0\n","\n","    def get_models(self):\n","        # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster\n","        return self.models\n","\n","    def __call__(self, env: lightgbm.callback.CallbackEnv):\n","        \"\"\"_summary_\n","\n","        Args:\n","            env (lightgbm.callback.CallbackEnv): _description_\n","            \"model\",\n","            \"params\",\n","            \"iteration\",\n","            \"begin_iteration\",\n","            \"end_iteration\",\n","            \"evaluation_result_list\"\n","        \"\"\"\n","        print(\"\")\n","\n","        self.counter += 1\n","        print(\"-------------------\")\n","        print(f\"Counter: {self.counter}\")\n","        print(f\"Iteration: {env.iteration}\")\n","        print(f\"Begin_iteration: {env.begin_iteration}\")\n","        print(f\"End_iteration: {env.end_iteration}\")\n","        print(f\"Evaluation_result_list: {env.evaluation_result_list}\")\n","        print(f\"Model best_iteration: {env.model.best_iteration}\")\n","        print(\"Params: \", env.params)\n","        #self.models.append(env.model)\n","        del env\n","\n","        collect();\n","        print(GetMemUsage())\n","\n","def optuna_tuning(df, n_splits, features, valid_name, model_save_path):\n","    df_train = df[features]\n","    df_valid = df[valid_name]\n","    \n","    trains = optuna_lgb.Dataset(df_train, df_valid)\n","    \n","    print(\"------- Optuna Tuning Start -------\")\n","    now_time = time.time()\n","    print(f\"num_boost_round: {num_boost_round}, stopping_rounds: {stopping_rounds}, folds: {num_folds}\")\n","\n","    folds = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","    checkpoint_cb = TunerCVCheckpointCallback()\n","    \n","    verbose_eval = 0\n","    # https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.lightgbm.LightGBMTunerCV.html\n","    tuner = optuna_lgb.LightGBMTunerCV(\n","        optuna_params,\n","        trains,\n","        num_boost_round=num_boost_round,\n","        folds=folds,\n","        show_progress_bar=False,\n","        return_cvbooster=True,\n","        verbosity=-1,\n","        model_dir=model_save_path,\n","        optuna_seed=seed,\n","        time_budget=OPTUNA_TIME_BUDGET,\n","        callbacks=[\n","                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n","                lgb.log_evaluation(verbose_eval),\n","                checkpoint_cb\n","        ]\n","    )\n","    \n","    tuner.run()\n","    best_params = tuner.best_params\n","    \n","    print(\"Params: \")\n","    for key, value in best_params.items():\n","        print(\" {}: {}\".format(key, value))\n","\n","    print(\"\")\n","    print(\"len(tuner.study.trials): \", len(tuner.study.trials))\n","    #print(\"len(checkpoint_cb.cv_boosters): \", len(checkpoint_cb.models))\n","    print(\"Tuner best_params\", tuner.best_params)\n","    print(\"Tuner best score: \", tuner.best_score)\n","   \n","    # 最も良かったパラメータをキーにして学習済みモデルを取り出す\n","    best_booster = tuner.get_best_booster()\n","    score = -1\n","    train_time = time.time() - now_time\n","    mem_usage = sys.getsizeof(best_booster) / (1024 * 1024) # MB\n","    feature_importance = np.mean(best_booster.feature_importance(), axis=0)\n","\n","    best_model = Model(best_booster, 1, feature_importance, score, best_booster.best_iteration, train_time, weight= 1, mem_usage=mem_usage, train_func=\"optuna_lgb\")\n","    print(\"------- Optuna Tuning End -------\")\n","    return best_params, best_model\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------\n","Cross Train key id -1: start, shape: (26455, 148), n_splits: 2\n","num_boost_round: 1, stopping_rounds: 1, folds: 2\n","----- Train -1: 0 start -----\n","       stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n","0             0        0                  0    3.180603e+06   \n","1             0        0                 10    1.299773e+06   \n","2             0        0                 20    1.299773e+06   \n","3             0        0                 30    1.299773e+06   \n","4             0        0                 40    1.218204e+06   \n","...         ...      ...                ...             ...   \n","26450         0      480                500    0.000000e+00   \n","26451         0      480                510    0.000000e+00   \n","26452         0      480                520    4.755137e+05   \n","26453         0      480                530    4.755137e+05   \n","26454         0      480                540    4.755137e+05   \n","\n","       imbalance_buy_sell_flag  ...  index_std_match_balance_diff_3  \\\n","0                            1  ...                             NaN   \n","1                            1  ...                             NaN   \n","2                            1  ...                             NaN   \n","3                            1  ...                             NaN   \n","4                            1  ...                             NaN   \n","...                        ...  ...                             ...   \n","26450                        0  ...                             NaN   \n","26451                        0  ...                             NaN   \n","26452                       -1  ...                             NaN   \n","26453                       -1  ...                             NaN   \n","26454                       -1  ...                             NaN   \n","\n","       index_std_match_balance_diff_7  index_std_match_balance_vix_3  \\\n","0                                 NaN                            NaN   \n","1                                 NaN                            NaN   \n","2                                 NaN                            NaN   \n","3                                 NaN                            NaN   \n","4                                 NaN                            NaN   \n","...                               ...                            ...   \n","26450                             NaN                            NaN   \n","26451                             NaN                            NaN   \n","26452                             NaN                            NaN   \n","26453                             NaN                            NaN   \n","26454                             NaN                            NaN   \n","\n","       index_std_match_balance_vix_5  index_std_match_balance_vix_7  \n","0                                NaN                            NaN  \n","1                                NaN                            NaN  \n","2                                NaN                            NaN  \n","3                                NaN                            NaN  \n","4                                NaN                            NaN  \n","...                              ...                            ...  \n","26450                            NaN                            NaN  \n","26451                            NaN                            NaN  \n","26452                            NaN                            NaN  \n","26453                            NaN                            NaN  \n","26454                            NaN                            NaN  \n","\n","[26455 rows x 148 columns]\n","train_indices: [    1     2     7 ... 26451 26453 26454], valid_indices: [    0     3     4 ... 26445 26446 26452]\n","X_train: (13227, 144), X_valid: (13228, 144), y_train: (13227,), y_valid: (13228,)\n","Use params:\n","{'task': 'train', 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.01, 'lambda_l1': 0.5, 'lambda_l2': 0.5, 'num_leaves': 10, 'feature_fraction': 0.5, 'bagging_fraction': 0.5, 'bagging_freq': 5, 'min_child_samples': 10, 'seed': 2023, 'device': 'cpu', 'verbosity': -1}\n","Training until validation scores don't improve for 1 rounds\n","Did not meet early stopping. Best iteration is:\n","[1]\tvalid_0's rmse: 6.0728\n","defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 6.072803134951002)])})\n","-1: 0 end, score: 4.395829916596198, time: 0.2008681297302246, best_iteration: 1, memory usage: 5.340576171875e-05\n","RAM memory GB usage = 1.462\n","----- Train -1: 0 end -----\n","----- Train -1: 1 start -----\n","       stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n","0             0        0                  0    3.180603e+06   \n","1             0        0                 10    1.299773e+06   \n","2             0        0                 20    1.299773e+06   \n","3             0        0                 30    1.299773e+06   \n","4             0        0                 40    1.218204e+06   \n","...         ...      ...                ...             ...   \n","26450         0      480                500    0.000000e+00   \n","26451         0      480                510    0.000000e+00   \n","26452         0      480                520    4.755137e+05   \n","26453         0      480                530    4.755137e+05   \n","26454         0      480                540    4.755137e+05   \n","\n","       imbalance_buy_sell_flag  ...  index_std_match_balance_diff_3  \\\n","0                            1  ...                             NaN   \n","1                            1  ...                             NaN   \n","2                            1  ...                             NaN   \n","3                            1  ...                             NaN   \n","4                            1  ...                             NaN   \n","...                        ...  ...                             ...   \n","26450                        0  ...                             NaN   \n","26451                        0  ...                             NaN   \n","26452                       -1  ...                             NaN   \n","26453                       -1  ...                             NaN   \n","26454                       -1  ...                             NaN   \n","\n","       index_std_match_balance_diff_7  index_std_match_balance_vix_3  \\\n","0                                 NaN                            NaN   \n","1                                 NaN                            NaN   \n","2                                 NaN                            NaN   \n","3                                 NaN                            NaN   \n","4                                 NaN                            NaN   \n","...                               ...                            ...   \n","26450                             NaN                            NaN   \n","26451                             NaN                            NaN   \n","26452                             NaN                            NaN   \n","26453                             NaN                            NaN   \n","26454                             NaN                            NaN   \n","\n","       index_std_match_balance_vix_5  index_std_match_balance_vix_7  \n","0                                NaN                            NaN  \n","1                                NaN                            NaN  \n","2                                NaN                            NaN  \n","3                                NaN                            NaN  \n","4                                NaN                            NaN  \n","...                              ...                            ...  \n","26450                            NaN                            NaN  \n","26451                            NaN                            NaN  \n","26452                            NaN                            NaN  \n","26453                            NaN                            NaN  \n","26454                            NaN                            NaN  \n","\n","[26455 rows x 148 columns]\n","train_indices: [    0     3     4 ... 26445 26446 26452], valid_indices: [    1     2     7 ... 26451 26453 26454]\n","X_train: (13228, 144), X_valid: (13227, 144), y_train: (13228,), y_valid: (13227,)\n","Use params:\n","{'task': 'train', 'boosting_type': 'gbdt', 'objective': 'regression', 'metric': 'rmse', 'learning_rate': 0.01, 'lambda_l1': 0.5, 'lambda_l2': 0.5, 'num_leaves': 10, 'feature_fraction': 0.5, 'bagging_fraction': 0.5, 'bagging_freq': 5, 'min_child_samples': 10, 'seed': 2023, 'device': 'cpu', 'verbosity': -1}\n","Training until validation scores don't improve for 1 rounds\n","Did not meet early stopping. Best iteration is:\n","[1]\tvalid_0's rmse: 6.03614\n","defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('rmse', 6.03613647926317)])})\n","-1: 1 end, score: 4.363177932328552, time: 0.24135899543762207, best_iteration: 1, memory usage: 5.340576171875e-05\n","RAM memory GB usage = 1.497\n","----- Train -1: 1 end -----\n","Cross train -1 model len 2\n","----------------------------------------\n","RAM memory GB usage = 1.497\n","CPU times: user 1.35 s, sys: 70.4 ms, total: 1.42 s\n","Wall time: 827 ms\n"]}],"source":["%%time\n","\n","KEY = \"-1\"\n","\n","# Train\n","best_params = None\n","key_models = None\n","if USE_OPTUNA:\n","    model_save_base_path = f\"{BASE_OUTPUT_PATH}/model\"\n","    if os.path.exists(model_save_base_path):\n","        print(f\"{model_save_base_path} already exists, clean up it.\")\n","        shutil.rmtree(model_save_base_path)\n","    os.makedirs(model_save_base_path)\n","    print(f\"model_save_base_path: {model_save_base_path}\")\n","\n","    best_params, best_model = optuna_tuning(df=df_train, n_splits=num_folds, features=features, valid_name=\"target\", model_save_path=model_save_base_path)\n","    key_models = [(KEY, [best_model])]\n","else:\n","    #key_models = df_train.groupby(\"seconds_in_bucket\").apply(lambda x: cross_train(df=x, key=x.name, n_splits=num_folds, feature_name=feature_name, valid_name=\"target\", best_params=best_params))\n","    key_models = [cross_train(df_train, key=KEY, n_splits=num_folds, features=features, valid_name=\"target\", best_params=best_params)]\n","    if IS_USE_SAVED_MODEL:\n","        model_save_base_path = f\"{BASE_OUTPUT_PATH}/model\"\n","        if os.path.exists(model_save_base_path):\n","            print(f\"{model_save_base_path} already exists, clean up it.\")\n","            shutil.rmtree(model_save_base_path)\n","        os.makedirs(model_save_base_path)\n","\n","        key_model_paths = []\n","        for key, models in key_models:\n","            model_save_path = f\"{model_save_base_path}/{key}\"\n","            os.makedirs(model_save_path)\n","            model_paths = []\n","            for model in models:\n","                model_save_fullpath = f\"{model_save_path}/model_{key}_{model.fold}.txt\"\n","                model.model.save_model(model_save_fullpath)\n","                model_paths.append(model_save_fullpath)\n","            key_model_paths.append((key, model_paths))\n","\n","        model_dict_saved = {key: model_paths for key, model_paths in key_model_paths}\n","        print(model_dict_saved)\n","\n","\n","model_dict = {key: model for key, model in key_models}\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Update model using test"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["date_duration 165\n","Update model with test date\n","MIN LEARN MODE : [0]\n","Memory usage of generate_basic_features is 0.09 MB\n","Memory usage after optimization is: 0.05 MB\n","Decreased by 50.46%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.07 MB\n","Memory usage after optimization is: 0.06 MB\n","Decreased by 10.96%\n","update_global_train_cache\n","Memory usage of global_train_cache is 0.09 MB\n","Memory usage after optimization is: 0.09 MB\n","Decreased by 1.18%\n","Updated global_train_cache, len:  165\n","165\n","     date_id  seconds_in_bucket  stock_id  origin    target\n","0        478                  0         0       1 -5.429983\n","1        478                 10         0       1 -3.489852\n","2        478                 20         0       1 -3.330112\n","3        478                 30         0       1 -1.900196\n","4        478                 40         0       1  2.189875\n","..       ...                ...       ...     ...       ...\n","160      480                500         0       0  3.999472\n","161      480                510         0       0  3.190041\n","162      480                520         0       0 -0.169873\n","163      480                530         0       0  3.110170\n","164      480                540         0       0  0.760555\n","\n","[165 rows x 5 columns]\n","Update model with test date end\n","RAM memory GB usage = 1.513\n","CPU times: user 665 ms, sys: 27.2 ms, total: 693 ms\n","Wall time: 692 ms\n"]}],"source":["%%time\n","\n","# global train cache for continuous update\n","global_train_cache = df_train.copy()\n","# origin 0 is train, 1 is test, 2 is revaled\n","global_train_cache['origin'] = 0\n","date_duration = DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span\n","print(\"date_duration\", date_duration)\n","\n","def update_global_train_cache(df, origin, valid_key: str = 'target'):\n","    global global_train_cache\n","    df['origin'] = origin\n","    print(\"update_global_train_cache\")\n","    global_train_cache = pd.concat([global_train_cache, df], axis=0)\n","    global_train_cache = global_train_cache.dropna(subset=['target'])\n","    global_train_cache = global_train_cache.sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id', 'origin'])\n","    global_train_cache = global_train_cache.drop_duplicates(['date_id', 'seconds_in_bucket', 'stock_id'], keep='last')\n","    global_train_cache.reset_index(drop=True, inplace=True)\n","    global_train_cache = global_train_cache.groupby(['stock_id']).tail(date_duration).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n","\n","    global_train_cache = reduce_mem_usage(global_train_cache, 'global_train_cache')\n","    print(f\"Updated global_train_cache, len: \", len(global_train_cache))\n","    if IS_DEBUG:\n","        print(len(global_train_cache))\n","        if USE_REVEALED_TARGETS:\n","            cdf = global_train_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'origin', 'revealed_target', 'target']]\n","        else:\n","            cdf = global_train_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'origin', 'target']]\n","        print(cdf)\n","\n","def update_models(df, models, features, valid_name):\n","    \"\"\" For Update Model\n","\n","    Args:\n","        df (_type_): _description_\n","        n_splits (_type_): _description_\n","\n","    Returns:\n","        _type_: _description_\n","    \"\"\"\n","\n","    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","    df.reset_index(drop=True, inplace=True)\n","    \n","    for fold, (train_indices, valid_indices) in enumerate(kf.split(df)):\n","        print(f\"{key}: {fold} update\")\n","        now_time = time.time()\n","        X_train, X_valid = df[features].iloc[train_indices], df[features].iloc[valid_indices]\n","        y_train, y_valid = df[valid_name].loc[train_indices], df[valid_name].loc[valid_indices]\n","        trains = lgb.Dataset(X_train, y_train, free_raw_data=False)\n","        valids = lgb.Dataset(X_valid, y_valid, reference=trains)\n","\n","        print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}, y_train: {y_train.shape}, y_valid: {y_valid.shape}\")\n","        models[fold].booster.update(trains)\n","\n","def keep_train_models(df, models, features, valid_name, is_append=False):\n","    print(f\"----------------- keep_train_models, is_append: {is_append}, model len: {len(models)}, df len: {len(df)} ---------------------\")\n","    train_x = df[features]\n","    train_y = df[valid_name]\n","    trains = lgb.Dataset(train_x, train_y, free_raw_data=False)\n","    verbose_eval = -1\n","\n","    counter = 0\n","    r_models = []\n","    if IS_DEBUG:\n","        print(f\"Re-train dataset:\")\n","        if USE_REVEALED_TARGETS:\n","            print(df[['date_id', 'seconds_in_bucket', 'stock_id', 'target', 'revealed_target']])\n","        else:\n","            print(df[['date_id', 'seconds_in_bucket', 'stock_id', 'target']])\n","    for model in models:\n","        print(f\"---- train start, counter: {counter} ----\")\n","        if model.is_latest:\n","            print(\"Update latest model\")\n","            now_time = time.time()\n","            booster = lgb.train(\n","                lgb_params,\n","                trains,\n","                num_boost_round=update_num_boost_round,\n","                keep_training_booster=True,\n","                init_model=model.booster,\n","            )\n","            train_time = time.time() - now_time\n","            updated_model = Model(\n","                booster=booster,\n","                fold=1,\n","                best_iteration=booster.best_iteration, \n","                feature_importance=booster.feature_importance(),\n","                score=-1, \n","                train_time=train_time, \n","                weight=-1, \n","                mem_usage=-1,\n","                is_latest=True,\n","                train_func=\"lightgbm update by test\")\n","            r_models.append(updated_model)\n","            if is_append:\n","                print(\"Adding previous model\")\n","                model.is_latest = False\n","                r_models.append(model)\n","        else:\n","            print(\"Dose not latest, just append\")\n","            r_models.append(model)\n","        counter = counter + 1\n","    print(f\"---- train end, train time: {train_time}, updated model len {len(r_models)} ----\")\n","    return r_models\n","\n","if USE_CONTINUOUS_UPDATE:\n","    try:\n","        print(\"Update model with test date\")\n","        df_test = load_test_dataset()\n","        if IS_MIN_LEARN:\n","            print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n","            df_test = df_test[df_test[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","        df_test = generate_basic_features(df_test)\n","        df_test = generate_enhance_features(df_test)\n","        update_global_train_cache(df_test, 1)\n","        #model_dict[KEY] = keep_train_models(global_train_cache, model_dict[KEY], features, \"target\", is_append=True)\n","        print(\"Update model with test date end\")\n","    except Exception as e:\n","        print(\"Cannot get test date\", e)\n","\n","collect()\n","print(GetMemUsage())\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Key: -1, model len: 2\n","           fold     score  best_iteration  train_time\n","count  2.000000  2.000000             2.0    2.000000\n","mean   0.500000  4.379504             1.0    0.221114\n","std    0.707107  0.023088             0.0    0.028631\n","min    0.000000  4.363178             1.0    0.200868\n","25%    0.250000  4.371341             1.0    0.210991\n","50%    0.500000  4.379504             1.0    0.221114\n","75%    0.750000  4.387667             1.0    0.231236\n","max    1.000000  4.395830             1.0    0.241359\n"]}],"source":["# Show results\n","\n","for key, models in model_dict.items():\n","    print(f\"Key: {key}, model len: {len(models)}\")\n","    data = []\n","    for model in models:\n","        score = model.score\n","        best_iteration = model.best_iteration\n","        fold = model.fold\n","        train_time = model.train_time\n","        data.append({\"key\": key, \"fold\": fold, \"score\": score, \"best_iteration\": best_iteration, \"train_time\": train_time})\n","\n","    df_model = pd.DataFrame(data)\n","    print(df_model.describe())"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fold</th>\n","      <th>score</th>\n","      <th>best_iteration</th>\n","      <th>train_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>2.000000</td>\n","      <td>2.000000</td>\n","      <td>2.0</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.500000</td>\n","      <td>4.379504</td>\n","      <td>1.0</td>\n","      <td>0.221114</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.707107</td>\n","      <td>0.023088</td>\n","      <td>0.0</td>\n","      <td>0.028631</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.000000</td>\n","      <td>4.363178</td>\n","      <td>1.0</td>\n","      <td>0.200868</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.250000</td>\n","      <td>4.371341</td>\n","      <td>1.0</td>\n","      <td>0.210991</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.500000</td>\n","      <td>4.379504</td>\n","      <td>1.0</td>\n","      <td>0.221114</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.750000</td>\n","      <td>4.387667</td>\n","      <td>1.0</td>\n","      <td>0.231236</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.000000</td>\n","      <td>4.395830</td>\n","      <td>1.0</td>\n","      <td>0.241359</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           fold     score  best_iteration  train_time\n","count  2.000000  2.000000             2.0    2.000000\n","mean   0.500000  4.379504             1.0    0.221114\n","std    0.707107  0.023088             0.0    0.028631\n","min    0.000000  4.363178             1.0    0.200868\n","25%    0.250000  4.371341             1.0    0.210991\n","50%    0.500000  4.379504             1.0    0.221114\n","75%    0.750000  4.387667             1.0    0.231236\n","max    1.000000  4.395830             1.0    0.241359"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Check model quality\n","data = []\n","\n","for key, i_models in model_dict.items():\n","    for model in i_models:\n","        score = model.score\n","        best_iteration = model.best_iteration\n","        fold = model.fold\n","        train_time = model.train_time\n","        data.append({\"key\": key, \"fold\": fold, \"score\": score, \"best_iteration\": best_iteration, \"train_time\": train_time})\n","\n","df_model = pd.DataFrame(data)\n","df_model.describe()"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>reference_price_wap_imb</th>\n","      <td>2.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size</th>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size_shift_10</th>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_bid_size_imbalance_size_imb2</th>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_ret_2</th>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_vix_7</th>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_shift_1</th>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>price_spread</th>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>spread_intensity</th>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_ret_3</th>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_shift_10</th>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price</th>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_shift_3</th>\n","      <td>0.5</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_near_price_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_shift_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_diff_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_match_balance_diff_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_diff_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_shift_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_ret_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_bid_price_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_size_diff_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>price_pressure</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size_ret_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_kurt</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price_wap_reference_price_imb2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size_ret_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_mean</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_diff_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_bid_price_wap_imb2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size_shift_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>near_price_ask_price_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_vix_5</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>near_price_bid_price_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag_ret_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_shift_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag_ret_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_ret_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_size_diff_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_size_diff_10</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_vix_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_diff_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_vix_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_diff_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_wap</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_wap_vix_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_vix_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_vix_5</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_match_balance_diff_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_wap_diff_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_wap_diff_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_diff_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_match_balance_vix_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_diff_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_match_balance_diff_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_match_balance_vix_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_vix_5</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_wap_vix_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_vix_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_match_balance</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_wap_diff_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_diff_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_diff_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_vix_5</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_vix_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_diff_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_vix_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_vix_7</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_wap_vix_5</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_wap_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_wap_reference_price_imb2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>stock_id</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price_diff_10</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_size_diff_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>size_imbalance</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_bid_price_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size_ret_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_ask_size_imbalance_size_imb2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_ret_10</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_size_diff_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_diff_10</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_shift_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag_ret_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_bid_size_ask_size_imb2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>mid_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_imbalance</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price_wap_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_near_price_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size_shift_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size_ret_10</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size_shift_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_ask_price_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_size_diff_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>market_urgency</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_skew</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_size</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_size</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>near_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_ret_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_ret_10</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>near_price_wap_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_kurt</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag_shift_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_bid_price_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>seconds_in_bucket</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_far_price_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag_shift_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_size_diff_10</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_size_diff_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_ask_price_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_shift_10</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag_ret_10</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>liquidity_imbalance</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>depth_pressure</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>volume</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag_shift_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_wap_imb</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_momentum</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_size_ask_size_imbalance_size_imb2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_std</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_mean</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag_shift_10</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_std</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price_diff_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price_diff_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_diff_3</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_ret_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price_diff_2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_skew</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_bid_price_reference_price_imb2</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_diff_1</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_match_balance_vix_5</th>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          importance\n","reference_price_wap_imb                          2.0\n","imbalance_size                                   1.0\n","imbalance_size_shift_10                          1.0\n","matched_size_bid_size_imbalance_size_imb2        0.5\n","matched_size_ret_2                               0.5\n","match_balance_vix_7                              0.5\n","reference_price_shift_1                          0.5\n","price_spread                                     0.5\n","spread_intensity                                 0.5\n","reference_price_ret_3                            0.5\n","reference_price_shift_10                         0.5\n","bid_price                                        0.5\n","matched_size_shift_3                             0.5\n","reference_price_near_price_imb                   0.0\n","matched_size_shift_2                             0.0\n","index_mean_match_balance_diff_3                  0.0\n","index_std_match_balance_diff_7                   0.0\n","index_mean_match_balance_diff_1                  0.0\n","matched_size_shift_1                             0.0\n","reference_price_ret_2                            0.0\n","reference_price_bid_price_imb                    0.0\n","bid_size_diff_3                                  0.0\n","price_pressure                                   0.0\n","imbalance_size_ret_2                             0.0\n","all_sizes_kurt                                   0.0\n","bid_price_wap_reference_price_imb2               0.0\n","imbalance_size_ret_1                             0.0\n","all_prices_mean                                  0.0\n","ask_price_diff_2                                 0.0\n","ask_price_bid_price_wap_imb2                     0.0\n","imbalance_size_shift_2                           0.0\n","near_price_ask_price_imb                         0.0\n","index_mean_match_balance_vix_5                   0.0\n","near_price_bid_price_imb                         0.0\n","imbalance_buy_sell_flag_ret_1                    0.0\n","reference_price_shift_3                          0.0\n","imbalance_buy_sell_flag_ret_3                    0.0\n","matched_size_ret_3                               0.0\n","ask_size_diff_1                                  0.0\n","bid_size_diff_10                                 0.0\n","wap_vix_7                                        0.0\n","match_balance_diff_1                             0.0\n","index_mean_match_balance_vix_3                   0.0\n","index_mean_match_balance_diff_7                  0.0\n","index_std_wap                                    0.0\n","index_std_wap_vix_7                              0.0\n","wap_vix_3                                        0.0\n","wap_diff_7                                       0.0\n","index_mean_wap_vix_5                             0.0\n","index_std_match_balance_diff_3                   0.0\n","wap_diff_3                                       0.0\n","index_std_wap_diff_3                             0.0\n","index_std_wap_diff_7                             0.0\n","index_mean_wap_diff_1                            0.0\n","index_std_match_balance_vix_3                    0.0\n","match_balance_diff_7                             0.0\n","index_std_match_balance_diff_1                   0.0\n","index_std_match_balance_vix_7                    0.0\n","wap_vix_5                                        0.0\n","index_std_wap_vix_3                              0.0\n","match_balance_vix_3                              0.0\n","index_mean_match_balance                         0.0\n","index_std_match_balance                          0.0\n","index_std_wap_diff_1                             0.0\n","index_mean_wap_diff_7                            0.0\n","match_balance_diff_3                             0.0\n","match_balance_vix_5                              0.0\n","index_mean_wap                                   0.0\n","index_mean_wap_vix_7                             0.0\n","index_mean_wap_diff_3                            0.0\n","index_mean_wap_vix_3                             0.0\n","index_mean_match_balance_vix_7                   0.0\n","index_std_wap_vix_5                              0.0\n","wap_diff_1                                       0.0\n","far_price_wap_imb                                0.0\n","ask_price_wap_reference_price_imb2               0.0\n","stock_id                                         0.0\n","bid_price_diff_10                                0.0\n","ask_size_diff_2                                  0.0\n","size_imbalance                                   0.0\n","ask_price_bid_price_imb                          0.0\n","imbalance_size_ret_3                             0.0\n","matched_size_ask_size_imbalance_size_imb2        0.0\n","reference_price_ret_10                           0.0\n","ask_size_diff_3                                  0.0\n","ask_price_diff_10                                0.0\n","reference_price_shift_2                          0.0\n","imbalance_buy_sell_flag_ret_2                    0.0\n","matched_size_bid_size_ask_size_imb2              0.0\n","mid_price                                        0.0\n","matched_imbalance                                0.0\n","bid_price_wap_imb                                0.0\n","far_price_near_price_imb                         0.0\n","imbalance_size_shift_3                           0.0\n","imbalance_size_ret_10                            0.0\n","imbalance_size_shift_1                           0.0\n","far_price_ask_price_imb                          0.0\n","bid_size_diff_2                                  0.0\n","market_urgency                                   0.0\n","all_sizes_skew                                   0.0\n","wap                                              0.0\n","ask_size                                         0.0\n","ask_price                                        0.0\n","bid_size                                         0.0\n","near_price                                       0.0\n","far_price                                        0.0\n","matched_size                                     0.0\n","reference_price                                  0.0\n","imbalance_buy_sell_flag                          0.0\n","matched_size_ret_1                               0.0\n","matched_size_ret_10                              0.0\n","near_price_wap_imb                               0.0\n","all_prices_kurt                                  0.0\n","imbalance_buy_sell_flag_shift_3                  0.0\n","far_price_bid_price_imb                          0.0\n","seconds_in_bucket                                0.0\n","reference_price_far_price_imb                    0.0\n","imbalance_buy_sell_flag_shift_2                  0.0\n","ask_size_diff_10                                 0.0\n","bid_size_diff_1                                  0.0\n","reference_price_ask_price_imb                    0.0\n","matched_size_shift_10                            0.0\n","imbalance_buy_sell_flag_ret_10                   0.0\n","liquidity_imbalance                              0.0\n","depth_pressure                                   0.0\n","volume                                           0.0\n","imbalance_buy_sell_flag_shift_1                  0.0\n","ask_price_wap_imb                                0.0\n","imbalance_momentum                               0.0\n","bid_size_ask_size_imbalance_size_imb2            0.0\n","all_prices_std                                   0.0\n","all_sizes_mean                                   0.0\n","imbalance_buy_sell_flag_shift_10                 0.0\n","all_sizes_std                                    0.0\n","bid_price_diff_3                                 0.0\n","bid_price_diff_1                                 0.0\n","ask_price_diff_3                                 0.0\n","reference_price_ret_1                            0.0\n","bid_price_diff_2                                 0.0\n","all_prices_skew                                  0.0\n","ask_price_bid_price_reference_price_imb2         0.0\n","match_balance                                    0.0\n","ask_price_diff_1                                 0.0\n","index_std_match_balance_vix_5                    0.0"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize an empty DataFrame for aggregated importances\n","aggregated_importance = pd.DataFrame(index=features, columns=['importance'])\n","\n","# Aggregate the importances from each model\n","for key, i_models in model_dict.items():\n","    for model in i_models:\n","        importance = pd.DataFrame({'feature': features, 'importance': model.feature_importance})\n","        aggregated_importance = aggregated_importance.add(importance.set_index('feature'), fill_value=0)\n","\n","aggregated_importance['importance'] /= len(df_model)\n","\n","pd_display_max()\n","# Sort the features by importance\n","aggregated_importance = aggregated_importance.sort_values(by='importance', ascending=False)\n","aggregated_importance"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset types"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"data":{"text/plain":["stock_id                                        int8\n","seconds_in_bucket                              int16\n","imbalance_size                               float32\n","imbalance_buy_sell_flag                         int8\n","reference_price                              float32\n","matched_size                                 float32\n","far_price                                    float32\n","near_price                                   float32\n","bid_price                                    float32\n","bid_size                                     float32\n","ask_price                                    float32\n","ask_size                                     float32\n","wap                                          float32\n","spread_intensity                             float32\n","price_spread                                 float32\n","all_sizes_skew                               float32\n","market_urgency                               float32\n","bid_size_diff_2                              float32\n","far_price_ask_price_imb                      float32\n","imbalance_size_shift_1                       float32\n","imbalance_size_shift_3                       float32\n","ask_size_diff_2                              float32\n","far_price_near_price_imb                     float32\n","bid_price_wap_imb                            float32\n","reference_price_shift_1                      float32\n","matched_imbalance                            float32\n","mid_price                                    float32\n","matched_size_bid_size_ask_size_imb2          float32\n","imbalance_buy_sell_flag_ret_2                float32\n","reference_price_shift_2                      float32\n","ask_price_diff_10                            float32\n","ask_size_diff_3                              float32\n","reference_price_ret_10                       float32\n","matched_size_ask_size_imbalance_size_imb2    float32\n","imbalance_size_ret_3                         float32\n","ask_price_bid_price_imb                      float32\n","size_imbalance                               float32\n","matched_size_ret_1                           float32\n","imbalance_size_ret_10                        float32\n","reference_price_wap_imb                      float32\n","matched_size_ret_10                          float32\n","imbalance_momentum                           float32\n","ask_price_diff_1                             float32\n","match_balance                                float32\n","ask_price_bid_price_reference_price_imb2     float32\n","all_prices_skew                              float32\n","bid_price_diff_2                             float32\n","reference_price_ret_1                        float32\n","ask_price_diff_3                             float32\n","bid_price_diff_1                             float32\n","imbalance_size_shift_10                      float32\n","bid_price_diff_3                             float32\n","all_sizes_std                                float32\n","imbalance_buy_sell_flag_shift_10             float32\n","all_sizes_mean                               float32\n","all_prices_std                               float32\n","bid_size_ask_size_imbalance_size_imb2        float32\n","ask_price_wap_imb                            float32\n","all_prices_kurt                              float32\n","imbalance_buy_sell_flag_shift_1              float32\n","reference_price_ret_3                        float32\n","volume                                       float32\n","depth_pressure                               float32\n","liquidity_imbalance                          float32\n","imbalance_buy_sell_flag_ret_10               float32\n","matched_size_shift_10                        float32\n","reference_price_ask_price_imb                float32\n","bid_size_diff_1                              float32\n","matched_size_ret_2                           float32\n","ask_size_diff_10                             float32\n","imbalance_buy_sell_flag_shift_2              float32\n","reference_price_far_price_imb                float32\n","matched_size_bid_size_imbalance_size_imb2    float32\n","far_price_bid_price_imb                      float32\n","imbalance_buy_sell_flag_shift_3              float32\n","near_price_wap_imb                           float32\n","bid_price_diff_10                            float32\n","far_price_wap_imb                            float32\n","ask_price_wap_reference_price_imb2           float32\n","reference_price_shift_10                     float32\n","matched_size_shift_3                         float32\n","bid_size_diff_3                              float32\n","ask_price_diff_2                             float32\n","ask_size_diff_1                              float32\n","reference_price_near_price_imb               float32\n","all_prices_mean                              float32\n","bid_size_diff_10                             float32\n","matched_size_ret_3                           float32\n","imbalance_buy_sell_flag_ret_3                float32\n","reference_price_shift_3                      float32\n","imbalance_buy_sell_flag_ret_1                float32\n","near_price_bid_price_imb                     float32\n","near_price_ask_price_imb                     float32\n","imbalance_size_shift_2                       float32\n","ask_price_bid_price_wap_imb2                 float32\n","imbalance_size_ret_1                         float32\n","matched_size_shift_2                         float32\n","bid_price_wap_reference_price_imb2           float32\n","all_sizes_kurt                               float32\n","imbalance_size_ret_2                         float32\n","price_pressure                               float32\n","reference_price_bid_price_imb                float32\n","reference_price_ret_2                        float32\n","matched_size_shift_1                         float32\n","index_mean_match_balance_diff_1              float32\n","index_std_match_balance_diff_7               float32\n","index_mean_match_balance_diff_3              float32\n","wap_vix_7                                    float32\n","index_mean_match_balance_vix_5               float32\n","index_mean_match_balance_vix_3               float32\n","index_std_wap_diff_3                         float32\n","match_balance_diff_1                         float32\n","wap_diff_1                                   float32\n","match_balance_vix_7                          float32\n","index_std_wap_vix_5                          float32\n","index_mean_match_balance_vix_7               float32\n","index_mean_wap_vix_3                         float32\n","index_mean_wap_diff_3                        float32\n","index_mean_wap_vix_7                         float32\n","index_mean_wap                               float32\n","match_balance_vix_5                          float32\n","match_balance_diff_3                         float32\n","index_mean_wap_diff_7                        float32\n","index_std_wap_diff_1                         float32\n","index_std_match_balance                      float32\n","index_mean_match_balance                     float32\n","match_balance_vix_3                          float32\n","index_std_wap_vix_3                          float32\n","wap_vix_5                                    float32\n","index_std_wap                                float32\n","index_std_wap_vix_7                          float32\n","wap_vix_3                                    float32\n","wap_diff_7                                   float32\n","index_mean_wap_vix_5                         float32\n","index_std_match_balance_diff_3               float32\n","index_mean_match_balance_diff_7              float32\n","wap_diff_3                                   float32\n","index_std_wap_diff_7                         float32\n","index_mean_wap_diff_1                        float32\n","index_std_match_balance_vix_3                float32\n","match_balance_diff_7                         float32\n","index_std_match_balance_diff_1               float32\n","index_std_match_balance_vix_7                float32\n","index_std_match_balance_vix_5                float32\n","dtype: object"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["features_types = df_train[features].dtypes\n","features_types"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["def convert_dtypes(df):\n","    df_types = df[features].dtypes\n","    different_types = [col for col in df_types.index if col in features_types and df_types[col] != features_types[col]]\n","    print(f\"Different Types: {different_types}\")\n","    return different_types\n","\n","def update_dtypes_by_origin(df):\n","    diff_types = convert_dtypes(df)\n","    for col in diff_types:\n","        df[col] = df[col].astype(features_types[col])\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["# Clear trains"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["RAM memory GB usage = 1.501\n"]}],"source":["# Clean up\n","pd_clear_display_max()\n","del key_models\n","if IS_USE_SAVED_MODEL:\n","    print(\"Delete model_dict\")\n","    del model_dict\n","# del df_train\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Infer"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-11-23T00:31:37.373993Z","iopub.status.busy":"2023-11-23T00:31:37.373688Z","iopub.status.idle":"2023-11-23T00:31:38.383908Z","shell.execute_reply":"2023-11-23T00:31:38.382989Z","shell.execute_reply.started":"2023-11-23T00:31:37.373968Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 7 µs, sys: 1e+03 ns, total: 8 µs\n","Wall time: 11 µs\n"]}],"source":["%%time\n","\n","y_min, y_max = -64, 64\n","predictions = []\n","cache = pd.DataFrame()\n","df_result = pd.DataFrame()\n","\n","counter = 0\n","df_revealed_targets = pd.DataFrame()\n","\n","if IS_INFER:\n","    if IS_LOCAL or IS_DEBUG:\n","        print(\"Infer Local\")\n","        env = make_env()\n","    else:\n","        print(\"Infer Submission\")\n","        import optiver2023\n","        env = optiver2023.make_env()\n","    iter_test = env.iter_test()\n","\n","    try:\n","        for (test, revealed_targets, sample_prediction) in iter_test:\n","            now_time = time.time()\n","            print(f\"------- counter {counter} start -------\")\n","\n","            # It faults due to test is iterator\n","            #seconds_in_bucket = test['seconds_in_bucket'][0]\n","            #print(f\"prdict: {test['date_id'][0]}, {seconds_in_bucket}\")\n","\n","            # Generate cahce\n","            cache = pd.concat([cache, test], ignore_index=True, axis=0)\n","            if IS_MIN_LEARN:\n","                print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n","                cache = cache[cache[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","            if counter > 0:\n","                # Clear cache data\n","                cache = cache.groupby(['stock_id']).tail(DATA_COUNT_IN_SAME_BUCKET * continuos_train_span + 1).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n","                if USE_CONTINUOUS_UPDATE:\n","                    cache = cache.drop(['target'], axis=1)\n","                if USE_REVEALED_TARGETS:\n","                    cache = cache.drop(['revealed_target'], axis=1)\n","            print(f\"cache len {len(cache)}\")\n","\n","            # Add revealed target as target for counituous update\n","            copy_revealed_targets = revealed_targets.copy()\n","            copy_revealed_targets = copy_revealed_targets.dropna()\n","            print(\"copy_revealed_targets len\", len(copy_revealed_targets))\n","\n","            if len(copy_revealed_targets) > 0:\n","                print(\"Update revealed_targets\")\n","                copy_revealed_targets['revealed_date_id'] = copy_revealed_targets['revealed_date_id'].astype(int).astype(str)\n","                copy_revealed_targets['date_id'] = copy_revealed_targets['date_id'].astype(int).astype(str)\n","                copy_revealed_targets['seconds_in_bucket'] = copy_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n","                copy_revealed_targets['stock_id'] = copy_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n","                copy_revealed_targets['revealed_row_id'] = copy_revealed_targets['revealed_date_id'] + '_' + copy_revealed_targets['seconds_in_bucket'] + '_' + copy_revealed_targets['stock_id']\n","                copy_revealed_targets['row_id'] = copy_revealed_targets['date_id'] + '_' + copy_revealed_targets['seconds_in_bucket'] + '_' + copy_revealed_targets['stock_id']\n","                copy_revealed_targets['revealed_target'] = copy_revealed_targets['revealed_target'].astype('float32')\n","\n","                df_revealed_targets = pd.concat([df_revealed_targets, copy_revealed_targets], ignore_index=True, axis=0)\n","                # 余分なデータを削除する、範囲はcacheの2倍必要\n","                df_revealed_targets = df_revealed_targets.groupby(['stock_id']).tail((DATA_COUNT_IN_SAME_BUCKET * continuos_train_span + 1) * 3).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n","                df_revealed_targets = reduce_mem_usage(df_revealed_targets, 'df_revealed_targets')\n","\n","            # USE_CONTINUOUS_UPDATEが有効の時、cacheのrow_idとdf_revealed_targetsのrevealed_row_idをleft joinする\n","            if USE_CONTINUOUS_UPDATE:\n","                df_r = df_revealed_targets[['revealed_row_id', 'revealed_target']]\n","                df_r.rename(columns={'revealed_target': 'target'}, inplace=True)\n","                df_r.rename(columns={'revealed_row_id': 'row_id'}, inplace=True)\n","                cache = pd.merge(cache, df_r, how='left', on='row_id')\n","\n","\n","            # USE_REVEALED_TARGETSが有効の時、cacheのrow_idとdf_revealed_targetsのrow_idをleft joinする\n","            if USE_REVEALED_TARGETS:\n","                df_r = df_revealed_targets[['row_id', 'revealed_target']]\n","                cache = pd.merge(cache, df_r, how='left', on='row_id')\n","\n","            cache = reduce_mem_usage(cache, 'cache')\n","\n","            # Generate features\n","            df_valid = generate_basic_features(cache)\n","            df_valid = generate_enhance_features(df_valid)\n","            df_valid = reduce_mem_usage(df_valid, 'df_valid')\n","\n","            # Update model\n","            if (counter > 0) and (counter % (DATA_COUNT_IN_SAME_BUCKET * continuos_train_span) == 0) and USE_CONTINUOUS_UPDATE:\n","                train_now_time = time.time()\n","                print(\"Update model with revealed_target date start\")\n","                print(f\"df_valid len {len(df_valid)}\")\n","                df_train = df_valid.copy()\n","                df_train.dropna(subset=['target'], inplace=True)\n","                df_train = update_dtypes_by_origin(df_train)\n","                df_train.reset_index(drop=True, inplace=True)\n","\n","                update_global_train_cache(df_train, 2)\n","                model_dict[KEY] = keep_train_models(global_train_cache, model_dict[KEY], features, \"target\", True)\n","                print(f\"ReTrain Time: {time.time() - train_now_time}\")\n","                del df_train\n","\n","            if not IS_MIN_LEARN:\n","                # stockの数分取得する\n","                df_valid = df_valid[-len(test):].reset_index(drop=True)\n","            else:\n","                df_valid = df_valid[-len(TARGET_STOCK_IDS):].reset_index(drop=True)\n","\n","            # Get seconds_in_bucket and date_id\n","            seconds_in_bucket = df_test['seconds_in_bucket'][0]\n","            date_id = df_test['date_id'][0]\n","            print(f\"Predict: {date_id}, {seconds_in_bucket}\")\n","\n","            # Predict\n","            predictions = model_infer(KEY, df_valid[features])\n","            df_valid['pred'] = predictions\n","            scaled_predictions = zero_clip(df_valid, predictions)\n","            df_valid['scaled_pred'] = scaled_predictions\n","\n","            # For save\n","            if IS_DEBUG:\n","                df_result = pd.concat([df_result, df_valid], ignore_index=True, axis=0)\n","\n","            # Submit\n","            if not IS_MIN_LEARN:\n","                print(\"Submit prediction\")\n","                sample_prediction['target'] = scaled_predictions\n","                env.predict(sample_prediction)\n","            else:\n","                print(\"Submit dummy prediction\")\n","                sample_prediction['target'] = 0\n","                env.predict(sample_prediction)\n","\n","            # Clean up\n","            execution_time = time.time() - now_time\n","            del df_valid, predictions, scaled_predictions\n","            collect()\n","            print(GetMemUsage())\n","            print(f\"------- counter {counter}, execution_time {execution_time} end -------\")\n","            counter += 1\n","    except Exception as e:\n","        print(\"Error\", e)\n","\n","collect()\n","print(GetMemUsage())\n","if IS_DEBUG:\n","    df_result.to_csv(f\"{BASE_OUTPUT_PATH}/result.csv\", index=False)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Infer Local\n","------- counter 0 start -------\n","MIN LEARN MODE : [0]\n","cache len 1\n","copy_revealed_targets len 11000\n","Update revealed_targets\n","Memory usage of df_revealed_targets is 0.71 MB\n","Memory usage after optimization is: 0.61 MB\n","Decreased by 14.70%\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 27.65%\n","Memory usage of generate_basic_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 10.34%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 9.15%\n","Memory usage of df_valid is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.05417345]\n","Submit dummy prediction\n","------- counter 0, execution_time 1.053267002105713 end -------\n","------- counter 1 start -------\n","MIN LEARN MODE : [0]\n","cache len 2\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 19.81%\n","Memory usage of generate_basic_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 11.57%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 9.99%\n","Memory usage of df_valid is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 1, execution_time 1.1792020797729492 end -------\n","------- counter 2 start -------\n","MIN LEARN MODE : [0]\n","cache len 3\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 20.67%\n","Memory usage of generate_basic_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 12.05%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 10.30%\n","Memory usage of df_valid is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 2, execution_time 0.5385410785675049 end -------\n","------- counter 3 start -------\n","MIN LEARN MODE : [0]\n","cache len 4\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 21.13%\n","Memory usage of generate_basic_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 12.31%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 10.47%\n","Memory usage of df_valid is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 3, execution_time 0.5426568984985352 end -------\n","------- counter 4 start -------\n","MIN LEARN MODE : [0]\n","cache len 5\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 21.41%\n","Memory usage of generate_basic_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 12.47%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 10.57%\n","Memory usage of df_valid is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 4, execution_time 0.6478409767150879 end -------\n","------- counter 5 start -------\n","MIN LEARN MODE : [0]\n","cache len 6\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 21.60%\n","Memory usage of generate_basic_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 12.57%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 10.64%\n","Memory usage of df_valid is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 5, execution_time 0.7447307109832764 end -------\n","------- counter 6 start -------\n","MIN LEARN MODE : [0]\n","cache len 7\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 21.75%\n","Memory usage of generate_basic_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 12.65%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 10.68%\n","Memory usage of df_valid is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 6, execution_time 0.5581209659576416 end -------\n","------- counter 7 start -------\n","MIN LEARN MODE : [0]\n","cache len 8\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 21.85%\n","Memory usage of generate_basic_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 12.71%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 10.72%\n","Memory usage of df_valid is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 7, execution_time 0.6479129791259766 end -------\n","------- counter 8 start -------\n","MIN LEARN MODE : [0]\n","cache len 9\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 21.94%\n","Memory usage of generate_basic_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 12.76%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.75%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 8, execution_time 0.6915609836578369 end -------\n","------- counter 9 start -------\n","MIN LEARN MODE : [0]\n","cache len 10\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 22.00%\n","Memory usage of generate_basic_features is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 12.79%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.78%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 9, execution_time 0.6633470058441162 end -------\n","------- counter 10 start -------\n","MIN LEARN MODE : [0]\n","cache len 11\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.00 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 22.06%\n","Memory usage of generate_basic_features is 0.01 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 12.83%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.79%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 10, execution_time 0.5497438907623291 end -------\n","------- counter 11 start -------\n","MIN LEARN MODE : [0]\n","cache len 12\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.01 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 22.11%\n","Memory usage of generate_basic_features is 0.01 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 12.85%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.81%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 11, execution_time 0.5994291305541992 end -------\n","------- counter 12 start -------\n","MIN LEARN MODE : [0]\n","cache len 13\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.01 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 22.14%\n","Memory usage of generate_basic_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 12.87%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.82%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 12, execution_time 0.7139182090759277 end -------\n","------- counter 13 start -------\n","MIN LEARN MODE : [0]\n","cache len 14\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.01 MB\n","Memory usage after optimization is: 0.00 MB\n","Decreased by 21.97%\n","Memory usage of generate_basic_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 12.87%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.82%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 13, execution_time 0.5641751289367676 end -------\n","------- counter 14 start -------\n","MIN LEARN MODE : [0]\n","cache len 15\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 22.00%\n","Memory usage of generate_basic_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 12.88%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.83%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 14, execution_time 0.5332069396972656 end -------\n","------- counter 15 start -------\n","MIN LEARN MODE : [0]\n","cache len 16\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 22.02%\n","Memory usage of generate_basic_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 12.90%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.84%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 15, execution_time 0.5528860092163086 end -------\n","------- counter 16 start -------\n","MIN LEARN MODE : [0]\n","cache len 17\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 22.04%\n","Memory usage of generate_basic_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 12.91%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.85%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 16, execution_time 0.626929759979248 end -------\n","------- counter 17 start -------\n","MIN LEARN MODE : [0]\n","cache len 18\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 22.07%\n","Memory usage of generate_basic_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 12.92%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.85%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 17, execution_time 0.7295138835906982 end -------\n","------- counter 18 start -------\n","MIN LEARN MODE : [0]\n","cache len 19\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 22.08%\n","Memory usage of generate_basic_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 12.93%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.86%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 18, execution_time 0.5347099304199219 end -------\n","------- counter 19 start -------\n","MIN LEARN MODE : [0]\n","cache len 20\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 22.10%\n","Memory usage of generate_basic_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 12.94%\n","Use index\n","generate_index_features\n","generate_historical_features\n","Memory usage of generate_enhance_features is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 10.87%\n","Memory usage of df_valid is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 0.00%\n","Predict: 478, 0\n","Predictor target models len 2\n","std_predictions [0.07169763]\n","Submit dummy prediction\n","------- counter 19, execution_time 0.5456669330596924 end -------\n","------- counter 20 start -------\n","MIN LEARN MODE : [0]\n","cache len 21\n","copy_revealed_targets len 0\n","Memory usage of cache is 0.01 MB\n","Memory usage after optimization is: 0.01 MB\n","Decreased by 22.11%\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<timed exec>:77\u001b[0m\n","Cell \u001b[0;32mIn[7], line 87\u001b[0m, in \u001b[0;36mgenerate_basic_features\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     84\u001b[0m df \u001b[39m=\u001b[39m imbalance_features(df)\n\u001b[1;32m     85\u001b[0m df \u001b[39m=\u001b[39m numba_imb_features(df)\n\u001b[0;32m---> 87\u001b[0m df \u001b[39m=\u001b[39m reduce_mem_usage(df, \u001b[39m\"\u001b[39;49m\u001b[39mgenerate_basic_features\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     89\u001b[0m collect()  \u001b[39m# Perform garbage collection to free up memory\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39mreturn\u001b[39;00m df\n","Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mreduce_mem_usage\u001b[0;34m(df, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m     df[col] \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     34\u001b[0m \u001b[39melif\u001b[39;00m c_min \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mfinfo(np\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mmin \u001b[39mand\u001b[39;00m c_max \u001b[39m<\u001b[39m np\u001b[39m.\u001b[39mfinfo(np\u001b[39m.\u001b[39mfloat32)\u001b[39m.\u001b[39mmax:\n\u001b[0;32m---> 35\u001b[0m     df[col] \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39;49mastype(np\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m     36\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     37\u001b[0m     df[col] \u001b[39m=\u001b[39m df[col]\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/generic.py:6534\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6530\u001b[0m     results \u001b[39m=\u001b[39m [ser\u001b[39m.\u001b[39mastype(dtype, copy\u001b[39m=\u001b[39mcopy) \u001b[39mfor\u001b[39;00m _, ser \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m   6532\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6533\u001b[0m     \u001b[39m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6534\u001b[0m     new_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49mastype(dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   6535\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[39m=\u001b[39mnew_data\u001b[39m.\u001b[39maxes)\n\u001b[1;32m   6536\u001b[0m     \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mastype\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/managers.py:414\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    412\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 414\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m    415\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mastype\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    416\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m    417\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    418\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    419\u001b[0m     using_cow\u001b[39m=\u001b[39;49musing_copy_on_write(),\n\u001b[1;32m    420\u001b[0m )\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/managers.py:354\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m         applied \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39mapply(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    353\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m         applied \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(b, f)(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    355\u001b[0m     result_blocks \u001b[39m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    357\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mfrom_blocks(result_blocks, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes)\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/internals/blocks.py:616\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[39mCoerce to the new dtype.\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[39mBlock\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    614\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues\n\u001b[0;32m--> 616\u001b[0m new_values \u001b[39m=\u001b[39m astype_array_safe(values, dtype, copy\u001b[39m=\u001b[39;49mcopy, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m    618\u001b[0m new_values \u001b[39m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    620\u001b[0m refs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:238\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     dtype \u001b[39m=\u001b[39m dtype\u001b[39m.\u001b[39mnumpy_dtype\n\u001b[1;32m    237\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 238\u001b[0m     new_values \u001b[39m=\u001b[39m astype_array(values, dtype, copy\u001b[39m=\u001b[39;49mcopy)\n\u001b[1;32m    239\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    240\u001b[0m     \u001b[39m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[39m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n","File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:175\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtype:\n\u001b[1;32m    174\u001b[0m     \u001b[39mif\u001b[39;00m copy:\n\u001b[0;32m--> 175\u001b[0m         \u001b[39mreturn\u001b[39;00m values\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    176\u001b[0m     \u001b[39mreturn\u001b[39;00m values\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(values, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    179\u001b[0m     \u001b[39m# i.e. ExtensionArray\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["%%time\n","\n","y_min, y_max = -64, 64\n","predictions = []\n","cache = pd.DataFrame()\n","df_result = pd.DataFrame()\n","\n","counter = 0\n","df_revealed_targets = pd.DataFrame()\n","\n","if IS_INFER:\n","    if IS_LOCAL:\n","        print(\"Infer Local\")\n","        env = make_env()\n","    else:\n","        print(\"Infer Submission\")\n","        import optiver2023\n","        env = optiver2023.make_env()\n","    iter_test = env.iter_test()\n","\n","    for (test, revealed_targets, sample_prediction) in iter_test:\n","        now_time = time.time()\n","        print(f\"------- counter {counter} start -------\")\n","\n","        # It faults due to test is iterator\n","        #seconds_in_bucket = test['seconds_in_bucket'][0]\n","        #print(f\"prdict: {test['date_id'][0]}, {seconds_in_bucket}\")\n","\n","        # Generate cahce\n","        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n","        if IS_MIN_LEARN:\n","            print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n","            cache = cache[cache[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","        if counter > 0:\n","            # Clear cache data\n","            cache = cache.groupby(['stock_id']).tail(DATA_COUNT_IN_SAME_BUCKET * continuos_train_span + 1).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n","            if USE_CONTINUOUS_UPDATE:\n","                cache = cache.drop(['target'], axis=1)\n","            if USE_REVEALED_TARGETS:\n","                cache = cache.drop(['revealed_target'], axis=1)\n","        print(f\"cache len {len(cache)}\")\n","\n","        # Add revealed target as target for counituous update\n","        copy_revealed_targets = revealed_targets.copy()\n","        copy_revealed_targets = copy_revealed_targets.dropna()\n","        print(\"copy_revealed_targets len\", len(copy_revealed_targets))\n","        \n","        if len(copy_revealed_targets) > 0:\n","            print(\"Update revealed_targets\")\n","            copy_revealed_targets['revealed_date_id'] = copy_revealed_targets['revealed_date_id'].astype(int).astype(str)\n","            copy_revealed_targets['date_id'] = copy_revealed_targets['date_id'].astype(int).astype(str)\n","            copy_revealed_targets['seconds_in_bucket'] = copy_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n","            copy_revealed_targets['stock_id'] = copy_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n","            copy_revealed_targets['revealed_row_id'] = copy_revealed_targets['revealed_date_id'] + '_' + copy_revealed_targets['seconds_in_bucket'] + '_' + copy_revealed_targets['stock_id']\n","            copy_revealed_targets['row_id'] = copy_revealed_targets['date_id'] + '_' + copy_revealed_targets['seconds_in_bucket'] + '_' + copy_revealed_targets['stock_id']\n","            copy_revealed_targets['revealed_target'] = copy_revealed_targets['revealed_target'].astype('float32')\n","\n","            df_revealed_targets = pd.concat([df_revealed_targets, copy_revealed_targets], ignore_index=True, axis=0)\n","            # 余分なデータを削除する、範囲はcacheの2倍必要\n","            df_revealed_targets = df_revealed_targets.groupby(['stock_id']).tail((DATA_COUNT_IN_SAME_BUCKET * continuos_train_span + 1) * 3).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n","            df_revealed_targets = reduce_mem_usage(df_revealed_targets, 'df_revealed_targets')\n","\n","        # USE_CONTINUOUS_UPDATEが有効の時、cacheのrow_idとdf_revealed_targetsのrevealed_row_idをleft joinする\n","        if USE_CONTINUOUS_UPDATE:\n","            df_r = df_revealed_targets[['revealed_row_id', 'revealed_target']]\n","            df_r.rename(columns={'revealed_target': 'target'}, inplace=True)\n","            df_r.rename(columns={'revealed_row_id': 'row_id'}, inplace=True)\n","            cache = pd.merge(cache, df_r, how='left', on='row_id')\n","\n","            \n","        # USE_REVEALED_TARGETSが有効の時、cacheのrow_idとdf_revealed_targetsのrow_idをleft joinする\n","        if USE_REVEALED_TARGETS:\n","            df_r = df_revealed_targets[['row_id', 'revealed_target']]\n","            cache = pd.merge(cache, df_r, how='left', on='row_id')\n","\n","        cache = reduce_mem_usage(cache, 'cache')\n","\n","        # Generate features\n","        df_valid = generate_basic_features(cache)\n","        df_valid = generate_enhance_features(df_valid)\n","        df_valid = reduce_mem_usage(df_valid, 'df_valid')\n","\n","        # Update model\n","        if (counter > 0) and (counter % (DATA_COUNT_IN_SAME_BUCKET * continuos_train_span) == 0) and USE_CONTINUOUS_UPDATE:\n","            train_now_time = time.time()\n","            print(\"Update model with revealed_target date start\")\n","            print(f\"df_valid len {len(df_valid)}\")\n","            df_train = df_valid.copy()\n","            df_train.dropna(subset=['target'], inplace=True)\n","            df_train = update_dtypes_by_origin(df_train)\n","            df_train.reset_index(drop=True, inplace=True)\n","\n","            update_global_train_cache(df_train, 2)\n","            model_dict[KEY] = keep_train_models(global_train_cache, model_dict[KEY], features, \"target\", True)\n","            print(f\"ReTrain Time: {time.time() - train_now_time}\")\n","            del df_train\n","\n","        if not IS_MIN_LEARN:\n","            # stockの数分取得する\n","            df_valid = df_valid[-len(test):].reset_index(drop=True)\n","        else:\n","            df_valid = df_valid[-len(TARGET_STOCK_IDS):].reset_index(drop=True)\n","\n","        # Get seconds_in_bucket and date_id\n","        seconds_in_bucket = df_test['seconds_in_bucket'][0]\n","        date_id = df_test['date_id'][0]\n","        print(f\"Predict: {date_id}, {seconds_in_bucket}\")\n","        \n","        # Predict\n","        predictions = model_infer(KEY, df_valid[features])\n","        df_valid['pred'] = predictions\n","        scaled_predictions = zero_clip(df_valid, predictions)\n","        df_valid['scaled_pred'] = scaled_predictions\n","\n","        # For save\n","        if IS_DEBUG:\n","            df_result = pd.concat([df_result, df_valid], ignore_index=True, axis=0)\n","            \n","        # Submit\n","        if not IS_MIN_LEARN:\n","            print(\"Submit prediction\")\n","            sample_prediction['target'] = scaled_predictions\n","            env.predict(sample_prediction)\n","        else:\n","            print(\"Submit dummy prediction\")\n","            sample_prediction['target'] = 0\n","            env.predict(sample_prediction)\n","\n","        # Clean up\n","        execution_time = time.time() - now_time\n","        print(f\"------- counter {counter}, execution_time {execution_time} end -------\")\n","        del df_valid, predictions, scaled_predictions\n","        collect()\n","        counter += 1\n","\n","collect()\n","print(GetMemUsage())\n","if IS_DEBUG:\n","    df_result.to_csv(f\"{BASE_OUTPUT_PATH}/result.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if IS_DEBUG:\n","    df_revealed_targets = pd.read_csv(REVEALED_TARGETS_FILE)\n","    df_revealed_targets = df_revealed_targets.dropna(subset=['revealed_date_id', 'seconds_in_bucket', 'stock_id'])\n","    df_revealed_targets['revealed_date_id'] = df_revealed_targets['revealed_date_id'].astype(int).astype(str)\n","    df_revealed_targets['seconds_in_bucket'] = df_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n","    df_revealed_targets['stock_id'] = df_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n","\n","    # Concatenate the columns\n","    df_revealed_targets['row_id'] = df_revealed_targets['revealed_date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n","    \n","    df_pred = df_result[['row_id', 'scaled_pred']]\n","    df = pd.merge(df_pred, df_revealed_targets, how='left', on='row_id')\n","\n","    df = df.rename(columns={'revealed_target': 'target'})\n","    df = df.dropna(subset=['target'])\n","    #df['score'] = (df['scaled_pred'] - df['target']).abs()\n","\n","    df['score'] = mean_absolute_error(df['scaled_pred'], df['target'])\n","    df = df[['row_id', 'scaled_pred', 'target', 'score']]\n","    print(df.describe())\n","\n","    print(\"  \")\n","    print(df)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7056235,"sourceId":57891,"sourceType":"competition"}],"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
