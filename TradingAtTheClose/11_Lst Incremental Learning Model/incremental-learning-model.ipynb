{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Incremental Learning Model\n","This origin comming from https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model/notebook"]},{"cell_type":"markdown","metadata":{},"source":["# Init"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":353,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:04.714475Z","iopub.status.busy":"2023-12-15T11:18:04.713507Z","iopub.status.idle":"2023-12-15T11:18:04.721276Z","shell.execute_reply":"2023-12-15T11:18:04.720367Z","shell.execute_reply.started":"2023-12-15T11:18:04.714439Z"},"trusted":true},"outputs":[],"source":["from pathlib import Path\n","import os\n","import warnings\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import gc  # Garbage collection for memory management\n","import os  # Operating system-related functions\n","import time  # Time-related functions\n","import warnings  # Handling warnings\n","from itertools import combinations  # For creating combinations of elements\n","from warnings import simplefilter  # Simplifying warning handling\n","import joblib  # For saving and loading models\n","import numpy as np  # Numerical operations\n","import pandas as pd  # Data manipulation and analysis\n","from sklearn.metrics import mean_absolute_error  # Metric for evaluation\n","from sklearn.model_selection import KFold, TimeSeriesSplit  # Cross-validation techniques\n","from concurrent.futures import ThreadPoolExecutor\n","from numba import njit, prange  # Compiling Python code for performance"]},{"cell_type":"markdown","metadata":{},"source":["## Global params"]},{"cell_type":"code","execution_count":354,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:04.723350Z","iopub.status.busy":"2023-12-15T11:18:04.723059Z","iopub.status.idle":"2023-12-15T11:18:04.747414Z","shell.execute_reply":"2023-12-15T11:18:04.746461Z","shell.execute_reply.started":"2023-12-15T11:18:04.723327Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["BASE_OUTPUT_PATH: ../output\n","BASE_INPUT_PATH: ../kaggle/input/optiver-trading-at-the-close\n","TRAIN_FILE: ../kaggle/input/optiver-trading-at-the-close/train.csv\n","TEST_FILE: ../kaggle/input/optiver-trading-at-the-close/test.csv\n","IS_LOCAL: True\n","IS_INFER: True\n","IS_USE_SAVED_MODEL: False\n","IS_MIN_LEARN: True\n","USE_OPTUNA: False\n","USE_CONTINUOUS_UPDATE: True\n","USE_ALL_FEATUTES: True\n","USE_REVEALED_TARGETS: False\n","USE_INDEX: True\n"]}],"source":["# Disable warnings to keep the code clean\n","warnings.filterwarnings(\"ignore\")\n","simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n","\n","warnings.filterwarnings(\"ignore\")\n","\n","seed = 2023\n","DATA_COUNT_IN_SAME_BUCKET = 55 # 同じbucket内のデータ数\n","\n","# For kaggle environment\n","if os.environ.get(\"KAGGLE_DATA_PROXY_TOKEN\") != None:\n","    BASE_OUTPUT_PATH = Path(f'/kaggle/working')\n","    BASE_INPUT_PATH = Path(f'/kaggle/input/optiver-trading-at-the-close')\n","    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n","    TEST_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/test.csv')\n","    \n","    IS_LOCAL = False # If kaggle environment, set False\n","    IS_INFER = True # If kaggle environment, set True\n","    IS_USE_SAVED_MODEL = False # Use saved model or not\n","    IS_MIN_LEARN = False # Use min learning or not\n","    USE_OPTUNA = False # Use optuna or not\n","    USE_CONTINUOUS_UPDATE = True # Use test date on train or not\n","    USE_ALL_FEATUTES = True # Use all features or not\n","    USE_REVEALED_TARGETS = True # Use revealed targets or not\n","    USE_INDEX = True # Use index or not\n","    IS_DEBUG = False\n","    USE_ADDITIONAL_TRAIN = True # 追加学習を行うかどうか\n","    USE_CUSTOME_FOLD = True\n","\n","    NUM_THREADS = 4\n","\n","    SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/sample_submission.csv')\n","    REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/example_test_files/revealed_targets.csv')\n","\n","    stopping_rounds = 10 # early_stopping用コールバック関数\n","    num_boost_round = 100 # 計算回数\n","    update_num_boost_round = 50 # 再学習の計算回数\n","    num_folds = 3 # クロスバリデーションの分割数\n","    ADDITIONAL_TRAIN_THRESHOLD = 5 # x以上のデータを追加学習する\n","    MODEL_NUM = 2 # モデルの数、クロスバリデーションで学習されたモデルのうち、最も良いモデルから使用する、NUM_THREADSと同じかそれ以下の値にすること\n","\n","    continuos_dataset_span = 20 # DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span が更新の使用する対象のデータ\n","    continuos_train_span = 5 # DATA_COUNT_IN_SAME_BUCKET * continuos_train_span が更新の頻度\n","\n","    DEVICE = 'gpu' # cpu or gpu\n","    OPTUNA_TIME_BUDGET = 60 * 60 * 4 # 1 hours\n","    TARGET_STOCK_IDS = [0, 1]\n","\n","    optuna_params = {\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression',         # 回帰\n","        'metric': 'rmse',                  # 損失（誤差）\n","        'verbosity': -1,\n","        'deterministic':True, #再現性確保用のパラメータ\n","        'force_row_wise':True,  #再現性確保用のパラメータ\n","        'device': DEVICE\n","    }\n","\n","    # サンプルのパラメータ\n","    lgb_params = {\n","        \"objective\": \"mae\",\n","        \"n_estimators\": 3000,\n","        \"num_leaves\": 256,\n","        \"subsample\": 0.6,\n","        \"colsample_bytree\": 0.8,\n","        \"learning_rate\": 0.015, #0.00871,\n","        'max_depth': 11,\n","        \"n_jobs\": 4,\n","        \"verbosity\": -1,\n","        \"importance_type\": \"gain\",\n","        \"device\": DEVICE,\n","    }\n","    \n","    \"\"\" \n","    # こっちのパラメータの方が、計算時間がかかる\n","    lgb_params = {\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression', \n","        'metric': 'rmse', \n","        'verbosity': -1, \n","        'device': DEVICE,\n","        'feature_pre_filter': False, \n","        'lambda_l1': 0.0,\n","        'lambda_l2': 0.0,\n","        'num_leaves': 31, \n","        'feature_fraction': 0.8, \n","        'bagging_fraction': 1.0, \n","        'bagging_freq': 0, \n","        'min_child_samples': 20,\n","        'seed': seed,                       # シード値\n","    }\n","    \"\"\"\n","\n","    # 計算が早い\n","    \"\"\"\n","    lgb_params = {\n","        'task': 'train',                   # 学習\n","        'objective': 'regression',                # 目的関数の種類。ここでは回帰タスクを指定\n","        'metric': 'rmse',                          # 評価指標\n","        'boosting_type': 'gbdt',                  # ブースティングタイプ。勾配ブースティング決定木\n","        \"n_estimators\": 32,                        # ブースティングに使用する木の数。多いほど性能が向上するが計算コストが増加\n","        \"num_leaves\": 64,                         # 木に存在する最大の葉の数。大きい値は精度を向上させるが過学習のリスクが増加\n","        \"subsample\": 0.8,                         # 各木のトレーニングに使用されるデータの割合。過学習を防ぐために一部のデータをサンプリング\n","        \"colsample_bytree\": 0.8,                  # 木を構築する際に使用される特徴の割合。特徴のサブセットを使用し過学習を防ぐ\n","        \"learning_rate\": 0.01,                 # 学習率。小さい値は堅牢なモデルを生成するが収束に時間がかかる\n","        'max_depth': 32,                           # 木の最大の深さ。深い木は複雑なモデルを作成するが過学習のリスクがある\n","        \"device\": DEVICE,                         # トレーニングに使用するデバイス（CPUまたはGPU）\n","        \"verbosity\": -1,                          # LightGBMのログ出力のレベル。-1はログを出力しないことを意味する\n","       # \"importance_type\": \"gain\",                # 特徴重要度を計算する際の指標。\"gain\"は分割による平均情報利得\n","        'lambda_l1': 0.5,                         # L1正則化項の係数。過学習を防ぐためにモデルの複雑さにペナルティを課す\n","        'lambda_l2': 0.5,                         # L2正則化項の係数。同じく過学習を防ぐ\n","        'bagging_freq': 5,                 # バギング実施頻度\n","        'min_child_samples': 10,           # 葉に含まれる最小データ数\n","        'seed': seed,                       # シード値\n","    }\n","    \"\"\"\n","\n","# For local environment\n","else:\n","    BASE_OUTPUT_PATH = Path(f'../output')\n","    BASE_INPUT_PATH = Path(f'../kaggle/input/optiver-trading-at-the-close')\n","    TRAIN_FILE = Path(f'{BASE_INPUT_PATH}/train.csv')\n","    TEST_FILE = Path(f'{BASE_INPUT_PATH}/test.csv')\n","\n","    SAMPLE_SUBMISSION_FILE = Path(f'{BASE_INPUT_PATH}/sample_submission.csv')\n","    REVEALED_TARGETS_FILE = Path(f'{BASE_INPUT_PATH}/revealed_targets.csv')\n","\n","    IS_LOCAL = True\n","    IS_INFER = True\n","    IS_USE_SAVED_MODEL = False # Use saved model or not\n","    IS_MIN_LEARN = True\n","    USE_OPTUNA = False # Use optuna or not\n","    USE_ALL_FEATUTES = True # Use all features or not\n","    USE_CONTINUOUS_UPDATE = True # Use test date on train or not\n","    USE_REVEALED_TARGETS = False # Use revealed targets or not \n","    USE_INDEX = True # Use index or not\n","    IS_DEBUG = True\n","    USE_ADDITIONAL_TRAIN = True\n","    USE_CUSTOME_FOLD = True\n","    \n","    TARGET_STOCK_IDS = [0]\n","    NUM_THREADS = 2\n","    ADDITIONAL_TRAIN_THRESHOLD = 5 # x以上のデータを追加学習する\n","    MODEL_NUM = 1 # モデルの数、クロスバリデーションで学習されたモデルのうち、最も良いモデルから使用する、NUM_THREADSと同じかそれ以下の値にすること\n","\n","    # For training\n","    stopping_rounds = 1 # early_stopping用コールバック関数\n","    num_boost_round = 1 # 計算回数\n","    update_num_boost_round = 1\n","    num_folds = 2 # クロスバリデーションの分割数\n","    continuos_dataset_span = 3 # DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span が更新の使用する対象のデータ\n","    continuos_train_span = 2 # DATA_COUNT_IN_SAME_BUCKET * continuos_train_span が更新の頻度\n","\n","    DEVICE = 'cpu' # cpu or gpu\n","    OPTUNA_TIME_BUDGET = 60 # 1 min\n","\n","    optuna_params = {\n","        'boosting_type': 'gbdt',\n","        'objective': 'regression',         # 回帰\n","        'metric': 'rmse',                  # 損失（誤差）\n","        'verbosity': -1,\n","        'deterministic':True, #再現性確保用のパラメータ\n","        'force_row_wise':True,  #再現性確保用のパラメータ\n","        'device': DEVICE\n","    }\n","\n","    lgb_params = {\n","        \"objective\": \"mae\",\n","        \"n_estimators\": 6000,\n","        \"num_leaves\": 256,\n","        \"subsample\": 0.6,\n","        \"colsample_bytree\": 0.8,\n","        \"learning_rate\": 0.2,\n","        'max_depth': 11,\n","        \"n_jobs\": 4,\n","        \"verbosity\": -1,\n","        \"importance_type\": \"gain\",\n","        \"device\": DEVICE,\n","    }\n","\n","\n","print(f\"BASE_OUTPUT_PATH: {BASE_OUTPUT_PATH}\")\n","print(f\"BASE_INPUT_PATH: {BASE_INPUT_PATH}\")\n","print(f\"TRAIN_FILE: {TRAIN_FILE}\")\n","print(f\"TEST_FILE: {TEST_FILE}\")\n","print(f\"IS_LOCAL: {IS_LOCAL}\")\n","print(f\"IS_INFER: {IS_INFER}\")\n","print(f\"IS_USE_SAVED_MODEL: {IS_USE_SAVED_MODEL}\")\n","print(f\"IS_MIN_LEARN: {IS_MIN_LEARN}\")\n","print(f\"USE_OPTUNA: {USE_OPTUNA}\")\n","print(f\"USE_CONTINUOUS_UPDATE: {USE_CONTINUOUS_UPDATE}\")\n","print(f\"USE_ALL_FEATUTES: {USE_ALL_FEATUTES}\")\n","print(f\"USE_REVEALED_TARGETS: {USE_REVEALED_TARGETS}\")\n","print(f\"USE_INDEX: {USE_INDEX}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Functions"]},{"cell_type":"markdown","metadata":{},"source":["## Memory Functions"]},{"cell_type":"code","execution_count":355,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:04.749082Z","iopub.status.busy":"2023-12-15T11:18:04.748680Z","iopub.status.idle":"2023-12-15T11:18:05.034316Z","shell.execute_reply":"2023-12-15T11:18:05.033290Z","shell.execute_reply.started":"2023-12-15T11:18:04.749042Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["RAM memory GB usage = 0.4107\n","CPU times: user 233 ms, sys: 124 ms, total: 357 ms\n","Wall time: 568 ms\n"]}],"source":["%%time \n","\n","from gc import collect;\n","from psutil import Process;\n","from os import system, getpid, walk;\n","\n","# Defining global configurations and functions:-\n","\n","    \n","def GetMemUsage():\n","    \"\"\"\n","    This function defines the memory usage across the kernel. \n","    Source-\n","    https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\n","    \"\"\";\n","    \n","    pid = getpid();\n","    py = Process(pid);\n","    memory_use = py.memory_info()[0] / 2. ** 30;\n","    return f\"RAM memory GB usage = {memory_use :.4}\";\n","\n","\n","collect();\n","print(GetMemUsage())"]},{"cell_type":"code","execution_count":356,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.037181Z","iopub.status.busy":"2023-12-15T11:18:05.036784Z","iopub.status.idle":"2023-12-15T11:18:05.050967Z","shell.execute_reply":"2023-12-15T11:18:05.050001Z","shell.execute_reply.started":"2023-12-15T11:18:05.037144Z"},"trusted":true},"outputs":[],"source":["# 🧹 Function to reduce memory usage of a Pandas DataFrame\n","def reduce_mem_usage(df, name: str, show_optimization: bool = False):\n","    \"\"\"\n","    Iterate through all numeric columns of a dataframe and modify the data type\n","    to reduce memory usage.\n","    \"\"\"\n","    \n","    # 📏 Calculate the initial memory usage of the DataFrame\n","    start_mem = df.memory_usage().sum() / 1024**2\n","\n","    # 🔄 Iterate through each column in the DataFrame\n","    for col in df.columns:\n","        col_type = df[col].dtype\n","\n","        # Check if the column's data type is not 'object' (i.e., numeric)\n","        if col_type != object:\n","            c_min = df[col].min()\n","            c_max = df[col].max()\n","            \n","            # Check if the column's data type is an integer\n","            if str(col_type)[:3] == \"int\":\n","                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                    df[col] = df[col].astype(np.int8)\n","                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                    df[col] = df[col].astype(np.int16)\n","                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                    df[col] = df[col].astype(np.int32)\n","                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                    df[col] = df[col].astype(np.int64)\n","            else:\n","                # Check if the column's data type is a float\n","                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n","                    df[col] = df[col].astype(np.float32)\n","                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                    df[col] = df[col].astype(np.float32)\n","                else:\n","                    df[col] = df[col].astype(np.float32)\n","\n","    if show_optimization:\n","        print(f\"Memory usage of {name} is {start_mem:.2f} MB\")\n","        end_mem = df.memory_usage().sum() / 1024**2\n","        print(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n","        decrease = 100 * (start_mem - end_mem) / start_mem\n","        print(f\"Decreased by {decrease:.2f}%\")\n","\n","    # 🔄 Return the DataFrame with optimized memory usage\n","\n","    start_mem = df.memory_usage().sum() / 1024**2\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["## API Function"]},{"cell_type":"code","execution_count":357,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.052330Z","iopub.status.busy":"2023-12-15T11:18:05.052047Z","iopub.status.idle":"2023-12-15T11:18:05.072366Z","shell.execute_reply":"2023-12-15T11:18:05.071440Z","shell.execute_reply.started":"2023-12-15T11:18:05.052306Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 43 µs, sys: 8 µs, total: 51 µs\n","Wall time: 53.9 µs\n"]}],"source":["%%time \n","\n","from typing import Sequence, Tuple\n","import pandas as pd\n","\n","# for local execution\n","class MockApi:\n","    def __init__(self):\n","        '''\n","        YOU MUST UPDATE THE FIRST THREE LINES of this method.\n","        They've been intentionally left in an invalid state.\n","\n","        Variables to set:\n","            input_paths: a list of two or more paths to the csv files to be served\n","            group_id_column: the column that identifies which groups of rows the API should serve.\n","                A call to iter_test serves all rows of all dataframes with the current group ID value.\n","            export_group_id_column: if true, the dataframes iter_test serves will include the group_id_column values.\n","        '''\n","        self.input_paths: Sequence[str] = [TEST_FILE, REVEALED_TARGETS_FILE, SAMPLE_SUBMISSION_FILE]\n","        self.group_id_column: str = 'time_id'\n","        self.export_group_id_column: bool = True\n","        # iter_test is only designed to support at least two dataframes, such as test and sample_submission\n","        assert len(self.input_paths) >= 2\n","\n","        self._status = 'initialized'\n","        self.predictions = []\n","\n","    def iter_test(self) -> Tuple[pd.DataFrame]:\n","        '''\n","        Loads all of the dataframes specified in self.input_paths,\n","        then yields all rows in those dataframes that equal the current self.group_id_column value.\n","        '''\n","        if self._status != 'initialized':\n","\n","            raise Exception('WARNING: the real API can only iterate over `iter_test()` once.')\n","\n","        dataframes = []\n","        for pth in self.input_paths:\n","            dataframes.append(pd.read_csv(pth, low_memory=False))\n","        group_order = dataframes[0][self.group_id_column].drop_duplicates().tolist()\n","        dataframes = [df.set_index(self.group_id_column) for df in dataframes]\n","\n","        for group_id in group_order:\n","            self._status = 'prediction_needed'\n","            current_data = []\n","            for df in dataframes:\n","                cur_df = df.loc[group_id].copy()\n","                # returning single line dataframes from df.loc requires special handling\n","                if not isinstance(cur_df, pd.DataFrame):\n","                    cur_df = pd.DataFrame({a: b for a, b in zip(cur_df.index.values, cur_df.values)}, index=[group_id])\n","                    cur_df.index.name = self.group_id_column\n","                cur_df = cur_df.reset_index(drop=not(self.export_group_id_column))\n","                current_data.append(cur_df)\n","            yield tuple(current_data)\n","\n","            while self._status != 'prediction_received':\n","                print('You must call `predict()` successfully before you can continue with `iter_test()`', flush=True)\n","                yield None\n","\n","        with open('submission.csv', 'w') as f_open:\n","            pd.concat(self.predictions).to_csv(f_open, index=False)\n","        self._status = 'finished'\n","\n","    def predict(self, user_predictions: pd.DataFrame):\n","        '''\n","        Accepts and stores the user's predictions and unlocks iter_test once that is done\n","        '''\n","        if self._status == 'finished':\n","            raise Exception('You have already made predictions for the full test set.')\n","        if self._status != 'prediction_needed':\n","            raise Exception('You must get the next test sample from `iter_test()` first.')\n","        if not isinstance(user_predictions, pd.DataFrame):\n","            raise Exception('You must provide a DataFrame.')\n","\n","        self.predictions.append(user_predictions)\n","        self._status = 'prediction_received'\n","\n","def make_env():\n","    return MockApi()"]},{"cell_type":"markdown","metadata":{},"source":["## Pandas Functions"]},{"cell_type":"code","execution_count":358,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.074216Z","iopub.status.busy":"2023-12-15T11:18:05.073557Z","iopub.status.idle":"2023-12-15T11:18:05.083338Z","shell.execute_reply":"2023-12-15T11:18:05.082562Z","shell.execute_reply.started":"2023-12-15T11:18:05.074184Z"},"trusted":true},"outputs":[],"source":["def pd_display_max():\n","    pd.set_option('display.max_rows', None)  # 行の最大表示数を無制限に設定\n","    pd.set_option('display.max_columns', None)  # 列の最大表示数を無制限に設定\n","    pd.set_option('display.width', None)  # 表示幅を拡張\n","    pd.set_option('display.max_colwidth', None)  # 列の幅を最大に設定\n","\n","def pd_clear_display_max():\n","    pd.set_option('display.max_rows', 10)\n","    pd.set_option('display.max_columns', 10)\n","    pd.set_option('display.width', None)  # 表示幅を拡張\n","    pd.set_option('display.max_colwidth', None)  # 列の幅を最大に設定"]},{"cell_type":"markdown","metadata":{},"source":["## Sorting Functions"]},{"cell_type":"code","execution_count":359,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.084614Z","iopub.status.busy":"2023-12-15T11:18:05.084361Z","iopub.status.idle":"2023-12-15T11:18:05.093911Z","shell.execute_reply":"2023-12-15T11:18:05.093194Z","shell.execute_reply.started":"2023-12-15T11:18:05.084593Z"},"trusted":true},"outputs":[],"source":["def default_sort(df):\n","    return df.sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["# Generationg train dataset"]},{"cell_type":"code","execution_count":360,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.096804Z","iopub.status.busy":"2023-12-15T11:18:05.096540Z","iopub.status.idle":"2023-12-15T11:18:05.108104Z","shell.execute_reply":"2023-12-15T11:18:05.107172Z","shell.execute_reply.started":"2023-12-15T11:18:05.096782Z"},"trusted":true},"outputs":[],"source":["def load_train_dataset():\n","    df = pd.read_csv(TRAIN_FILE)\n","    # 🧹 Remove rows with missing values in the \"target\" column\n","    df = df.dropna(subset=[\"target\"])\n","    # 🔁 Reset the index of the DataFrame and apply the changes in place\n","    df.reset_index(drop=True, inplace=True)\n","    return df\n","\n","def load_test_dataset():\n","    df_test = pd.read_csv(TEST_FILE)\n","    \n","    df_revealed_targets = pd.read_csv(REVEALED_TARGETS_FILE)\n","    df_revealed_targets = df_revealed_targets.dropna(subset=['date_id', 'seconds_in_bucket', 'stock_id'])\n","    df_revealed_targets['revealed_date_id'] = df_revealed_targets['revealed_date_id'].astype(int).astype(str)\n","    df_revealed_targets['seconds_in_bucket'] = df_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n","    df_revealed_targets['stock_id'] = df_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n","    df_revealed_targets['date_id'] = df_revealed_targets['date_id'].astype(int).astype(str)\n","    df_revealed_targets['row_id'] = df_revealed_targets['date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n","    df_revealed_targets['revealed_row_id'] = df_revealed_targets['revealed_date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n","\n","    # USE_CONTINUOUS_UPDATEが有効の時、cacheのrow_idとdf_revealed_targetsのrevealed_row_idをleft joinする\n","    if USE_CONTINUOUS_UPDATE:\n","        df_r = df_revealed_targets[['revealed_row_id', 'revealed_target']]\n","        df_r.rename(columns={'revealed_target': 'target'}, inplace=True)\n","        df_r.rename(columns={'revealed_row_id': 'row_id'}, inplace=True)\n","        df_test = pd.merge(df_test, df_r, how='left', on='row_id')\n","        \n","    # USE_REVEALED_TARGETSが有効の時、cacheのrow_idとdf_revealed_targetsのrow_idをleft joinする\n","    if USE_REVEALED_TARGETS:\n","        df_r = df_revealed_targets[['row_id', 'revealed_target']]\n","        df_test = pd.merge(df_test, df_r, how='left', on='row_id')\n","        \n","    df_test = df_test.dropna(subset=[\"target\"])\n","    df_test.reset_index(drop=True, inplace=True)\n","    return df_test"]},{"cell_type":"code","execution_count":361,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:05.110223Z","iopub.status.busy":"2023-12-15T11:18:05.109600Z","iopub.status.idle":"2023-12-15T11:18:18.238113Z","shell.execute_reply":"2023-12-15T11:18:18.237230Z","shell.execute_reply.started":"2023-12-15T11:18:05.110190Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Load train dataset\n","MIN LEARN MODE : [0]\n","['stock_id', 'seconds_in_bucket', 'imbalance_size', 'imbalance_buy_sell_flag', 'reference_price', 'matched_size', 'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price', 'ask_size', 'wap']\n","RAM memory GB usage = 0.9664\n","CPU times: user 7.05 s, sys: 2.81 s, total: 9.86 s\n","Wall time: 15 s\n"]}],"source":["%%time\n","# Check if the code is running in offline or online mode\n","print(\"Load train dataset\")\n","\n","df_train = load_train_dataset()\n","\n","if IS_MIN_LEARN:\n","    print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n","    # In local mode, stock id TARGET_STOCK_ID is used for training\n","    df_train = df_train[df_train[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","    \n","features = [c for c in df_train.columns if c not in [\"row_id\", \"target\", \"time_id\", \"row_id\", \"date_id\", \"currently_scored\"]]\n","print(features)\n","\n","collect();\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Generate Featuers"]},{"cell_type":"markdown","metadata":{},"source":["## Step1. Basic Features"]},{"cell_type":"code","execution_count":362,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:18.239979Z","iopub.status.busy":"2023-12-15T11:18:18.239682Z","iopub.status.idle":"2023-12-15T11:18:18.265404Z","shell.execute_reply":"2023-12-15T11:18:18.264498Z","shell.execute_reply.started":"2023-12-15T11:18:18.239952Z"},"trusted":true},"outputs":[],"source":["# Function to compute triplet imbalance in parallel using Numba\n","@njit(parallel=True)\n","def compute_triplet_imbalance(df_values, comb_indices):\n","    num_rows = df_values.shape[0]\n","    num_combinations = len(comb_indices)\n","    imbalance_features = np.empty((num_rows, num_combinations))\n","\n","    # 🔁 Loop through all combinations of triplets\n","    for i in prange(num_combinations):\n","        a, b, c = comb_indices[i]\n","        \n","        # 🔁 Loop through rows of the DataFrame\n","        for j in range(num_rows):\n","            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n","            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n","            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n","            \n","            # 🚫 Prevent division by zero\n","            if mid_val == min_val:\n","                imbalance_features[j, i] = np.nan\n","            else:\n","                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n","\n","    return imbalance_features\n","\n","# 📈 Function to calculate triplet imbalance for given price data and a DataFrame\n","def calculate_triplet_imbalance_numba(price, df):\n","    # Convert DataFrame to numpy array for Numba compatibility\n","    df_values = df[price].values\n","    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n","\n","    # Calculate the triplet imbalance using the Numba-optimized function\n","    features_array = compute_triplet_imbalance(df_values, comb_indices)\n","\n","    # Create a DataFrame from the results\n","    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n","    features = pd.DataFrame(features_array, columns=columns)\n","\n","    return features\n","\n","# Function to generate imbalance features\n","def imbalance_features(df):\n","    def __imbalance_features(df):\n","        # Define lists of price and size-related column names\n","        prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","        sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","\n","        # V1 features\n","        # Calculate various features using Pandas eval function\n","        df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n","        df[\"mid_price\"] = df.eval(\"ask_price + bid_price\")/2\n","        df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n","        df[\"matched_imbalance\"] = df.eval(\"imbalance_size-matched_size\")/df.eval(\"matched_size+imbalance_size\")\n","        df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n","        \n","        # Create features for pairwise price imbalances\n","        for c in combinations(prices, 2):\n","            df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n","            \n","        # V2 features\n","        # Calculate additional features\n","        df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n","        df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n","        df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n","        df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n","        df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n","        df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n","        # Calculate the imbalance ratio\n","        df['match_balance'] = ( df['matched_size']  + (df['imbalance_buy_sell_flag'] * df['imbalance_size'])) / df['matched_size']\n","        return df\n","\n","    if DEVICE == 'gpu':\n","        import cudf\n","        df = cudf.from_pandas(df)\n","        df = __imbalance_features(df)\n","        df = df.to_pandas()\n","    else:\n","        df = __imbalance_features(df)\n","    # Replace infinite values with 0\n","    return df.replace([np.inf, -np.inf], 0)\n","\n","def numba_imb_features(df):\n","    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n","    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n","    \n","    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n","        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n","        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n","        \n","    # Calculate triplet imbalance features using the Numba-optimized function\n","    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n","        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n","        df[triplet_feature.columns] = triplet_feature.values\n","    return df\n","\n","# 📅 Function to generate time and stock-related features\n","def other_features(df):\n","    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n","    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  # Seconds\n","    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  # Minutes\n","\n","    # Map global features to the DataFrame\n","    for key, value in global_stock_id_feats.items():\n","        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n","\n","    return df\n","\n","# 🚀 Function to generate all features by combining imbalance and other features\n","def generate_basic_features(df):\n","    prev_cols = list(df.columns)\n","\n","    # Generate imbalance features\n","    df = imbalance_features(df)\n","    df = numba_imb_features(df)\n","    df = other_features(df)\n","\n","    df = default_sort(df)    \n","    \n","    df = reduce_mem_usage(df, \"generate_basic_features\")\n","    collect()  # Perform garbage collection to free up memory\n","    return df"]},{"cell_type":"code","execution_count":363,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:18.267167Z","iopub.status.busy":"2023-12-15T11:18:18.266770Z","iopub.status.idle":"2023-12-15T11:18:58.098749Z","shell.execute_reply":"2023-12-15T11:18:58.097814Z","shell.execute_reply.started":"2023-12-15T11:18:18.267135Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Step1. Generate general Global Stock ID Features and basic features\n","['all_prices_skew', 'liquidity_imbalance', 'global_std_price', 'all_sizes_skew', 'far_price_bid_price_imb', 'far_price_wap_imb', 'global_ptp_price', 'near_price_ask_price_imb', 'reference_price_ask_price_imb', 'reference_price_wap_imb', 'size_imbalance', 'ask_price_bid_price_imb', 'all_sizes_kurt', 'far_price_near_price_imb', 'all_sizes_std', 'bid_price_wap_imb', 'global_median_size', 'ask_price_bid_price_wap_imb2', 'all_prices_std', 'global_ptp_size', 'matched_imbalance', 'price_pressure', 'ask_price_bid_price_reference_price_imb2', 'seconds', 'far_price_ask_price_imb', 'matched_size_bid_size_ask_size_imb2', 'market_urgency', 'global_median_price', 'reference_price_far_price_imb', 'spread_intensity', 'all_sizes_mean', 'bid_price_wap_reference_price_imb2', 'near_price_bid_price_imb', 'dow', 'depth_pressure', 'global_std_size', 'mid_price', 'minute', 'ask_price_wap_imb', 'match_balance', 'matched_size_ask_size_imbalance_size_imb2', 'bid_size_ask_size_imbalance_size_imb2', 'reference_price_near_price_imb', 'reference_price_bid_price_imb', 'all_prices_mean', 'volume', 'matched_size_bid_size_imbalance_size_imb2', 'imbalance_momentum', 'near_price_wap_imb', 'all_prices_kurt', 'price_spread', 'ask_price_wap_reference_price_imb2']\n","RAM memory GB usage = 1.083\n","CPU times: user 899 ms, sys: 60.9 ms, total: 960 ms\n","Wall time: 1.1 s\n"]}],"source":["%%time\n","\n","print(\"Step1. Generate general Global Stock ID Features and basic features\")\n","prev_cols = list(df_train.columns)\n","global_stock_id_feats = {\n","    \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n","    \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n","    \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n","    \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n","    \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n","    \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n","}\n","\n","df_train = generate_basic_features(df_train)\n","\n","generated_feature_name = list(set(df_train.columns) - set(prev_cols))\n","features += generated_feature_name\n","print(generated_feature_name)\n","\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["## Step2. Enhance features"]},{"cell_type":"code","execution_count":364,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:58.100454Z","iopub.status.busy":"2023-12-15T11:18:58.100119Z","iopub.status.idle":"2023-12-15T11:18:58.118236Z","shell.execute_reply":"2023-12-15T11:18:58.117215Z","shell.execute_reply.started":"2023-12-15T11:18:58.100429Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Step2. Generate enhanced features\n","CPU times: user 423 µs, sys: 71 µs, total: 494 µs\n","Wall time: 485 µs\n"]}],"source":["%%time\n","\n","print(\"Step2. Generate enhanced features\")\n","prev_cols = list(df_train.columns)\n","\n","@njit()\n","def cal_diff(x, window):\n","    # pands diffより遅い\n","    # xの長さと同じ大きさの配列を作成し、初期値をNaNに設定\n","    log_diff = np.full(x.shape, np.nan)\n","    # 指定されたwindowに基づいて差分を計算\n","    for i in range(window, len(x)):\n","        log_diff[i] = x[i] - x[i - window]\n","\n","    return log_diff\n","\n","#@njit()\n","#@njit(parallel=True)\n","@njit()\n","def cal_vix(x, window, offset=0):\n","    log_x = np.log(x + offset)\n","    log_diff = np.empty(log_x.shape)\n","    roll_std = np.empty(log_diff.shape)\n","\n","    for i in prange(1, len(log_x)):\n","        log_diff[i] = log_x[i] - log_x[i - 1]\n","    \n","    # ローリング標準偏差を計算\n","    # jitを使わない場合、roll_std[i] = np.std(log_diff[i-window+1:i+1], ddof=1)と書ける(不偏推定量を使うためddof=1)\n","    # jitを使う場合、ddof=1は使えないので、標準偏差の計算を自分で実装する\n","    for i in prange(window, len(log_diff)):\n","        window_values = log_diff[i-window+1:i+1]\n","        mean = np.mean(window_values)\n","        sum_sq_diff = np.sum((window_values - mean) ** 2)\n","        roll_std[i] = np.sqrt(sum_sq_diff / (window - 1))\n","\n","    return roll_std\n","\n","USE_DASK = False\n","import dask.dataframe as dd\n","def generate_historical_features(df):\n","    def __generate_historical_features(df):\n","        print(\"generate_historical_features\")\n","        target_cols = ['wap', 'match_balance']\n","        if USE_INDEX:\n","            target_cols.append('index_mean_wap')\n","            target_cols.append('index_mean_match_balance')\n","        if USE_REVEALED_TARGETS:\n","            target_cols.append('revealed_target')\n","\n","        grouped = df.groupby(['stock_id', 'date_id'])\n","\n","        for col in target_cols:\n","            for window in [3, 5, 7]:\n","                col_diff_name = f\"{col}_diff_{window}\"\n","                df[col_diff_name] = grouped[col].diff(window)\n","                #df[col_diff_name] = grouped[col].transform(lambda x: cal_diff(x.values, window))\n","\n","                col_vix_name = f\"{col}_vix_{window}\"\n","\n","                if col == 'revealed_target':\n","                    offset = 10\n","                else:\n","                    offset = 0\n","                \n","                #df[col_vix_name] = grouped[col].transform(lambda x: np.log(x).diff().rolling(window).std())\n","                df[col_vix_name] = grouped[col].transform(lambda x: cal_vix(x.values, window))\n","                #df[col_vix_name] = grouped[col].apply(lambda x: np.log(x + 100).diff().rolling(2).std()).reset_index()[col]\n","\n","        return df\n","\n","    # gpu, dskでも速度が出ないので、cpuで実行\n","    \"\"\"\n","    if DEVICE == 'gpu':\n","        import cudf\n","        df = cudf.from_pandas(df)\n","        df = __generate_historical_features(df)\n","        df = df.to_pandas()\n","    else:\n","        if USE_DASK:\n","            df = dd.from_pandas(df, npartitions=4)  # npartitionsは使用するコアの数に応じて調整\n","            df = df.set_index('stock_id')\n","            df = __generate_historical_features(df)\n","            df = df.compute()\n","        else:\n","            df = __generate_historical_features(df)\n","    \"\"\"\n","    df = __generate_historical_features(df)\n","\n","    df = df.replace([np.inf, -np.inf], 0)\n","    return df\n","\n","# サブセットを処理する関数\n","def subset_generate_historical_features(df_subset):\n","    return generate_historical_features(df_subset)\n","\n","# 並列処理を実行する関数\n","def parallel_generate_historical_features(df, num_threads=NUM_THREADS):\n","    # DataFrameを 'stock_id' でグループ化\n","    grouped = df.groupby('stock_id')\n","\n","    # 並列処理の実行\n","    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n","        results = executor.map(subset_generate_historical_features, [group for _, group in grouped])\n","\n","    # 結果の統合\n","    results = pd.concat(results)\n","    return results\n","\n","def generate_index_features(df):\n","     # Calculating mean and std for 'wap' and 'match_balance'\n","    wap_stats = df.groupby(['date_id', 'seconds_in_bucket'])['wap'].agg(['mean', 'std']).reset_index()\n","    match_balance_stats = df.groupby(['date_id', 'seconds_in_bucket'])['match_balance'].agg(['mean', 'std']).reset_index()\n","\n","    # Adding prefix and suffix\n","    wap_stats = wap_stats.add_prefix('index_').add_suffix('_wap')\n","    match_balance_stats = match_balance_stats.add_prefix('index_').add_suffix('_match_balance')\n","\n","    # Adjusting column names for merging\n","    wap_stats = wap_stats.rename(columns={'index_date_id_wap': 'date_id', 'index_seconds_in_bucket_wap': 'seconds_in_bucket'})\n","    match_balance_stats = match_balance_stats.rename(columns={'index_date_id_match_balance': 'date_id', 'index_seconds_in_bucket_match_balance': 'seconds_in_bucket'})\n","\n","    # Merging with the original dataframe\n","    df = df.merge(wap_stats, on=['date_id', 'seconds_in_bucket'], how='left')\n","    df = df.merge(match_balance_stats, on=['date_id', 'seconds_in_bucket'], how='left')\n","\n","    return df\n","\n","def generate_normalized_features(df, is_train):\n","    print(\"generate_normalized_features\")\n","    if is_train:\n","        df['n_target'] = (df['target'] - global_target['mean']) / global_target['std']\n","    df['n_wap'] = (df['wap'] - global_wap['mean']) / global_wap['std']\n","    df['n_match_balance'] = (df['match_balance'] - global_mathch_balance['mean']) / global_mathch_balance['std']\n","    df['n_reference_price'] = (df['reference_price'] - global_reference_price['mean']) / global_reference_price['std']\n","    \n","    df = reduce_mem_usage(df, \"generate_normalized_features\")\n","    return df\n","\n","def generate_enhance_features(df, is_train=False):\n","    print(\"generate_enhance_features\")\n","    if is_train:\n","        if USE_REVEALED_TARGETS:\n","            print(\"Use revealed targets\")\n","            df[f\"revealed_target\"] = df.groupby(['stock_id', 'seconds_in_bucket'])['target'].shift(1)\n","            df = df.dropna(subset=[\"revealed_target\"])\n","            df = default_sort(df)\n","        else:\n","            print(\"Dosent't use revealed targets\")\n","    if USE_INDEX:\n","        print(\"Use index\")\n","        current_time = time.time()\n","        df = generate_index_features(df)\n","        print(f\"generate_index_features {time.time() - current_time:.2f} [sec]\")\n","    current_time = time.time()\n","    \n","    if is_train:\n","        df = parallel_generate_historical_features(df)\n","    else:\n","        df = generate_historical_features(df)\n","    print(f\"generate_historical_features {time.time() - current_time:.2f} [sec]\")\n","\n","    df = df.reset_index(drop=True)\n","    df = default_sort(df)\n","    df = reduce_mem_usage(df, \"generate_enhance_features\")\n","    collect()\n","    return df"]},{"cell_type":"code","execution_count":365,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:18:58.120341Z","iopub.status.busy":"2023-12-15T11:18:58.119551Z","iopub.status.idle":"2023-12-15T11:19:30.664219Z","shell.execute_reply":"2023-12-15T11:19:30.663280Z","shell.execute_reply.started":"2023-12-15T11:18:58.120305Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["generate_enhance_features\n","Dosent't use revealed targets\n","Use index\n","generate_index_features 0.03 [sec]\n","generate_historical_features\n","generate_historical_features 0.56 [sec]\n","['index_std_wap', 'wap_diff_7', 'match_balance_diff_3', 'index_mean_wap_vix_5', 'match_balance_diff_7', 'index_mean_wap_diff_3', 'wap_vix_7', 'index_mean_wap', 'match_balance_diff_5', 'index_mean_wap_vix_7', 'index_mean_match_balance_diff_7', 'index_std_match_balance', 'match_balance_vix_3', 'wap_vix_5', 'match_balance_vix_7', 'index_mean_wap_diff_5', 'index_mean_wap_vix_3', 'wap_diff_3', 'index_mean_match_balance_vix_7', 'wap_vix_3', 'index_mean_wap_diff_7', 'index_mean_match_balance_vix_3', 'wap_diff_5', 'index_mean_match_balance_diff_3', 'index_mean_match_balance', 'index_mean_match_balance_diff_5', 'match_balance_vix_5', 'index_mean_match_balance_vix_5']\n","RAM memory GB usage = 1.127\n"]}],"source":["df_train = generate_enhance_features(df_train, is_train=True)\n","generated_feature_name = list(set(df_train.columns) - set(prev_cols))\n","features += generated_feature_name\n","print(generated_feature_name)\n","\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"code","execution_count":366,"metadata":{},"outputs":[],"source":["if IS_DEBUG:\n","    df_train.to_csv(f\"{BASE_OUTPUT_PATH}/df_train.csv\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Feature selection"]},{"cell_type":"code","execution_count":367,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:30.665637Z","iopub.status.busy":"2023-12-15T11:19:30.665355Z","iopub.status.idle":"2023-12-15T11:19:30.675372Z","shell.execute_reply":"2023-12-15T11:19:30.674460Z","shell.execute_reply.started":"2023-12-15T11:19:30.665612Z"},"trusted":true},"outputs":[{"data":{"text/plain":["['stock_id',\n"," 'seconds_in_bucket',\n"," 'imbalance_size',\n"," 'imbalance_buy_sell_flag',\n"," 'reference_price',\n"," 'matched_size',\n"," 'far_price',\n"," 'near_price',\n"," 'bid_price',\n"," 'bid_size',\n"," 'ask_price',\n"," 'ask_size',\n"," 'wap',\n"," 'all_prices_skew',\n"," 'liquidity_imbalance',\n"," 'global_std_price',\n"," 'all_sizes_skew',\n"," 'far_price_bid_price_imb',\n"," 'far_price_wap_imb',\n"," 'global_ptp_price',\n"," 'near_price_ask_price_imb',\n"," 'reference_price_ask_price_imb',\n"," 'reference_price_wap_imb',\n"," 'size_imbalance',\n"," 'ask_price_bid_price_imb',\n"," 'all_sizes_kurt',\n"," 'far_price_near_price_imb',\n"," 'all_sizes_std',\n"," 'bid_price_wap_imb',\n"," 'global_median_size',\n"," 'ask_price_bid_price_wap_imb2',\n"," 'all_prices_std',\n"," 'global_ptp_size',\n"," 'matched_imbalance',\n"," 'price_pressure',\n"," 'ask_price_bid_price_reference_price_imb2',\n"," 'seconds',\n"," 'far_price_ask_price_imb',\n"," 'matched_size_bid_size_ask_size_imb2',\n"," 'market_urgency',\n"," 'global_median_price',\n"," 'reference_price_far_price_imb',\n"," 'spread_intensity',\n"," 'all_sizes_mean',\n"," 'bid_price_wap_reference_price_imb2',\n"," 'near_price_bid_price_imb',\n"," 'dow',\n"," 'depth_pressure',\n"," 'global_std_size',\n"," 'mid_price',\n"," 'minute',\n"," 'ask_price_wap_imb',\n"," 'match_balance',\n"," 'matched_size_ask_size_imbalance_size_imb2',\n"," 'bid_size_ask_size_imbalance_size_imb2',\n"," 'reference_price_near_price_imb',\n"," 'reference_price_bid_price_imb',\n"," 'all_prices_mean',\n"," 'volume',\n"," 'matched_size_bid_size_imbalance_size_imb2',\n"," 'imbalance_momentum',\n"," 'near_price_wap_imb',\n"," 'all_prices_kurt',\n"," 'price_spread',\n"," 'ask_price_wap_reference_price_imb2',\n"," 'index_std_wap',\n"," 'wap_diff_7',\n"," 'match_balance_diff_3',\n"," 'index_mean_wap_vix_5',\n"," 'match_balance_diff_7',\n"," 'index_mean_wap_diff_3',\n"," 'wap_vix_7',\n"," 'index_mean_wap',\n"," 'match_balance_diff_5',\n"," 'index_mean_wap_vix_7',\n"," 'index_mean_match_balance_diff_7',\n"," 'index_std_match_balance',\n"," 'match_balance_vix_3',\n"," 'wap_vix_5',\n"," 'match_balance_vix_7',\n"," 'index_mean_wap_diff_5',\n"," 'index_mean_wap_vix_3',\n"," 'wap_diff_3',\n"," 'index_mean_match_balance_vix_7',\n"," 'wap_vix_3',\n"," 'index_mean_wap_diff_7',\n"," 'index_mean_match_balance_vix_3',\n"," 'wap_diff_5',\n"," 'index_mean_match_balance_diff_3',\n"," 'index_mean_match_balance',\n"," 'index_mean_match_balance_diff_5',\n"," 'match_balance_vix_5',\n"," 'index_mean_match_balance_vix_5']"]},"execution_count":367,"metadata":{},"output_type":"execute_result"}],"source":["# feature selection\n","if not USE_ALL_FEATUTES:\n","    features  = [\n","        \"revealed_target\",\n","        \"wap_diff_1\",\n","        \"index_mean_wap_diff_1\",\n","        \"seconds_in_bucket\",\n","        \"stock_id\",\n","    ]\n","#df_valid = df_train[\"target\"]\n","#df_train = df_train[features]\n","#if USE_REVEALED_TARGETS:\n","#    features.remove(\"revealed_target\")\n","features"]},{"cell_type":"code","execution_count":368,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:30.676954Z","iopub.status.busy":"2023-12-15T11:19:30.676623Z","iopub.status.idle":"2023-12-15T11:19:32.491520Z","shell.execute_reply":"2023-12-15T11:19:32.490523Z","shell.execute_reply.started":"2023-12-15T11:19:30.676923Z"},"trusted":true},"outputs":[{"data":{"text/plain":["index_std_wap                               26455\n","index_std_match_balance                     26455\n","far_price_near_price_imb                    14517\n","depth_pressure                              14517\n","far_price_ask_price_imb                     14517\n","far_price                                   14517\n","far_price_wap_imb                           14517\n","far_price_bid_price_imb                     14517\n","reference_price_far_price_imb               14517\n","near_price_ask_price_imb                    14430\n","near_price_bid_price_imb                    14430\n","reference_price_near_price_imb              14430\n","near_price_wap_imb                          14430\n","near_price                                  14430\n","bid_price_wap_reference_price_imb2           3614\n","ask_price_bid_price_reference_price_imb2     3528\n","index_mean_wap_diff_7                        3367\n","wap_diff_7                                   3367\n","index_mean_match_balance_diff_7              3367\n","match_balance_diff_7                         3367\n","index_mean_wap_diff_5                        2405\n","index_mean_match_balance_diff_5              2405\n","wap_diff_5                                   2405\n","match_balance_diff_5                         2405\n","wap_diff_3                                   1443\n","index_mean_match_balance_diff_3              1443\n","match_balance_diff_3                         1443\n","index_mean_wap_diff_3                        1443\n","index_mean_match_balance_vix_7                631\n","match_balance_vix_7                           625\n","index_mean_match_balance_vix_5                593\n","match_balance_vix_5                           572\n","index_mean_match_balance_vix_3                567\n","match_balance_vix_3                           518\n","index_mean_wap_vix_5                           78\n","ask_price_wap_reference_price_imb2             56\n","index_mean_wap_vix_7                           55\n","wap_vix_7                                      43\n","ask_price_bid_price_wap_imb2                   38\n","index_mean_wap_vix_3                           35\n","wap_vix_5                                      33\n","wap_vix_3                                      33\n","spread_intensity                                1\n","imbalance_momentum                              1\n","dtype: int64"]},"execution_count":368,"metadata":{},"output_type":"execute_result"}],"source":["pd_display_max()\n","nan_count = df_train[features].isna().sum()\n","#df_train[features].to_csv('train.csv', index=False)\n","nan_count = nan_count[nan_count > 0].sort_values(ascending=False)\n","nan_count"]},{"cell_type":"code","execution_count":369,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:32.493399Z","iopub.status.busy":"2023-12-15T11:19:32.492948Z","iopub.status.idle":"2023-12-15T11:19:32.497973Z","shell.execute_reply":"2023-12-15T11:19:32.496918Z","shell.execute_reply.started":"2023-12-15T11:19:32.493362Z"},"trusted":true},"outputs":[],"source":["pd_clear_display_max()"]},{"cell_type":"markdown","metadata":{},"source":["# Train function (lightgbm)"]},{"cell_type":"code","execution_count":370,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:32.499885Z","iopub.status.busy":"2023-12-15T11:19:32.499413Z","iopub.status.idle":"2023-12-15T11:19:33.389728Z","shell.execute_reply":"2023-12-15T11:19:33.388823Z","shell.execute_reply.started":"2023-12-15T11:19:32.499855Z"},"trusted":true},"outputs":[],"source":["# 📦 Import necessary libraries\n","import numpy as np\n","import lightgbm as lgb\n","from sklearn.metrics import mean_absolute_error\n","import gc\n","import os\n","from sklearn.model_selection import KFold\n","import numpy as np\n","from dataclasses import dataclass\n","import sys\n","import shutil\n","import lightgbm as lgb\n","\n","from warnings import simplefilter\n","simplefilter(\"ignore\", category=RuntimeWarning)\n","\n","@dataclass\n","class Model:\n","    booster: lgb.Booster\n","    fold: int\n","    feature_importance: pd.DataFrame\n","    score: float\n","    best_iteration: int\n","    train_time: float = None\n","    weight: float = None\n","    mem_usage: float = None\n","    train_func: str = None\n","    is_latest: bool = False\n","\n","def train_model(train_x, train_y, val_x, val_y, best_params=None):\n","    trains = lgb.Dataset(train_x, train_y)\n","    valids = lgb.Dataset(val_x, val_y, reference=trains)\n","\n","    verbose_eval = -1\n","    if best_params is None:\n","        params = lgb_params\n","    else:\n","        params = best_params\n","\n","    print(\"Use params:\")\n","    print(params)\n","    print(f\"stopping_rounds: {stopping_rounds}, num_boost_round: {num_boost_round}\")\n","    print(f\"train_x: {train_x.shape}, train_y: {train_y.shape}, val_x: {val_x.shape}, val_y: {val_y.shape}\")\n","    \n","    booster = lgb.train(\n","        params,\n","        trains,\n","        valid_sets=valids, # 検証データ\n","        num_boost_round=num_boost_round,\n","        keep_training_booster=True,\n","        callbacks=[\n","                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n","                lgb.log_evaluation(verbose_eval)\n","        ]\n","    )\n","\n","    del trains, valids\n","    return booster\n","\n","def cross_train(df, key, n_splits, features, valid_name, use_custome_fold, best_params=None):\n","    \"\"\" For Cross Train\n","\n","    Args:\n","        df (_type_): _description_\n","        n_splits (_type_): _description_\n","\n","    Returns:\n","        _type_: _description_\n","    \"\"\"\n","    print(\"----------------------------------------\")\n","    print(f\"Cross Train key id {key}: start, shape: {df_train.shape}, n_splits: {n_splits}\")\n","    print(f\"num_boost_round: {num_boost_round}, stopping_rounds: {stopping_rounds}, folds: {num_folds}\")\n","\n","    def __train(fold, X_train, y_train, X_valid, y_valid):\n","        now_time = time.time()\n","        booster = train_model(X_train, y_train, X_valid, y_valid, best_params)\n","        print(\"best score\", booster.best_score)\n","        train_time = time.time() - now_time\n","        print(f\"train_time: {train_time:.2f} [sec]\")\n","\n","        now_time = time.time()\n","        y_valid_pred = booster.predict(X_valid)\n","        score = mean_absolute_error(y_valid, y_valid_pred)\n","        valid_time = time.time() - now_time\n","        print(f\"valid_time: {valid_time:.2f} [sec]\")\n","\n","        mem_usage = sys.getsizeof(booster) / (1024 * 1024) # MB\n","        model = Model(booster, fold, booster.feature_importance(), score, booster.best_iteration, train_time, weight= 1 / n_splits, mem_usage=mem_usage, train_func=\"lightgbm\", is_latest=True)\n","        print(f\"{key}: {fold} end, score: {score}, time: {model.train_time}, best_iteration: {model.best_iteration}, memory usage: {model.mem_usage}\")\n","        \n","        del X_train, X_valid, y_train, y_valid\n","        gc.collect()\n","        print(GetMemUsage())\n","        return model\n","\n","    models = []\n","\n","    if use_custome_fold:\n","        print(\"use_custome_fold\")\n","        date_ids = df_train['date_id'].values\n","        fold_size = 480 // num_folds\n","        gap = 5\n","\n","        for fold in range(num_folds):\n","            print(f\"----- Train {key}: {fold} start -----\")\n","            start = fold * fold_size\n","            end = start + fold_size\n","            if fold < num_folds - 1:  # No need to purge after the last fold\n","                purged_start = end - 2\n","                purged_end = end + gap + 2\n","                train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n","            else:\n","                train_indices = (date_ids >= start) & (date_ids < end)\n","            \n","            valid_indices = (date_ids >= end) & (date_ids < end + fold_size)\n","            \n","            print(f\"train_indices: {train_indices}, valid_indices: {valid_indices}\")\n","            # 📊 Create fold-specific training and validation sets\n","            X_train, X_valid = df[features].iloc[train_indices], df[features].iloc[valid_indices]\n","            y_train, y_valid = df[valid_name].loc[train_indices], df[valid_name].loc[valid_indices]\n","            model = __train(fold, X_train, y_train, X_valid, y_valid)\n","            models.append(model)\n","            print(f\"----- Train {key}: {fold} end -----\")\n","\n","    else:\n","        print(\" NOT USE_CUSTOME_FOLD \")\n","        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","        df.reset_index(drop=True, inplace=True)\n","        \n","        for fold, (train_indices, valid_indices) in enumerate(kf.split(df)):\n","            print(f\"----- Train {key}: {fold} start -----\")\n","\n","            print(f\"train_indices: {train_indices}, valid_indices: {valid_indices}\")\n","            X_train, X_valid = df[features].iloc[train_indices], df[features].iloc[valid_indices]\n","            y_train, y_valid = df[valid_name].loc[train_indices], df[valid_name].loc[valid_indices]\n","            model = __train(fold, X_train, y_train, X_valid, y_valid)\n","            models.append(model)\n","            print(f\"----- Train {key}: {fold} end -----\")\n","            \n","\n","    print(f\"Cross train {key} model len {len(models)}\")\n","    models.sort(key=lambda x: x.score)\n","    [print(f\"fold: {model.fold}, score: {model.score}\") for model in models]\n","    models = models[:MODEL_NUM]\n","    print(\"model len\", len(models))\n","    print(\"----------------------------------------\")\n","    return key, models"]},{"cell_type":"markdown","metadata":{},"source":["# Train function (optuna)"]},{"cell_type":"code","execution_count":371,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:33.391267Z","iopub.status.busy":"2023-12-15T11:19:33.390965Z","iopub.status.idle":"2023-12-15T11:19:33.412793Z","shell.execute_reply":"2023-12-15T11:19:33.411830Z","shell.execute_reply.started":"2023-12-15T11:19:33.391226Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 88 µs, sys: 17 µs, total: 105 µs\n","Wall time: 107 µs\n"]}],"source":["%%time\n","\n","import optuna.integration.lightgbm as optuna_lgb\n","import optuna\n","import lightgbm\n","optuna.logging.set_verbosity(optuna.logging.ERROR)\n","\n","class TunerCVCheckpointCallback(object):\n","    \"\"\"Optuna の LightGBMTunerCV から学習済みモデルを取り出すためのコールバック\"\"\"\n","\n","    def __init__(self):\n","        # Models\n","        self.models = []\n","        self.counter = 0\n","\n","    def get_models(self):\n","        # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Booster.html#lightgbm.Booster\n","        return self.models\n","\n","    def __call__(self, env: lightgbm.callback.CallbackEnv):\n","        \"\"\"_summary_\n","\n","        Args:\n","            env (lightgbm.callback.CallbackEnv): _description_\n","            \"model\",\n","            \"params\",\n","            \"iteration\",\n","            \"begin_iteration\",\n","            \"end_iteration\",\n","            \"evaluation_result_list\"\n","        \"\"\"\n","        print(\"\")\n","\n","        self.counter += 1\n","        print(\"-------------------\")\n","        print(f\"Counter: {self.counter}\")\n","        print(f\"Iteration: {env.iteration}\")\n","        print(f\"Begin_iteration: {env.begin_iteration}\")\n","        print(f\"End_iteration: {env.end_iteration}\")\n","        print(f\"Evaluation_result_list: {env.evaluation_result_list}\")\n","        print(f\"Model best_iteration: {env.model.best_iteration}\")\n","        print(\"Params: \", env.params)\n","        #self.models.append(env.model)\n","        del env\n","\n","        collect();\n","        print(GetMemUsage())\n","\n","def optuna_tuning(df, n_splits, features, valid_name, model_save_path):\n","    df_train = df[features]\n","    df_valid = df[valid_name]\n","    \n","    trains = optuna_lgb.Dataset(df_train, df_valid)\n","    \n","    print(\"------- Optuna Tuning Start -------\")\n","    now_time = time.time()\n","    print(f\"num_boost_round: {num_boost_round}, stopping_rounds: {stopping_rounds}, folds: {num_folds}\")\n","\n","    folds = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n","    checkpoint_cb = TunerCVCheckpointCallback()\n","    \n","    verbose_eval = 0\n","    # https://optuna.readthedocs.io/en/stable/reference/generated/optuna.integration.lightgbm.LightGBMTunerCV.html\n","    tuner = optuna_lgb.LightGBMTunerCV(\n","        optuna_params,\n","        trains,\n","        num_boost_round=num_boost_round,\n","        folds=folds,\n","        show_progress_bar=False,\n","        return_cvbooster=True,\n","        verbosity=-1,\n","        model_dir=model_save_path,\n","        optuna_seed=seed,\n","        time_budget=OPTUNA_TIME_BUDGET,\n","        callbacks=[\n","                lgb.early_stopping(stopping_rounds=stopping_rounds, verbose=True),\n","                lgb.log_evaluation(verbose_eval),\n","                checkpoint_cb\n","        ]\n","    )\n","    \n","    tuner.run()\n","    best_params = tuner.best_params\n","    \n","    print(\"Params: \")\n","    for key, value in best_params.items():\n","        print(\" {}: {}\".format(key, value))\n","\n","    print(\"\")\n","    print(\"len(tuner.study.trials): \", len(tuner.study.trials))\n","    #print(\"len(checkpoint_cb.cv_boosters): \", len(checkpoint_cb.models))\n","    print(\"Tuner best_params\", tuner.best_params)\n","    print(\"Tuner best score: \", tuner.best_score)\n","   \n","    # 最も良かったパラメータをキーにして学習済みモデルを取り出す\n","    best_booster = tuner.get_best_booster()\n","    score = -1\n","    train_time = time.time() - now_time\n","    mem_usage = sys.getsizeof(best_booster) / (1024 * 1024) # MB\n","    feature_importance = np.mean(best_booster.feature_importance(), axis=0)\n","\n","    best_model = Model(best_booster, 1, feature_importance, score, best_booster.best_iteration, train_time, weight= 1, mem_usage=mem_usage, train_func=\"optuna_lgb\")\n","    print(\"------- Optuna Tuning End -------\")\n","    return best_params, best_model\n"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":372,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:19:33.414776Z","iopub.status.busy":"2023-12-15T11:19:33.414181Z","iopub.status.idle":"2023-12-15T11:26:09.828083Z","shell.execute_reply":"2023-12-15T11:26:09.827090Z","shell.execute_reply.started":"2023-12-15T11:19:33.414742Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------\n","Cross Train key id -1: start, shape: (26455, 97), n_splits: 2\n","num_boost_round: 1, stopping_rounds: 1, folds: 2\n","use_custome_fold\n","----- Train -1: 0 start -----\n","train_indices: [ True  True  True ...  True  True  True], valid_indices: [False False False ... False False False]\n","Use params:\n","{'objective': 'mae', 'n_estimators': 6000, 'num_leaves': 256, 'subsample': 0.6, 'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 11, 'n_jobs': 4, 'verbosity': -1, 'importance_type': 'gain', 'device': 'cpu'}\n","stopping_rounds: 1, num_boost_round: 1\n","train_x: (25905, 93), train_y: (25905,), val_x: (13200, 93), val_y: (13200,)\n","Training until validation scores don't improve for 1 rounds\n","Early stopping, best iteration is:\n","[942]\tvalid_0's l1: 1.64354\n","best score defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 1.6435391228641874)])})\n","train_time: 26.84 [sec]\n","valid_time: 1.19 [sec]\n","-1: 0 end, score: 1.6435391228641885, time: 26.838142156600952, best_iteration: 942, memory usage: 5.340576171875e-05\n","RAM memory GB usage = 0.9285\n","----- Train -1: 0 end -----\n","----- Train -1: 1 start -----\n","train_indices: [False False False ... False False False], valid_indices: [False False False ...  True  True  True]\n","Use params:\n","{'objective': 'mae', 'n_estimators': 6000, 'num_leaves': 256, 'subsample': 0.6, 'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 11, 'n_jobs': 4, 'verbosity': -1, 'importance_type': 'gain', 'device': 'cpu'}\n","stopping_rounds: 1, num_boost_round: 1\n","train_x: (13200, 93), train_y: (13200,), val_x: (55, 93), val_y: (55,)\n","Training until validation scores don't improve for 1 rounds\n","Early stopping, best iteration is:\n","[5]\tvalid_0's l1: 2.87765\n","best score defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 2.8776521278511398)])})\n","train_time: 0.19 [sec]\n","valid_time: 0.00 [sec]\n","-1: 1 end, score: 2.8776521278511398, time: 0.18565106391906738, best_iteration: 5, memory usage: 5.340576171875e-05\n","RAM memory GB usage = 0.9918\n","----- Train -1: 1 end -----\n","Cross train -1 model len 2\n","fold: 0, score: 1.6435391228641885\n","fold: 1, score: 2.8776521278511398\n","model len 1\n","----------------------------------------\n","RAM memory GB usage = 1.15\n","CPU times: user 39.8 s, sys: 9.97 s, total: 49.8 s\n","Wall time: 29.1 s\n"]}],"source":["%%time\n","\n","KEY = \"-1\"\n","\n","# Train\n","best_params = None\n","key_models = None\n","if USE_OPTUNA:\n","    model_save_base_path = f\"{BASE_OUTPUT_PATH}/model\"\n","    if os.path.exists(model_save_base_path):\n","        print(f\"{model_save_base_path} already exists, clean up it.\")\n","        shutil.rmtree(model_save_base_path)\n","    os.makedirs(model_save_base_path)\n","    print(f\"model_save_base_path: {model_save_base_path}\")\n","\n","    best_params, best_model = optuna_tuning(df=df_train, n_splits=num_folds, features=features, valid_name=\"target\", model_save_path=model_save_base_path)\n","    key_models = [(KEY, [best_model])]\n","else:\n","    #key_models = df_train.groupby(\"seconds_in_bucket\").apply(lambda x: cross_train(df=x, key=x.name, n_splits=num_folds, feature_name=feature_name, valid_name=\"target\", best_params=best_params))\n","    key_models = [cross_train(df_train, key=KEY, n_splits=num_folds, features=features, valid_name=\"target\", use_custome_fold=USE_CUSTOME_FOLD,  best_params=best_params)]\n","    if IS_USE_SAVED_MODEL:\n","        model_save_base_path = f\"{BASE_OUTPUT_PATH}/model\"\n","        if os.path.exists(model_save_base_path):\n","            print(f\"{model_save_base_path} already exists, clean up it.\")\n","            shutil.rmtree(model_save_base_path)\n","        os.makedirs(model_save_base_path)\n","\n","        key_model_paths = []\n","        for key, models in key_models:\n","            model_save_path = f\"{model_save_base_path}/{key}\"\n","            os.makedirs(model_save_path)\n","            model_paths = []\n","            for model in models:\n","                model_save_fullpath = f\"{model_save_path}/model_{key}_{model.fold}.txt\"\n","                model.model.save_model(model_save_fullpath)\n","                model_paths.append(model_save_fullpath)\n","            key_model_paths.append((key, model_paths))\n","\n","        model_dict_saved = {key: model_paths for key, model_paths in key_model_paths}\n","        print(model_dict_saved)\n","\n","\n","model_dict = {key: model for key, model in key_models}\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Additional training by important features"]},{"cell_type":"code","execution_count":373,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Additional Train\n","df_train_over: (2531, 99)\n","----------------------------------------\n","Cross Train key id -1: start, shape: (26455, 99), n_splits: 2\n","num_boost_round: 1, stopping_rounds: 1, folds: 2\n"," NOT USE_CUSTOME_FOLD \n","----- Train -1: 0 start -----\n","train_indices: [   0    1    3 ... 2521 2523 2527], valid_indices: [   2    8   13 ... 2528 2529 2530]\n","Use params:\n","{'objective': 'mae', 'n_estimators': 6000, 'num_leaves': 256, 'subsample': 0.6, 'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 11, 'n_jobs': 4, 'verbosity': -1, 'importance_type': 'gain', 'device': 'cpu'}\n","stopping_rounds: 1, num_boost_round: 1\n","train_x: (1265, 93), train_y: (1265,), val_x: (1266, 93), val_y: (1266,)\n","Training until validation scores don't improve for 1 rounds\n","Early stopping, best iteration is:\n","[22]\tvalid_0's l1: 11.2203\n","best score defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 11.220306048498593)])})\n","train_time: 0.22 [sec]\n","valid_time: 0.00 [sec]\n","-1: 0 end, score: 11.220306048498596, time: 0.21805477142333984, best_iteration: 22, memory usage: 5.340576171875e-05\n","RAM memory GB usage = 1.063\n","----- Train -1: 0 end -----\n","----- Train -1: 1 start -----\n","train_indices: [   2    8   13 ... 2528 2529 2530], valid_indices: [   0    1    3 ... 2521 2523 2527]\n","Use params:\n","{'objective': 'mae', 'n_estimators': 6000, 'num_leaves': 256, 'subsample': 0.6, 'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 11, 'n_jobs': 4, 'verbosity': -1, 'importance_type': 'gain', 'device': 'cpu'}\n","stopping_rounds: 1, num_boost_round: 1\n","train_x: (1266, 93), train_y: (1266,), val_x: (1265, 93), val_y: (1265,)\n","Training until validation scores don't improve for 1 rounds\n","Early stopping, best iteration is:\n","[12]\tvalid_0's l1: 12.0464\n","best score defaultdict(<class 'collections.OrderedDict'>, {'valid_0': OrderedDict([('l1', 12.04635160857711)])})\n","train_time: 0.07 [sec]\n","valid_time: 0.00 [sec]\n","-1: 1 end, score: 12.04635160857711, time: 0.07341480255126953, best_iteration: 12, memory usage: 5.340576171875e-05\n","RAM memory GB usage = 1.1\n","----- Train -1: 1 end -----\n","Cross train -1 model len 2\n","fold: 0, score: 11.220306048498596\n","fold: 1, score: 12.04635160857711\n","model len 1\n","----------------------------------------\n","RAM memory GB usage = 1.316\n","CPU times: user 9.58 s, sys: 305 ms, total: 9.89 s\n","Wall time: 4.96 s\n"]}],"source":["%%time\n","\n","additional_features = ['reference_price', 'match_balance_diff_5', 'wap', 'global_std_price',\n","       'all_sizes_skew', 'matched_size_bid_size_ask_size_imb2', 'ask_price',\n","       'index_mean_wap_diff_7', 'seconds_in_bucket', 'mid_price',\n","       'ask_price_bid_price_reference_price_imb2', 'wap_vix_7', 'wap_vix_3',\n","       'all_sizes_std', 'global_median_price', 'volume', 'all_sizes_mean',\n","       'revealed_target', 'wap_diff_7', 'global_ptp_size',\n","       'reference_price_wap_imb', 'bid_price_wap_reference_price_imb2',\n","       'stock_id', 'global_median_size']\n","\n","if not USE_REVEALED_TARGETS:\n","      additional_features.remove('revealed_target')\n","\n","if USE_ADDITIONAL_TRAIN:\n","   print(\"Additional Train\")\n","   boosters = [m.booster for m in model_dict[KEY]]\n","   predictions_list = [booster.predict(df_train[features]) for booster in boosters]\n","   predictions = np.mean(predictions_list, 0)\n","   df_train['pred'] = predictions\n","   df_train['score'] = np.abs(df_train['target'] - df_train['pred'])\n","   df_train_over = df_train[df_train['score'] > ADDITIONAL_TRAIN_THRESHOLD]\n","   print(f\"df_train_over: {df_train_over.shape}\")\n","\n","   key_additional_models = [cross_train(df_train_over, key=KEY, n_splits=num_folds, features=features, valid_name=\"target\", use_custome_fold=False, best_params=best_params)]\n","   additional_model_dict = {key: model for key, model in key_additional_models}\n","\n","   df_train = df_train.drop(columns=['pred', 'score'])\n","\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Update model using test"]},{"cell_type":"code","execution_count":374,"metadata":{"execution":{"iopub.execute_input":"2023-12-15T11:26:09.833812Z","iopub.status.busy":"2023-12-15T11:26:09.833233Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["date_duration 165\n","RAM memory GB usage = 1.326\n","CPU times: user 195 ms, sys: 3.35 ms, total: 199 ms\n","Wall time: 206 ms\n"]}],"source":["%%time\n","\n","# global train cache for continuous update\n","global_train_cache = df_train.copy()\n","# origin 0 is train, 1 is test, 2 is revaled\n","global_train_cache['origin'] = 0\n","date_duration = DATA_COUNT_IN_SAME_BUCKET * continuos_dataset_span\n","print(\"date_duration\", date_duration)\n","\n","def update_global_train_cache(df, origin, valid_key: str = 'target'):\n","    global global_train_cache\n","    df['origin'] = origin\n","    print(\"update_global_train_cache\")\n","    global_train_cache = pd.concat([global_train_cache, df], axis=0)\n","    global_train_cache = global_train_cache.dropna(subset=['target'])\n","    global_train_cache = global_train_cache.sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id', 'origin'])\n","    global_train_cache = global_train_cache.drop_duplicates(['date_id', 'seconds_in_bucket', 'stock_id'], keep='last')\n","    global_train_cache = global_train_cache.reset_index(drop=True)\n","    global_train_cache = global_train_cache.groupby(['stock_id']).tail(date_duration).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n","\n","    global_train_cache = reduce_mem_usage(global_train_cache, 'global_train_cache')\n","    print(f\"Updated global_train_cache, len: \", len(global_train_cache))\n","    if IS_DEBUG:\n","        print(len(global_train_cache))\n","        if USE_REVEALED_TARGETS:\n","            cdf = global_train_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'origin', 'revealed_target', 'target']]\n","        else:\n","            cdf = global_train_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'origin', 'target']]\n","        print(cdf)\n","\n","def update_models(df, models, features, valid_name):\n","    \"\"\" For Update Model\n","\n","    Args:\n","        df (_type_): _description_\n","        n_splits (_type_): _description_\n","\n","    Returns:\n","        _type_: _description_\n","    \"\"\"\n","\n","    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","    df.reset_index(drop=True, inplace=True)\n","    \n","    for fold, (train_indices, valid_indices) in enumerate(kf.split(df)):\n","        print(f\"{key}: {fold} update\")\n","        now_time = time.time()\n","        X_train, X_valid = df[features].iloc[train_indices], df[features].iloc[valid_indices]\n","        y_train, y_valid = df[valid_name].loc[train_indices], df[valid_name].loc[valid_indices]\n","        trains = lgb.Dataset(X_train, y_train, free_raw_data=False)\n","        valids = lgb.Dataset(X_valid, y_valid, reference=trains)\n","\n","        print(f\"X_train: {X_train.shape}, X_valid: {X_valid.shape}, y_train: {y_train.shape}, y_valid: {y_valid.shape}\")\n","        models[fold].booster.update(trains)\n","\n","def keep_train_models(df, models, features, valid_name, is_append=False):\n","    print(f\"----------------- keep_train_models, is_append: {is_append}, model len: {len(models)}, df len: {len(df)} ---------------------\")\n","    train_x = df[features]\n","    train_y = df[valid_name]\n","    trains = lgb.Dataset(train_x, train_y, free_raw_data=False)\n","    verbose_eval = -1\n","\n","    counter = 0\n","    r_models = []\n","    if IS_DEBUG:\n","        print(f\"Re-train dataset:\")\n","        if USE_REVEALED_TARGETS:\n","            print(df[['date_id', 'seconds_in_bucket', 'stock_id', 'target', 'revealed_target']])\n","        else:\n","            print(df[['date_id', 'seconds_in_bucket', 'stock_id', 'target']])\n","    for model in models:\n","        print(f\"---- train start, counter: {counter} ----\")\n","        if model.is_latest:\n","            print(\"Update latest model\")\n","            now_time = time.time()\n","            booster = lgb.train(\n","                lgb_params,\n","                trains,\n","                num_boost_round=update_num_boost_round,\n","                keep_training_booster=True,\n","                init_model=model.booster,\n","            )\n","            train_time = time.time() - now_time\n","            updated_model = Model(\n","                booster=booster,\n","                fold=1,\n","                best_iteration=booster.best_iteration, \n","                feature_importance=booster.feature_importance(),\n","                score=-1, \n","                train_time=train_time, \n","                weight=-1, \n","                mem_usage=-1,\n","                is_latest=True,\n","                train_func=\"lightgbm update by test\")\n","            r_models.append(updated_model)\n","            if is_append:\n","                print(\"Adding previous model\")\n","                model.is_latest = False\n","                r_models.append(model)\n","        else:\n","            print(\"Dose not latest, just append\")\n","            r_models.append(model)\n","        counter = counter + 1\n","    print(f\"---- train end, train time: {train_time}, updated model len {len(r_models)} ----\")\n","    return r_models\n","\n","\"\"\"\n","if USE_CONTINUOUS_UPDATE:\n","    try:\n","        print(\"Update model with test date\")\n","        df_test = load_test_dataset()\n","        if IS_MIN_LEARN:\n","            print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n","            df_test = df_test[df_test[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","        df_test = generate_basic_features(df_test)\n","        df_test = generate_enhance_features(df_test)\n","        update_global_train_cache(df_test, 1)\n","        #model_dict[KEY] = keep_train_models(global_train_cache, model_dict[KEY], features, \"target\", is_append=True)\n","        print(\"Update model with test date end\")\n","    except Exception as e:\n","        print(\"Cannot get test date\", e)\n","\"\"\"\n","\n","collect()\n","print(GetMemUsage())\n"]},{"cell_type":"code","execution_count":375,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Key: -1, model len: 1\n","       fold     score  best_iteration  train_time\n","count   1.0  1.000000             1.0    1.000000\n","mean    0.0  1.643539           942.0   26.838142\n","std     NaN       NaN             NaN         NaN\n","min     0.0  1.643539           942.0   26.838142\n","25%     0.0  1.643539           942.0   26.838142\n","50%     0.0  1.643539           942.0   26.838142\n","75%     0.0  1.643539           942.0   26.838142\n","max     0.0  1.643539           942.0   26.838142\n"]}],"source":["# Show results\n","\n","for key, models in model_dict.items():\n","    print(f\"Key: {key}, model len: {len(models)}\")\n","    data = []\n","    for model in models:\n","        score = model.score\n","        best_iteration = model.best_iteration\n","        fold = model.fold\n","        train_time = model.train_time\n","        data.append({\"key\": key, \"fold\": fold, \"score\": score, \"best_iteration\": best_iteration, \"train_time\": train_time})\n","\n","    df_model = pd.DataFrame(data)\n","    print(df_model.describe())"]},{"cell_type":"code","execution_count":376,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>fold</th>\n","      <th>score</th>\n","      <th>best_iteration</th>\n","      <th>train_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","      <td>1.0</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.0</td>\n","      <td>1.643539</td>\n","      <td>942.0</td>\n","      <td>26.838142</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.0</td>\n","      <td>1.643539</td>\n","      <td>942.0</td>\n","      <td>26.838142</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.0</td>\n","      <td>1.643539</td>\n","      <td>942.0</td>\n","      <td>26.838142</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.0</td>\n","      <td>1.643539</td>\n","      <td>942.0</td>\n","      <td>26.838142</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.0</td>\n","      <td>1.643539</td>\n","      <td>942.0</td>\n","      <td>26.838142</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.0</td>\n","      <td>1.643539</td>\n","      <td>942.0</td>\n","      <td>26.838142</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       fold     score  best_iteration  train_time\n","count   1.0  1.000000             1.0    1.000000\n","mean    0.0  1.643539           942.0   26.838142\n","std     NaN       NaN             NaN         NaN\n","min     0.0  1.643539           942.0   26.838142\n","25%     0.0  1.643539           942.0   26.838142\n","50%     0.0  1.643539           942.0   26.838142\n","75%     0.0  1.643539           942.0   26.838142\n","max     0.0  1.643539           942.0   26.838142"]},"execution_count":376,"metadata":{},"output_type":"execute_result"}],"source":["# Check model quality\n","data = []\n","\n","for key, i_models in model_dict.items():\n","    for model in i_models:\n","        score = model.score\n","        best_iteration = model.best_iteration\n","        fold = model.fold\n","        train_time = model.train_time\n","        data.append({\"key\": key, \"fold\": fold, \"score\": score, \"best_iteration\": best_iteration, \"train_time\": train_time})\n","\n","df_model = pd.DataFrame(data)\n","df_model.describe()"]},{"cell_type":"code","execution_count":377,"metadata":{"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>importance</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>bid_price_wap_reference_price_imb2</th>\n","      <td>3499.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_bid_price_reference_price_imb2</th>\n","      <td>3039.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_7</th>\n","      <td>2948.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_diff_3</th>\n","      <td>2805.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_diff_5</th>\n","      <td>2736.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_diff_7</th>\n","      <td>2481.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_vix_3</th>\n","      <td>2333.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_size</th>\n","      <td>2318.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_diff_3</th>\n","      <td>2314.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_bid_size_ask_size_imb2</th>\n","      <td>2303.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_momentum</th>\n","      <td>2267.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_diff_5</th>\n","      <td>2229.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_size</th>\n","      <td>2159.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_size_ask_size_imbalance_size_imb2</th>\n","      <td>2061.0</td>\n","    </tr>\n","    <tr>\n","      <th>volume</th>\n","      <td>2057.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_mean</th>\n","      <td>2054.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size</th>\n","      <td>2052.0</td>\n","    </tr>\n","    <tr>\n","      <th>spread_intensity</th>\n","      <td>2033.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_vix_7</th>\n","      <td>2001.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_vix_5</th>\n","      <td>1962.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_vix_7</th>\n","      <td>1951.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_wap_reference_price_imb2</th>\n","      <td>1928.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_size</th>\n","      <td>1838.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_vix_3</th>\n","      <td>1832.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_skew</th>\n","      <td>1829.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_bid_price_imb</th>\n","      <td>1795.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_wap_imb</th>\n","      <td>1763.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_vix_7</th>\n","      <td>1717.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price</th>\n","      <td>1711.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price_wap_imb</th>\n","      <td>1696.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_wap_imb</th>\n","      <td>1694.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_std</th>\n","      <td>1632.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_std</th>\n","      <td>1574.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_vix_7</th>\n","      <td>1553.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_kurt</th>\n","      <td>1524.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance_vix_5</th>\n","      <td>1489.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_prices_mean</th>\n","      <td>1405.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_ask_price_imb</th>\n","      <td>1403.0</td>\n","    </tr>\n","    <tr>\n","      <th>seconds_in_bucket</th>\n","      <td>1392.0</td>\n","    </tr>\n","    <tr>\n","      <th>match_balance</th>\n","      <td>1362.0</td>\n","    </tr>\n","    <tr>\n","      <th>price_pressure</th>\n","      <td>1355.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_skew</th>\n","      <td>1307.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_vix_5</th>\n","      <td>1301.0</td>\n","    </tr>\n","    <tr>\n","      <th>bid_price</th>\n","      <td>1298.0</td>\n","    </tr>\n","    <tr>\n","      <th>market_urgency</th>\n","      <td>1288.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_vix_5</th>\n","      <td>1279.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_vix_3</th>\n","      <td>1268.0</td>\n","    </tr>\n","    <tr>\n","      <th>depth_pressure</th>\n","      <td>1170.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price</th>\n","      <td>1148.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_bid_price_imb</th>\n","      <td>1140.0</td>\n","    </tr>\n","    <tr>\n","      <th>liquidity_imbalance</th>\n","      <td>1103.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_vix_3</th>\n","      <td>1031.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price</th>\n","      <td>965.0</td>\n","    </tr>\n","    <tr>\n","      <th>dow</th>\n","      <td>943.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_near_price_imb</th>\n","      <td>918.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap</th>\n","      <td>864.0</td>\n","    </tr>\n","    <tr>\n","      <th>near_price</th>\n","      <td>859.0</td>\n","    </tr>\n","    <tr>\n","      <th>seconds</th>\n","      <td>828.0</td>\n","    </tr>\n","    <tr>\n","      <th>price_spread</th>\n","      <td>726.0</td>\n","    </tr>\n","    <tr>\n","      <th>ask_price_bid_price_wap_imb2</th>\n","      <td>674.0</td>\n","    </tr>\n","    <tr>\n","      <th>mid_price</th>\n","      <td>559.0</td>\n","    </tr>\n","    <tr>\n","      <th>all_sizes_kurt</th>\n","      <td>543.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_3</th>\n","      <td>542.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap_diff_7</th>\n","      <td>536.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_diff_7</th>\n","      <td>536.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_ask_size_imbalance_size_imb2</th>\n","      <td>533.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_imbalance</th>\n","      <td>516.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_bid_price_imb</th>\n","      <td>512.0</td>\n","    </tr>\n","    <tr>\n","      <th>wap_diff_5</th>\n","      <td>499.0</td>\n","    </tr>\n","    <tr>\n","      <th>near_price_ask_price_imb</th>\n","      <td>473.0</td>\n","    </tr>\n","    <tr>\n","      <th>size_imbalance</th>\n","      <td>460.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_diff_5</th>\n","      <td>441.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance_diff_3</th>\n","      <td>424.0</td>\n","    </tr>\n","    <tr>\n","      <th>matched_size_bid_size_imbalance_size_imb2</th>\n","      <td>418.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_wap_imb</th>\n","      <td>381.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_match_balance</th>\n","      <td>280.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_near_price_imb</th>\n","      <td>270.0</td>\n","    </tr>\n","    <tr>\n","      <th>near_price_bid_price_imb</th>\n","      <td>266.0</td>\n","    </tr>\n","    <tr>\n","      <th>near_price_wap_imb</th>\n","      <td>240.0</td>\n","    </tr>\n","    <tr>\n","      <th>far_price_ask_price_imb</th>\n","      <td>236.0</td>\n","    </tr>\n","    <tr>\n","      <th>reference_price_far_price_imb</th>\n","      <td>217.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_mean_wap</th>\n","      <td>115.0</td>\n","    </tr>\n","    <tr>\n","      <th>minute</th>\n","      <td>107.0</td>\n","    </tr>\n","    <tr>\n","      <th>imbalance_buy_sell_flag</th>\n","      <td>84.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_std_size</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_ptp_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_std_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_wap</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>index_std_match_balance</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_median_size</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_median_price</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>global_ptp_size</th>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>stock_id</th>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          importance\n","bid_price_wap_reference_price_imb2            3499.0\n","ask_price_bid_price_reference_price_imb2      3039.0\n","wap_diff_7                                    2948.0\n","index_mean_wap_diff_3                         2805.0\n","index_mean_wap_diff_5                         2736.0\n","match_balance_diff_7                          2481.0\n","index_mean_wap_vix_3                          2333.0\n","bid_size                                      2318.0\n","match_balance_diff_3                          2314.0\n","matched_size_bid_size_ask_size_imb2           2303.0\n","imbalance_momentum                            2267.0\n","match_balance_diff_5                          2229.0\n","ask_size                                      2159.0\n","bid_size_ask_size_imbalance_size_imb2         2061.0\n","volume                                        2057.0\n","all_sizes_mean                                2054.0\n","matched_size                                  2052.0\n","spread_intensity                              2033.0\n","wap_vix_7                                     2001.0\n","index_mean_wap_vix_5                          1962.0\n","match_balance_vix_7                           1951.0\n","ask_price_wap_reference_price_imb2            1928.0\n","imbalance_size                                1838.0\n","match_balance_vix_3                           1832.0\n","all_prices_skew                               1829.0\n","ask_price_bid_price_imb                       1795.0\n","ask_price_wap_imb                             1763.0\n","index_mean_wap_vix_7                          1717.0\n","reference_price                               1711.0\n","bid_price_wap_imb                             1696.0\n","reference_price_wap_imb                       1694.0\n","all_prices_std                                1632.0\n","all_sizes_std                                 1574.0\n","index_mean_match_balance_vix_7                1553.0\n","all_prices_kurt                               1524.0\n","match_balance_vix_5                           1489.0\n","all_prices_mean                               1405.0\n","reference_price_ask_price_imb                 1403.0\n","seconds_in_bucket                             1392.0\n","match_balance                                 1362.0\n","price_pressure                                1355.0\n","all_sizes_skew                                1307.0\n","wap_vix_5                                     1301.0\n","bid_price                                     1298.0\n","market_urgency                                1288.0\n","index_mean_match_balance_vix_5                1279.0\n","wap_vix_3                                     1268.0\n","depth_pressure                                1170.0\n","ask_price                                     1148.0\n","reference_price_bid_price_imb                 1140.0\n","liquidity_imbalance                           1103.0\n","index_mean_match_balance_vix_3                1031.0\n","far_price                                      965.0\n","dow                                            943.0\n","far_price_near_price_imb                       918.0\n","wap                                            864.0\n","near_price                                     859.0\n","seconds                                        828.0\n","price_spread                                   726.0\n","ask_price_bid_price_wap_imb2                   674.0\n","mid_price                                      559.0\n","all_sizes_kurt                                 543.0\n","wap_diff_3                                     542.0\n","index_mean_wap_diff_7                          536.0\n","index_mean_match_balance_diff_7                536.0\n","matched_size_ask_size_imbalance_size_imb2      533.0\n","matched_imbalance                              516.0\n","far_price_bid_price_imb                        512.0\n","wap_diff_5                                     499.0\n","near_price_ask_price_imb                       473.0\n","size_imbalance                                 460.0\n","index_mean_match_balance_diff_5                441.0\n","index_mean_match_balance_diff_3                424.0\n","matched_size_bid_size_imbalance_size_imb2      418.0\n","far_price_wap_imb                              381.0\n","index_mean_match_balance                       280.0\n","reference_price_near_price_imb                 270.0\n","near_price_bid_price_imb                       266.0\n","near_price_wap_imb                             240.0\n","far_price_ask_price_imb                        236.0\n","reference_price_far_price_imb                  217.0\n","index_mean_wap                                 115.0\n","minute                                         107.0\n","imbalance_buy_sell_flag                         84.0\n","global_std_size                                  0.0\n","global_ptp_price                                 0.0\n","global_std_price                                 0.0\n","index_std_wap                                    0.0\n","index_std_match_balance                          0.0\n","global_median_size                               0.0\n","global_median_price                              0.0\n","global_ptp_size                                  0.0\n","stock_id                                         0.0"]},"execution_count":377,"metadata":{},"output_type":"execute_result"}],"source":["# Initialize an empty DataFrame for aggregated importances\n","aggregated_importance = pd.DataFrame(index=features, columns=['importance'])\n","\n","# Aggregate the importances from each model\n","for key, i_models in model_dict.items():\n","    for model in i_models:\n","        importance = pd.DataFrame({'feature': features, 'importance': model.feature_importance})\n","        aggregated_importance = aggregated_importance.add(importance.set_index('feature'), fill_value=0)\n","\n","aggregated_importance['importance'] /= len(df_model)\n","\n","pd_display_max()\n","# Sort the features by importance\n","aggregated_importance = aggregated_importance.sort_values(by='importance', ascending=False)\n","aggregated_importance"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset types"]},{"cell_type":"code","execution_count":378,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["stock_id                                        int8\n","seconds_in_bucket                              int16\n","imbalance_size                               float32\n","imbalance_buy_sell_flag                         int8\n","reference_price                              float32\n","matched_size                                 float32\n","far_price                                    float32\n","near_price                                   float32\n","bid_price                                    float32\n","bid_size                                     float32\n","ask_price                                    float32\n","ask_size                                     float32\n","wap                                          float32\n","all_prices_skew                              float32\n","liquidity_imbalance                          float32\n","global_std_price                             float32\n","all_sizes_skew                               float32\n","far_price_bid_price_imb                      float32\n","far_price_wap_imb                            float32\n","global_ptp_price                             float32\n","near_price_ask_price_imb                     float32\n","reference_price_ask_price_imb                float32\n","reference_price_wap_imb                      float32\n","size_imbalance                               float32\n","ask_price_bid_price_imb                      float32\n","all_sizes_kurt                               float32\n","far_price_near_price_imb                     float32\n","all_sizes_std                                float32\n","bid_price_wap_imb                            float32\n","global_median_size                           float32\n","ask_price_bid_price_wap_imb2                 float32\n","all_prices_std                               float32\n","global_ptp_size                              float32\n","matched_imbalance                            float32\n","price_pressure                               float32\n","ask_price_bid_price_reference_price_imb2     float32\n","seconds                                         int8\n","far_price_ask_price_imb                      float32\n","matched_size_bid_size_ask_size_imb2          float32\n","market_urgency                               float32\n","global_median_price                          float32\n","reference_price_far_price_imb                float32\n","spread_intensity                             float32\n","all_sizes_mean                               float32\n","bid_price_wap_reference_price_imb2           float32\n","near_price_bid_price_imb                     float32\n","dow                                             int8\n","depth_pressure                               float32\n","global_std_size                              float32\n","mid_price                                    float32\n","minute                                          int8\n","ask_price_wap_imb                            float32\n","match_balance                                float32\n","matched_size_ask_size_imbalance_size_imb2    float32\n","bid_size_ask_size_imbalance_size_imb2        float32\n","reference_price_near_price_imb               float32\n","reference_price_bid_price_imb                float32\n","all_prices_mean                              float32\n","volume                                       float32\n","matched_size_bid_size_imbalance_size_imb2    float32\n","imbalance_momentum                           float32\n","near_price_wap_imb                           float32\n","all_prices_kurt                              float32\n","price_spread                                 float32\n","ask_price_wap_reference_price_imb2           float32\n","index_std_wap                                float32\n","wap_diff_7                                   float32\n","match_balance_diff_3                         float32\n","index_mean_wap_vix_5                         float32\n","match_balance_diff_7                         float32\n","index_mean_wap_diff_3                        float32\n","wap_vix_7                                    float32\n","index_mean_wap                               float32\n","match_balance_diff_5                         float32\n","index_mean_wap_vix_7                         float32\n","index_mean_match_balance_diff_7              float32\n","index_std_match_balance                      float32\n","match_balance_vix_3                          float32\n","wap_vix_5                                    float32\n","match_balance_vix_7                          float32\n","index_mean_wap_diff_5                        float32\n","index_mean_wap_vix_3                         float32\n","wap_diff_3                                   float32\n","index_mean_match_balance_vix_7               float32\n","wap_vix_3                                    float32\n","index_mean_wap_diff_7                        float32\n","index_mean_match_balance_vix_3               float32\n","wap_diff_5                                   float32\n","index_mean_match_balance_diff_3              float32\n","index_mean_match_balance                     float32\n","index_mean_match_balance_diff_5              float32\n","match_balance_vix_5                          float32\n","index_mean_match_balance_vix_5               float32\n","dtype: object"]},"execution_count":378,"metadata":{},"output_type":"execute_result"}],"source":["features_types = df_train[features].dtypes\n","features_types"]},{"cell_type":"code","execution_count":379,"metadata":{"trusted":true},"outputs":[],"source":["def convert_dtypes(df):\n","    df_types = df[features].dtypes\n","    different_types = [col for col in df_types.index if col in features_types and df_types[col] != features_types[col]]\n","    print(f\"Different Types: {different_types}\")\n","    return different_types\n","\n","def update_dtypes_by_origin(df):\n","    diff_types = convert_dtypes(df)\n","    for col in diff_types:\n","        df[col] = df[col].astype(features_types[col])\n","    return df"]},{"cell_type":"markdown","metadata":{},"source":["# Clear trains"]},{"cell_type":"code","execution_count":380,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["RAM memory GB usage = 1.343\n"]}],"source":["# Clean up\n","pd_clear_display_max()\n","del key_models\n","if IS_USE_SAVED_MODEL:\n","    print(\"Delete model_dict\")\n","    del model_dict\n","del df_train\n","collect()\n","print(GetMemUsage())"]},{"cell_type":"markdown","metadata":{},"source":["# Infer"]},{"cell_type":"code","execution_count":381,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 11 µs, sys: 12 µs, total: 23 µs\n","Wall time: 23.8 µs\n"]}],"source":["%%time\n","\n","y_min, y_max = -64, 64\n","\n","# 📉 Define a function to adjust prices based on volumes\n","def zero_sum(prices, volumes):\n","    std_error = np.sqrt(volumes)  # 🧮 Calculate standard error based on volumes\n","    step = np.sum(prices) / np.sum(std_error)  # 🧮 Calculate the step size based on prices and standard error\n","    out = prices - std_error * step  # 💰 Adjust prices by subtracting the standardized step size\n","    return out\n","\n","def zero_clip(df, predictions):\n","    # Adjust the predictions based on the order book imbalance\n","    zerosum_predictions = zero_sum(predictions, df['bid_size'] + df['ask_size'])\n","    clipped_predictions = np.clip(zerosum_predictions, y_min, y_max)\n","    clipped_predictions.replace([np.nan, np.inf, -np.inf], 0, inplace=True)\n","    clipped_predictions = clipped_predictions.astype('float64').values  \n","    return clipped_predictions\n","\n","def model_infer(key, df_feat, additional_infer=False):\n","    def predictor(boosters):\n","        #print(f\"Predictor Feat len {len(df_feat)}\")\n","        if USE_OPTUNA:\n","            predictions_list = [np.mean(booster.predict(df_feat), 0) for booster in boosters]\n","        else:\n","            predictions_list = [booster.predict(df_feat) for booster in boosters]\n","        predictions = np.mean(predictions_list, 0)\n","        std_predictions = np.std(predictions_list, 0)\n","        #print(\"std_predictions\", std_predictions)\n","        return predictions\n","    \n","    if IS_USE_SAVED_MODEL:\n","        model_paths = model_dict_saved[key]\n","        models = [lgb.Booster(model_file=model_path) for model_path in model_paths]\n","        predictions = predictor(models)\n","        del models\n","    else:\n","        if additional_infer:\n","            print(\"Use additional model\")\n","            boosters = [m.booster for m in additional_model_dict[key]]\n","            print(f\"Additional predictor target models len {len(boosters)}\")\n","        else:\n","            boosters = [m.booster for m in model_dict[key]]\n","            print(f\"Predictor target models len {len(boosters)}\")\n","        predictions = predictor(boosters)\n","    collect()\n","    return predictions"]},{"cell_type":"code","execution_count":382,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Infer Local\n","------- counter 1 start -------\n","copy_revealed_targets len 11000\n","Update revealed_targets\n","MIN LEARN MODE : [0]\n","df_cache len 1\n","generate_enhance_features\n","Use index\n","generate_index_features 0.01 [sec]\n","generate_historical_features\n","generate_historical_features 0.01 [sec]\n","date_id: 478,  seconds_in_bucket: 0.0\n","df_cache_with_features len 1\n","Predictor target models len 1\n","prediction average -4.336570124992783\n","Additional prediction\n","Use additional model\n","Additional predictor target models len 1\n","additional prediction average -2.1176040787551642e-07\n","Submit dummy prediction\n","Error \"['revealed_target'] not found in axis\"\n","CPU times: user 1.48 s, sys: 30 ms, total: 1.51 s\n","Wall time: 1.66 s\n"]}],"source":["%%time\n","\n","predictions = []\n","df_cache = pd.DataFrame()\n","df_cache_with_features = pd.DataFrame()\n","df_result = pd.DataFrame()\n","\n","df_revealed_targets = pd.DataFrame()\n","\n","if IS_INFER:\n","    if IS_LOCAL or IS_DEBUG:\n","        print(\"Infer Local\")\n","        env = make_env()\n","    else:\n","        print(\"Infer Submission\")\n","        import optiver2023\n","        env = optiver2023.make_env()\n","    iter_test = env.iter_test()\n","    counter = 1\n","\n","    try:\n","        for (test, revealed_targets, sample_prediction) in iter_test:\n","            now_time = time.time()\n","            print(f\"------- counter {counter} start -------\")\n","\n","            # Add revealed target as target for counituous update\n","            copy_revealed_targets = revealed_targets.copy()\n","            copy_revealed_targets = copy_revealed_targets.dropna()\n","            print(\"copy_revealed_targets len\", len(copy_revealed_targets))\n","\n","            if len(copy_revealed_targets) > 0:\n","                print(\"Update revealed_targets\")\n","                copy_revealed_targets['revealed_date_id'] = copy_revealed_targets['revealed_date_id'].astype(int).astype(str)\n","                copy_revealed_targets['date_id'] = copy_revealed_targets['date_id'].astype(int).astype(str)\n","                copy_revealed_targets['seconds_in_bucket'] = copy_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n","                copy_revealed_targets['stock_id'] = copy_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n","                copy_revealed_targets['revealed_row_id'] = copy_revealed_targets['revealed_date_id'] + '_' + copy_revealed_targets['seconds_in_bucket'] + '_' + copy_revealed_targets['stock_id']\n","                copy_revealed_targets['row_id'] = copy_revealed_targets['date_id'] + '_' + copy_revealed_targets['seconds_in_bucket'] + '_' + copy_revealed_targets['stock_id']\n","                copy_revealed_targets['revealed_target'] = copy_revealed_targets['revealed_target'].astype('float32')\n","\n","                df_revealed_targets = pd.concat([df_revealed_targets, copy_revealed_targets], ignore_index=True, axis=0)\n","                df_revealed_targets = df_revealed_targets.groupby(['stock_id']).tail(DATA_COUNT_IN_SAME_BUCKET * 2)\n","                df_revealed_targets = reduce_mem_usage(df_revealed_targets, 'df_revealed_targets')\n","                \n","            df_cache = pd.concat([df_cache, test], ignore_index=True, axis=0)\n","\n","            if IS_MIN_LEARN:\n","                print(\"MIN LEARN MODE :\", TARGET_STOCK_IDS)\n","                df_cache = df_cache[df_cache[\"stock_id\"].isin(TARGET_STOCK_IDS)]\n","\n","            if counter > 0:\n","                # Clear cache data, tailはhistoricalで作成な分のみ残す\n","                df_cache = df_cache.groupby(['stock_id']).tail(10)\n","                df_cache = default_sort(df_cache)\n","                print(f\"df_cache len {len(df_cache)}\")\n","\n","            # USE_REVEALED_TARGETSが有効の時、cacheのrow_idとdf_revealed_targetsのrow_idをleft joinする\n","            if USE_REVEALED_TARGETS:\n","                df_r = df_revealed_targets[['row_id', 'revealed_target']]\n","                df_cache = pd.merge(df_cache, df_r, how='left', on='row_id')\n","                df_cache[['date_id', 'seconds_in_bucket', 'stock_id', 'revealed_target']]\n","\n","            # Generate features\n","            df_valid = df_cache.copy()\n","            df_valid = generate_basic_features(df_valid)\n","            df_valid = generate_enhance_features(df_valid)\n","            df_valid = reduce_mem_usage(df_valid, 'df_valid')\n","\n","            # testの分のみの長さを抽出\n","            if IS_MIN_LEARN:\n","                df_valid = df_valid[-len(TARGET_STOCK_IDS):].reset_index(drop=True)\n","            else:\n","                df_valid = df_valid[-len(test):].reset_index(drop=True)\n","\n","            df_cache_with_features = pd.concat([df_cache_with_features, df_valid], ignore_index=True, axis=0)\n","\n","            # It faults due to test is iterator\n","            #seconds_in_bucket = test['seconds_in_bucket'][0]\n","            #print(f\"prdict: {test['date_id'][0]}, {seconds_in_bucket}\")\n","\n","            seconds_in_bucket = df_valid['seconds_in_bucket'][0] / 10\n","            date_id = df_valid['date_id'][0]\n","            print(f\"date_id: {date_id},  seconds_in_bucket: {seconds_in_bucket}\")\n","\n","            if counter > 0:\n","                # Clear cache data, tailはhistoricalで作成な分のみ残す\n","                df_cache_with_features = df_cache_with_features.groupby(['stock_id']).tail(DATA_COUNT_IN_SAME_BUCKET * 2)\n","                df_cache_with_features = default_sort(df_cache_with_features)\n","                print(f\"df_cache_with_features len {len(df_cache_with_features)}\")\n","\n","            # Update global train cache\n","            if USE_CONTINUOUS_UPDATE  and (counter % DATA_COUNT_IN_SAME_BUCKET == 0):\n","                print(\"Update global train cache\")\n","                df_r = df_revealed_targets[['revealed_row_id', 'revealed_target']]\n","                df_r.rename(columns={'revealed_target': 'target'}, inplace=True)\n","                df_r.rename(columns={'revealed_row_id': 'row_id'}, inplace=True)\n","                df_update = pd.merge(df_cache_with_features, df_r, how='left', on='row_id')\n","                update_global_train_cache(df_update, 2)\n","\n","            # Update model\n","            if (counter % (DATA_COUNT_IN_SAME_BUCKET * continuos_train_span) == 0) and USE_CONTINUOUS_UPDATE:\n","                print(\"Update model with revealed_target date start\")\n","                train_now_time = time.time()\n","                model_dict[KEY] = keep_train_models(global_train_cache, model_dict[KEY], features, \"target\", True)\n","                print(f\"ReTrain Time: {time.time() - train_now_time}\")\n","\n","            # Predict\n","            predictions = model_infer(KEY, df_valid[features])\n","            scaled_predictions = zero_clip(df_valid, predictions)\n","\n","            df_valid['base_pred'] = predictions\n","            df_valid['base_scaled_pred'] = scaled_predictions\n","            print(\"prediction average\", np.mean(predictions))\n","\n","            if USE_ADDITIONAL_TRAIN:\n","                print(\"Additional prediction\")\n","                additional_predictions = model_infer(KEY, df_valid[features], additional_infer=True)\n","                additional_scaled_predictions = zero_clip(df_valid, additional_predictions)\n","                df_valid['additional_pred'] = additional_predictions\n","                df_valid['additional_scaled_pred'] = additional_scaled_predictions\n","                print(\"additional prediction average\", np.mean(additional_scaled_predictions))\n","\n","                predictions = (predictions + additional_predictions) / 2\n","                df_valid['average_pred'] = predictions\n","                scaled_predictions = zero_clip(df_valid, predictions)\n","                df_valid['average_scaled_pred'] = scaled_predictions\n","\n","            # For save\n","            if IS_DEBUG:\n","                df_result = pd.concat([df_result, df_valid], ignore_index=True, axis=0)\n","\n","            # Submit\n","            if not IS_MIN_LEARN:\n","                print(\"Submit prediction\")\n","                sample_prediction['target'] = scaled_predictions\n","                env.predict(sample_prediction)\n","            else:\n","                print(\"Submit dummy prediction\")\n","                sample_prediction['target'] = 0\n","                env.predict(sample_prediction)\n","\n","            # Clean up\n","            execution_time = time.time() - now_time\n","            if USE_REVEALED_TARGETS\n","                df_cache = df_cache.drop('revealed_target', axis=1)\n","            del df_valid\n","            collect()\n","            print(GetMemUsage())\n","            print(f\"------- counter {counter}, execution_time {execution_time} end -------\")\n","            counter += 1\n","    except Exception as e:\n","        print(\"Error\", e)"]},{"cell_type":"code","execution_count":383,"metadata":{},"outputs":[],"source":["if IS_DEBUG:\n","    df_cache_with_features.to_csv(f\"{BASE_OUTPUT_PATH}/df_cache_with_features.csv\", index=False)\n","    df_result.to_csv(f\"{BASE_OUTPUT_PATH}/result.csv\", index=False)"]},{"cell_type":"code","execution_count":384,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["score\n","          score\n","count  1.000000\n","mean   5.429983\n","std         NaN\n","min    5.429983\n","25%    5.429983\n","50%    5.429983\n","75%    5.429983\n","max    5.429983\n","additional_pred\n","       additional_score  average_score\n","count          1.000000       1.000000\n","mean           5.429983       5.429983\n","std                 NaN            NaN\n","min            5.429983       5.429983\n","25%            5.429983       5.429983\n","50%            5.429983       5.429983\n","75%            5.429983       5.429983\n","max            5.429983       5.429983\n"]}],"source":["if IS_DEBUG:\n","    df_revealed_targets = pd.read_csv(REVEALED_TARGETS_FILE)\n","    df_revealed_targets = df_revealed_targets.dropna(subset=['revealed_date_id', 'seconds_in_bucket', 'stock_id'])\n","    df_revealed_targets['revealed_date_id'] = df_revealed_targets['revealed_date_id'].astype(int).astype(str)\n","    df_revealed_targets['seconds_in_bucket'] = df_revealed_targets['seconds_in_bucket'].astype(int).astype(str)\n","    df_revealed_targets['stock_id'] = df_revealed_targets['stock_id'].astype(int).astype(str)  # Converting to int first to remove any decimal points\n","\n","    # Concatenate the columns\n","    df_revealed_targets['row_id'] = df_revealed_targets['revealed_date_id'] + '_' + df_revealed_targets['seconds_in_bucket'] + '_' + df_revealed_targets['stock_id']\n","    \n","    df_pred = df_result[['row_id', 'base_scaled_pred']]\n","    df = pd.merge(df_pred, df_revealed_targets, how='left', on='row_id')\n","\n","    df = df.rename(columns={'revealed_target': 'target'})\n","    df = df.dropna(subset=['target'])\n","    df['score'] = (df['base_scaled_pred'] - df['target']).abs()\n","\n","    df = df[['score']]\n","    print(\"score\")\n","    print(df.describe())\n","\n","    if USE_ADDITIONAL_TRAIN:\n","        df_pred = df_result[['row_id', 'additional_scaled_pred', 'average_scaled_pred']]\n","        df = pd.merge(df_pred, df_revealed_targets, how='left', on='row_id')\n","\n","        df = df.rename(columns={'revealed_target': 'target'})\n","        df = df.dropna(subset=['target'])\n","        df['additional_score'] = (df['additional_scaled_pred'] - df['target']).abs()\n","        df['average_score'] = (df['average_scaled_pred'] - df['target']).abs()\n","\n","        #df['score'] = mean_absolute_error(df['scaled_pred'], df['target'])\n","        print(\"additional_pred\")\n","        df = df[['additional_score', 'average_score']]\n","        print(df.describe())\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7056235,"sourceId":57891,"sourceType":"competition"}],"dockerImageVersionId":30588,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
